{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BrickFlow","text":"<p>BrickFlow is a CLI tool for development and deployment of Python based Databricks Workflows in a declarative way.</p>"},{"location":"#concept","title":"Concept","text":"<p><code>brickflow</code> aims to improve development experience for building any pipelines on databricks via:</p> <ul> <li>Providing a declarative way to describe workflows via decorators</li> <li>Provide intelligent defaults to compute targets</li> <li>Provide a code and git first approach to managing and deploying workflows</li> <li>Use databricks asset bundles to deploy workflows seamlessly. It is powered using terraform which helps manage state   across deployments.</li> <li>CLI tool helps facilitate setting up a projects</li> <li>Provides additional functionality through the context library to be able to do additional things for workflows.</li> </ul>"},{"location":"#feedback","title":"Feedback","text":"<p>Issues with <code>brickflow</code>? Found a  bug? Have a great idea for an addition? Want to improve the documentation? Please feel free to file an issue.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>To contribute please fork and create a pull request. Here is a guide to help you through this process.</p>"},{"location":"bundles-quickstart/","title":"Brickflow Projects","text":"","boost":3},{"location":"bundles-quickstart/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install Locally (optional):</p> <ol> <li>Python &gt;= 3.8</li> </ol> </li> <li> <p>Configure the databricks cli cfg file. <code>pip install databricks-cli</code> and then <code>databricks configure -t</code> which    will configure the databricks cli with a token. </p> <pre><code>pip install databricks-cli\ndatabricks configure -t\n</code></pre> </li> <li> <p>Install brickflow cli</p> <pre><code>pip install brickflows\n</code></pre> </li> </ol>","boost":3},{"location":"bundles-quickstart/#confirming-the-installation","title":"Confirming the installation","text":"<ul> <li> <p>To confirm the setup run the following command:</p> <pre><code>bf --help\n</code></pre> </li> <li> <p>Also confirm the connectivity to databricks:</p> <p><pre><code>databricks workspace list /\n</code></pre>   or if you have specific profile</p> <pre><code>databricks workspace list /  --profile &lt;profile&gt;\n</code></pre> </li> </ul>","boost":3},{"location":"bundles-quickstart/#brickflow-projects-setup","title":"Brickflow Projects Setup","text":"<p>Brickflow introduced projects in version 0.9.2 for managing mono repos with multiple projects or workflows that need to be deployed in groups. It helps with the following things:</p> <ol> <li>It helps manage statefiles and simplifies deployment.</li> <li>It helps you manage clean up of state, etc.</li> <li>It also helps the framework resolve imports for python modules, etc in your repo.</li> </ol>","boost":3},{"location":"bundles-quickstart/#concepts","title":"Concepts","text":"<ol> <li>Project - A project is a collection of workflows that are deployed together. A project is a folder with a    entrypoint and a set of workflows.</li> <li>Workflow - A workflow is a collection of tasks that are deployed together which may be DLT pipelines, notebooks,    wheels, jars, etc.</li> </ol>","boost":3},{"location":"bundles-quickstart/#monorepo-style","title":"Monorepo Style","text":"<p>A monorepo style project is a repository that has multiple folders and modules that can contain multiple brickflow projects. Learn more here.</p> <p>Folder structure:</p> <pre><code>repo-root/\n\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 projects/\n\u2502   \u251c\u2500\u2500 project_abc/\n\u2502   \u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 shared_functions.py\n\u2502   \u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow_abc.py\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u2514\u2500\u2500 .brickflow-project-root.yml\n\u2502   \u2514\u2500\u2500 project_xyz/\n\u2502       \u251c\u2500\u2500 workflows_geo_b/\n\u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n\u2502       \u251c\u2500\u2500 workflows_geo_a/\n\u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n\u2502       \u2514\u2500\u2500 .brickflow-project-root.yml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 brickflow-multi-project.yml\n\u2514\u2500\u2500 README.md\n</code></pre> <ol> <li>entrypoint.py: This is the entrypoint for your project. It is the file that will be used to identify all the    workflows to be deployed.</li> <li>brickflow-multi-project.yml: This is the project file that will be generated by brickflow. It will contain the    list of projects and a path to the project root config. This will be created in the git repository root (where your    .git folder is).</li> </ol> <p>Example for monorepo with multiple projects:</p> <pre><code>```yaml\nproject_roots:\n  project_abc:\n    root_yaml_rel_path: projects/project_abc\n  project_xyz_geo_a:\n    root_yaml_rel_path: projects/project_xyz\n  project_xyz_geo_b:\n    root_yaml_rel_path: projects/project_xyz\nversion: v1\n```\n</code></pre> <ol> <li>brickflow-project-root.yml: This is the project root config file. It will contain the list of workflows and a    path to the workflows root config.</li> </ol> <p>Example for monorepo with multiple projects for <code>repo-root/projects/project_xyz/.brickflow-project-root.yml</code>:</p> <pre><code>```yaml\n# DO NOT MODIFY THIS FILE - IT IS AUTO GENERATED BY BRICKFLOW AND RESERVED FOR FUTURE USAGE\nprojects:\n  project_xyz_geo_a:\n    brickflow_version: auto # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_xyz_geo_a\n    path_from_repo_root_to_project_root: projects/project_xyz # path from the repo root (where your .git folder is) to the project root\n    path_project_root_to_workflows_dir: workflows_geo_a\n  project_xyz_geo_b:\n    brickflow_version: auto  # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_xyz_geo_b\n    path_from_repo_root_to_project_root: projects/project_xyz\n    path_project_root_to_workflows_dir: workflows_geo_b\nversion: v1\n```\n</code></pre> <p>The important fields are:</p> <ul> <li>path_from_repo_root_to_project_root: This is the path from the repo root to the project root. This is the path   that will be used to find the entrypoint file.</li> <li>path_project_root_to_workflows_dir: This is the path from the project .git root and is used to find and load   modules into python<ul> <li>This is what helps you make your imports work in your notebooks. It is the path from the project root to the   workflows directory.</li> </ul> </li> </ul>","boost":3},{"location":"bundles-quickstart/#polyrepo-style","title":"Polyrepo Style","text":"<p>A polyrepo style project is a repository that has multiple repositories that can contain multiple brickflow projects.</p> <p>Folder structure</p> <pre><code>repo-root/\n\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 shared_functions.py\n\u2502   \u251c\u2500\u2500 workflows_a/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502   \u2502   \u2514\u2500\u2500 workflow_a.py\n\u2502   \u251c\u2500\u2500 workflows_b/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502   \u2502   \u2514\u2500\u2500 workflow_b.py\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .brickflow-project-root.yml\n\u251c\u2500\u2500 brickflow-multi-project.yml\n\u2514\u2500\u2500 README.md\n</code></pre> <ol> <li>entrypoint.py: This is the entrypoint for your project. It is the file that will be used to identify all the    workflows to be deployed.</li> <li>brickflow-multi-project.yml: This is the project file that will be generated by brickflow. It will contain the    list of projects and a path to the project root config. This will be created in the git repository root (where your    .git folder is).</li> </ol> <p>Example for polyrepo with multiple projects:</p> <pre><code>```yaml\nproject_roots:\n  project_abc:\n    root_yaml_rel_path: .\n  project_abc_workflows_2:\n    root_yaml_rel_path: .\n  project_xyz:\n    root_yaml_rel_path: .\nversion: v1\n```\n</code></pre> <ol> <li>brickflow-project-root.yml: This is the project root config file. It will contain the list of workflows and a    path to the workflows root config.</li> </ol> <p>Example for polyrepo with multiple projects:</p> <pre><code>```yaml\n# DO NOT MODIFY THIS FILE - IT IS AUTO GENERATED BY BRICKFLOW AND RESERVED FOR FUTURE USAGE\nprojects:\n  project_abc:\n    brickflow_version: auto # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_abc\n    path_from_repo_root_to_project_root: . # path from the repo root (where your .git folder is) to the project root\n    path_project_root_to_workflows_dir: workflows\n  project_abc_workflows_2:\n    brickflow_version: auto  # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_abc_workflows_2\n    path_from_repo_root_to_project_root: .\n    path_project_root_to_workflows_dir: workflows2\nversion: v1\n```\n</code></pre> <p>The important fields are:</p> <pre><code>* path_from_repo_root_to_project_root: This is the path from the repo root to the project root. This is the path\n  that will be used to find the entrypoint file.\n* path_project_root_to_workflows_dir: This is the path from the project .git root and is used to find and load\n  modules into python\n    * This is what helps you make your imports work in your notebooks. It is the path from the project root to the\n      workflows directory.\n</code></pre>","boost":3},{"location":"bundles-quickstart/#initialize-project","title":"Initialize Project","text":"<p>The first step is to create a new project.</p> <p>Warning</p> <p>Make sure you are in repository root (where your .git folder is) to do this! Otherwise you will run into validation issues.</p> <p>Note</p> <p>Please note that if you are an advanced user and understand the concepts of both files described above,  you can manually create the files thats brickflow projects add creates.</p> <ol> <li>Run the following command: <pre><code>bf projects add\n</code></pre></li> <li> <p>Update your .gitignore file with the correct directories to ignore. <code>.databricks</code> and <code>bundle.yml</code> should be ignored.</p> </li> <li> <p>It will prompt you for the:</p> </li> </ol> <pre><code>Project Name: # (1)!\nPath from repo root to project root (optional) [.]: # (2)!\nPath from project root to workflows dir: # (3)!\nGit https url: # (4)!\nBrickflow version [auto]: # (5)!\nSpark expectations version [0.8.0]: # (6)!  \nSkip entrypoint [y/N]: # (7)!\n</code></pre> <ol> <li>A name thats not already used please only use alphanumeric characters</li> <li>If you have a polyrepo leave this a <code>.</code>. Look above for polyrepo sections    and monorepo sections for guidance.</li> <li>Look above for polyrepo sections and monorepo sections for guidance.</li> <li>Used to populate entrypoint and used for deployment to higher environments</li> <li>Auto or hard code specific version to be shipped with the project during deployment</li> <li>If you want to use spark expectations. Visit spark-expectations for    more information.</li> <li>If you already have an entrypoint in that folder you can skip this step.</li> </ol>","boost":3},{"location":"bundles-quickstart/#validating-your-project","title":"Validating your project","text":"<ul> <li> <p>To test your configuration run the following command:</p> <pre><code>bf projects synth --project &lt;project_name&gt; --profile &lt;profile&gt; # profile is optional its your databricks profile\n</code></pre> </li> <li> <p>This will generate the following output at the end:</p> <pre><code>SUCCESSFULLY SYNTHESIZED BUNDLE.YML FOR PROJECT: &lt;project_name&gt;\n</code></pre> </li> <li> <p>This should create a bundle.yml file in your project root and it should contain all the information for your workflow.</p> </li> <li> <p>Anything else would indicate an error.</p> </li> </ul>","boost":3},{"location":"bundles-quickstart/#gitignore","title":"gitignore","text":"<ul> <li> <p>For now all the bundle.yml files will be code generated so you can add the following to your .gitignore file:</p> <pre><code>**/bundle.yml\n</code></pre> </li> </ul>","boost":3},{"location":"bundles-quickstart/#deploying-your-project","title":"Deploying your Project","text":"<ul> <li> <p>To deploy the workflow run the following command</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> </ul> <p>By default this will deploy to local.</p> <p>Important</p> <p>Keep in mind that environments are logical, your profile controls where the workflows are deployed and your code  may have business logic based on which environment you are on.</p> <p>If you want to deploy to a higher environment you can use the following command:</p> <ul> <li> <p>dev:</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e dev --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> <li> <p>test:</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> <li> <p>prod:</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e prod --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> </ul>","boost":3},{"location":"bundles-quickstart/#deployments-by-release-candidates-or-prs","title":"Deployments By Release Candidates or PRs","text":"<p>Sometimes you may want to deploy multiple RC branches into the same \"test\" environment. Your objective will be to:</p> <ol> <li>Deploy the workflows</li> <li>Run and test the workflows</li> <li>Destroy the workflows after confirming the tests pass</li> </ol> <p>To do this you can use the <code>BRICKFLOW_WORKFLOW_PREFIX</code> and <code>BRICKFLOW_WORKFLOW_SUFFIX</code> environment variables.</p> <ul> <li>Doing it based on release candidates</li> </ul> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-rc1\" bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> <ul> <li>Doing it based on PRs</li> </ul> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-pr34\" bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> <p>Make sure when using the suffix and prefix that you destroy them, they are considered independent deployments and have their own state.</p> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-rc1\" bf projects destroy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> <ul> <li>Doing it based on PRs</li> </ul> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-pr34\" bf projects destroy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre>","boost":3},{"location":"bundles-quickstart/#destroying-your-project","title":"Destroying your project","text":"<ul> <li> <p>To destroy the workflow run the following command</p> <pre><code>bf projects destroy --project &lt;project&gt; -p &lt;profile&gt; --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> </ul>","boost":3},{"location":"environment-variables/","title":"ENV Variables","text":""},{"location":"environment-variables/#environment-variables","title":"Environment Variables","text":"Environment Variable Default Value Description BRICKFLOW_ENV local The environment name for Brickflow BRICKFLOW_GIT_REPO N/A The URL of the Git repository for Brickflow BRICKFLOW_GIT_REF N/A The Git reference (branch, tag, commit) for Brickflow BRICKFLOW_GIT_PROVIDER github The Git provider (e.g., GitHub, GitLab) for Brickflow DATABRICKS_CONFIG_PROFILE default The profile name for Databricks configuration BRICKFLOW_DEPLOY_ONLY_WORKFLOWS N/A List of workflows to deploy exclusively BRICKFLOW_WORKFLOW_PREFIX N/A Prefix to add to workflow names during deployment BRICKFLOW_WORKFLOW_SUFFIX N/A Suffix to add to workflow names during deployment BRICKFLOW_INTERACTIVE_MODE True Flag indicating whether to enable interactive mode BRICKFLOW_BUNDLE_BASE_PATH /Users/${workspace.current_user.userName} The base path for the bundle in the S3 backend BRICKFLOW_BUNDLE_OBJ_NAME .brickflow_bundles The name of the folder post appended to your base path BRICKFLOW_BUNDLE_CLI_EXEC databricks The executable command for bundle execution. By default it will be downloaded on the fly. BRICKFLOW_BUNDLE_NO_DOWNLOAD False Flag indicating whether to skip downloading the databricks bundle cli. Useful if you are in locked down network. BRICKFLOW_BUNDLE_CLI_VERSION 0.200.0 The version of the bundle CLI tool BRICKFLOW_MONOREPO_PATH_TO_BUNDLE_ROOT N/A The path to the bundle root directory in a monorepo. Default assumes you are not using a monorepo"},{"location":"environment-variables/#workflow-prefixing-or-suffixing","title":"Workflow prefixing or suffixing","text":"<p>This allows for adding suffixes or prefixes in the name of the workflow:</p> <ul> <li>BRICKFLOW_WORKFLOW_PREFIX</li> <li>BRICKFLOW_WORKFLOW_SUFFIX</li> </ul> <p>Setting the above is semantically the same as doing this in code:</p> <pre><code>wf = Workflow(\n\"thanks\",\nprefix=\"so_long_\",  # same as BRICKFLOW_WORKFLOW_PREFIX\nsuffix=\"_and_thanks_for_all_the_fish\"  # same as BRICKFLOW_WORKFLOW_SUFFIX\n)\n</code></pre> <p><code>wf.name</code> would then result in \"so_long_and_thanks_for_all_the_fish\"</p> <p>this is to allow 'unique' names while deploying the same workflow to same environments while still needing to keep them separate.</p> <p>For example, consider this scenario:</p> <ul> <li>You have a workflow named <code>inventory_upsert</code>;</li> <li>Two features are being developed on in parallel in the DEV environment, let's name these <code>feature_1</code> and <code>feature_2</code>;</li> <li>If you don't have the ability to uniquely set the name for a workflow, the workflow you are creating in dev (no matter   in which feature/branch they originate from) will always be named <code>dev_inventory_upsert</code>;</li> <li>with using the prefix/suffix mechanism, we can set a ENV variable and end up with unique names for each feature,   i.e. <code>dev_inventory_upsert_feature_1</code> and <code>dev_inventory_upsert_feature_2</code>.</li> </ul> <p>Ideal usage for this is in CI/CD pipelines.</p>"},{"location":"highlevel/","title":"HighLevel","text":""},{"location":"highlevel/#brickflow-overview","title":"Brickflow Overview","text":"<p>The objective of Brickflow is to provide a thin layer on top of databricks workflows to help deploy  and manage workflows in Databricks. It also provides plugins/extras to be able to run airflow  operators directly in the workflows.</p>"},{"location":"highlevel/#brickflow-to-airflow-term-mapping","title":"Brickflow to Airflow Term Mapping","text":"Object Airflow Brickflow Collection of Workflows Airflow Cluster (Airflow Dag Bag) Project/Entrypoint Workflow Airflow Dag Workflow Task Airflow Operator Task Schedule Unix Cron Quartz Cron Inter Task Communication XComs Task Values Managing Connections to External Services Airflow Connections Mocked Airflow connections or Databricks Secrets Variables to Tasks Variables Task Parameters [ctx.get_parameter(key, default)] Context values (execution_date, etc.) Airflow Macros, context[\"ti\"] ctx.&lt;task parameter&gt;"},{"location":"how-imports-work/","title":"Importing Modules","text":""},{"location":"how-imports-work/#how-do-imports-work","title":"How do imports work?","text":"<p>Warning</p> <p>This is very important to understand how imports work for mono repos. Please read this carefully. Otherwise you might run into issues during deployments.</p> <p>When using brickflow projects every project will have a <code>.brickflow-project-root.yml</code> file. When you import brickflow, which you will in your entrypoint or workflows, brickflow will inspect all paths all stackframes during the import and recursively go up the path until it finds the <code>.brickflow-project-root.yml</code> file. The first instance of brickflow-project-root.yml will be added to the sys.path to help with module imports.</p> <p>Let us take a quick example of how to get imports to properly work!</p> <p>Let us say you have a project structure like this:</p> <pre><code>    repo-root/\n    \u251c\u2500\u2500 .git\n    \u251c\u2500\u2500 projects/\n    \u2502   \u251c\u2500\u2500 project_abc/\n    \u2502   \u2502   \u251c\u2500\u2500 lib/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 shared_functions.py\n    \u2502   \u2502   \u251c\u2500\u2500 workflows/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow_abc.py\n    \u2502   \u2502   \u251c\u2500\u2500 setup.py\n    \u2502   \u2502   \u2514\u2500\u2500 .brickflow-project-root.yml\n    \u2502   \u2514\u2500\u2500 project_xyz/\n    \u2502       \u251c\u2500\u2500 workflows_geo_b/\n    \u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n    \u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n    \u2502       \u251c\u2500\u2500 workflows_geo_a/\n    \u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n    \u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n    \u2502       \u2514\u2500\u2500 .brickflow-project-root.yml\n    \u251c\u2500\u2500 .gitignore\n    \u251c\u2500\u2500 brickflow-multi-project.yml\n    \u2514\u2500\u2500 README.md\n</code></pre> <p>If let us say you are looking at adding imports from lib into <code>workflow_abc.py</code>, you need to:</p> <pre><code>from lib import share_functions\nshare_functions.some_function(....)\n</code></pre> <p>Since in the project structure the <code>.brickflow-project-root.yml</code> is at <code>repo-root/projects/project_abc</code> then everything in that <code>project_abc</code> folder is added to sys.path in python. So you can import any of the folders under there.</p>"},{"location":"projects/","title":"Projects","text":"<p>The project is similar to a map cluster it can be composed of various different Workflows or dags.</p> <p>Here is an example of an entrypoint.  Click the plus buttons to understand all the parts of the entrypoint file.</p> entrypoint.py<pre><code># Databricks notebook source  (1)\nimport examples.brickflow_examples.workflows\nfrom brickflow import Project, PypiTaskLibrary, MavenTaskLibrary\ndef main() -&gt; None:\n\"\"\"Project entrypoint\"\"\"\nwith Project(\n\"brickflow-demo\",  # (3)!\ngit_repo=\"https://github.com/nike-inc/brickflow\",  # (4)!\nprovider=\"github\",  # (5)!\nlibraries=[  # (6)!\nPypiTaskLibrary(package=\"networkx\"),\n],\n) as f:\nf.add_pkg(examples.brickflow_examples.workflows)  # (7)!\nif __name__ == \"__main__\":  # (2)!\nmain()\n</code></pre> <ol> <li>Uploading this Python file into databricks with this comment on the first line treats the python file     as a notebook.</li> <li>This makes sure this only runs when this file is run via python entrypoint.py</li> <li>This is the project name you provided when you do <code>bf projects add</code></li> <li>This is the git repo that is introspected when running <code>bf projects add</code></li> <li>This is the github provider that you decide on.</li> <li>You can provide a list of packages that need to be installed in all of your clusters when running ETL.</li> <li>You can add multiple packages in your project where you are defining workflows.</li> </ol>"},{"location":"tasks/","title":"Tasks","text":"<p>A task in Databricks workflows refers to a single unit of work that is executed as part of a larger data processing  pipeline. Tasks are typically designed to perform a specific set of operations on data, such as loading data from a  source, transforming the data, and storing it in a destination. In brickflow, tasks as designed in such a way that </p> <p>Assuming, that this is already read - workflow and workflow object is created</p>"},{"location":"tasks/#task","title":"Task","text":"<p>Databricks workflow task can be created by decorating a python function with brickflow's task function</p> task<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task  # (1)!\ndef start():\npass\n@wf.task(name=\"custom_end\")  # (2)!\ndef end():\npass\n</code></pre> <ol> <li>Create a task using a decorator pattern. The task name would default to the python function name. So a task will be      created with the name \"start\"</li> <li>Creating a task and defining the task name explicitly instead of using the function name \"end\". The task will be    created with the new name \"custom_end\"</li> </ol>"},{"location":"tasks/#task-dependency","title":"Task dependency","text":"<p>Define task dependency by using a variable \"depends_on\" in the task function. You can provide the dependent tasks as direct python callables or string or list of callables/strings</p> task_dependency<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task\ndef start():\npass\n@wf.task(depends_on=start)  # (1)!\ndef bronze_layer():\npass\n@wf.task(depends_on=\"bronze_layer\")  # (2)!\ndef x_silver():\npass\n@wf.task(depends_on=bronze_layer)\ndef y_silver():\npass\n@wf.task(depends_on=[x_silver, y_silver])  # (3)!\ndef xy_gold():\npass\n@wf.task(name=\"custom_z_gold\", depends_on=[x_silver, \"y_silver\"])  # (4)!\ndef z_gold():\npass\n@wf.task(depends_on=[\"xy_gold\", \"custom_z_gold\"])  # (5)!\ndef end():\npass\n</code></pre> <ol> <li>Create dependency on task \"start\" and it is passed as callable</li> <li>Create dependency on task \"bronze_layer\" and it is passed as a string</li> <li>Create dependency on multiple tasks using list and the tasks are callables</li> <li>Create dependency on multiple tasks using list but one task is a callable and another is a string</li> <li>Create dependency on multiple tasks using list and tasks are passed as string. \"custom_z_gold\" is the task name that    is explicitly defined - should not use \"z_gold\" which is a function name</li> </ol>"},{"location":"tasks/#task-parameters","title":"Task parameters","text":"<p>Task parameters can be defined as key value pairs in the function definition on which task is defined</p> task_parameters<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task\ndef task_function(*, test=\"var\", test1=\"var1\"):  # (1)!\nprint(test)\nprint(test1)\n</code></pre> <ol> <li>To pass the task specific parameters, need to start with \"*\" and then key value pairs start</li> </ol>"},{"location":"tasks/#common-task-parameters","title":"Common task parameters","text":"<p>In the workflows section, we saw how the common task parameters are created at  the workflow level. Now in this section, we shall see how to use the common task parameters</p> use_common_task_parameters<pre><code>from brickflow import Workflow, ctx\nwf = Workflow(...)\n@wf.task\ndef common_params():\nimport some_pyspark_function  # (1)!\ncatalog_env = ctx.get_parameter(key=\"catalog\", debug=\"local\")  # (2)!\nsome_pyspark_function(catalog_env)  # (3)!\n</code></pre> <ol> <li>It is recommended to use localized imports in tasks rather than the global imports</li> <li>Brickflow provides the context using which we can fetch the task parameters that are defined. Providing debug is    mandatory or else there will be a compilation error while deploying</li> <li>The extracted task_parameter_value can be used as any python variable. In this example, we are just passing the    variable to \"some_pyspark_function\"</li> </ol>"},{"location":"tasks/#inbuilt-task-parameters","title":"Inbuilt task parameters","text":"<p>There are many inbuilt task parameters that be accessed using brickflow context like above</p> inbuilt_task_parameters<pre><code>from brickflow import Workflow, ctx\nwf = Workflow(...)\n@wf.task\ndef inbuilt_params():\nprint(ctx.get_parameter(\nkey=\"brickflow_env\",  # (1)! \ndebug=\"local\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_run_id\",  # (2)! \ndebug=\"788868\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_job_id\",  # (3)! \ndebug=\"987987987987987\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_start_date\",  # (4)! \ndebug=\"2023-05-03\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_start_time\",  # (5)! \ndebug=\"1683102411626\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_task_retry_count\",  # (6)! \ndebug=\"2\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_parent_run_id\",  # (7)! \ndebug=\"788869\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_task_key\",  # (8)! \ndebug=\"inbuilt_params\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_workflow_name\",  # (9)! \ndebug=\"Sample_Workflow\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_task_name\",  # (10)! \ndebug=\"inbuilt_params\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_workflow_prefix\",  # (11)! \ndebug=\"inbuilt_params\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_workflow_suffix\",  # (12)! \ndebug=\"inbuilt_params\"))\n</code></pre> <ol> <li>\"brickflow_env\" holds the value of the --env variable which was used when brickflow is deployed</li> <li>\"brickflow_run_id\" holds the value of the current task run id</li> <li>\"brickflow_job_id\" holds the value of the current workflow job id</li> <li>\"brickflow_start_date\" holds the value of the current workflow start date</li> <li>\"brickflow_start_time\" holds the value of the current task start time</li> <li>\"brickflow_task_retry_count\" holds the value of number of retries a task can run, when a failure occurs</li> <li>\"brickflow_parent_run_id\" hold the value of the current workflow run_id</li> <li>\"brickflow_task_key\" holds the value of the current task name</li> <li>\"brickflow_internal_workflow_name\" holds the value of the current workflow name</li> <li>\"brickflow_internal_task_name\" holds the value of the current task name</li> <li>\"brickflow_internal_workflow_prefix\" holds the value of the prefix used for the current workflow name</li> <li>\"brickflow_internal_workflow_suffix\" holds the value of the suffix used for the current workflow name</li> </ol>"},{"location":"tasks/#clusters","title":"Clusters","text":"<p>There is a flexibility to use different clusters for each task or assign custom clusters</p> clusters<pre><code>from brickflow import Workflow, Cluster\nwf = Workflow(...)\n@wf.task(cluster=Cluster(...))  # (1)!\ndef custom_cluster():\npass\n</code></pre> <ol> <li>You will be able to create a job cluster or use existing cluster. Refer to this section in     the workflows to understand how to implement</li> </ol>"},{"location":"tasks/#libraries","title":"Libraries","text":"<p>There is a flexibility to use specific libraries for a particular task</p> libraries<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task(libraries=[...])  # (1)!\ndef custom_libraries():\npass\n</code></pre> <ol> <li>You will be able to install libraries that are specific to a task. Refer to this section in    the workflows to understand how to implement</li> </ol>"},{"location":"tasks/#task-types","title":"Task types","text":"<p>There are different task types that are supported by brickflow right now. The default task type that is used by  brickflow is NOTEBOOK</p> task_types<pre><code>from brickflow import Workflow, TaskType, BrickflowTriggerRule, TaskResponse\nwf = Workflow(...)\n@wf.task\ndef notebook_task():\npass\n@wf.task(task_type=TaskType.DLT)\ndef dlt_task():\npass\n</code></pre> <ol> <li>Provide the task type that is to be used for this task. Default is a notebook task</li> <li>Trigger rule can be attached. It can be ALL_SUCCESS or NONE_FAILED. In this case, this task will be triggered, if all    the upstream tasks are at-least run and completed.</li> </ol>"},{"location":"tasks/#trigger-rules","title":"Trigger rules","text":"<p>There are two types of trigger rules that can be applied on a task. It can be either ALL_SUCCESS or NONE_FAILED</p> task_types<pre><code>from brickflow import Workflow, BrickflowTriggerRule\nwf = Workflow(...)\n@wf.task(\ntrigger_rule=BrickflowTriggerRule.NONE_FAILED  # (1)!\n)\ndef none_failed_task():\npass\n@wf.task(\ntrigger_rule=BrickflowTriggerRule.ALL_SUCCESS  # (2)!\n)\ndef all_success_task():\npass\n</code></pre> <ol> <li>NONE_FAILED - use this if you want to trigger the task irrespective of the upstream tasks success or failure state</li> <li>ALL_SUCCESS - use this if you want to trigger the task only if all the upstream tasks are all having success state</li> </ol>"},{"location":"tasks/#tasks-conditional-run","title":"Tasks conditional run","text":"<p>Adding condition for task running based on result of parent tasks</p> task_conditional_run<pre><code>from brickflow import Workflow, TaskRunCondition, TaskSettings\nwf = Workflow(...)\n@wf.task(\ntask_settings=TaskSettings(run_if=TaskRunCondition.AT_LEAST_ONE_FAILED)\n)\ndef none_failed_task():\npass\n</code></pre> <p>This option is determining whether the task is run once its dependencies have been completed. Available options: 1. <code>ALL_SUCCESS</code>: All dependencies have executed and succeeded 2. <code>AT_LEAST_ONE_SUCCESS</code>: At least one dependency has succeeded 3. <code>NONE_FAILED</code>: None of the dependencies have failed and at least one was executed 4. <code>ALL_DONE</code>: All dependencies completed and at least one was executed 5. <code>AT_LEAST_ONE_FAILED</code>: At least one dependency failed 6. <code>ALL_FAILED</code>: ALl dependencies have failed</p>"},{"location":"tasks/#airflow-operators","title":"Airflow Operators","text":"<p>We have adopted/extended certain airflow operators that might be needed to run as a task in databricks workflows. Typically for airflow operators we return the operator and brickflow will execute the operator based on task return type.</p>"},{"location":"tasks/#bash-operator","title":"Bash Operator","text":"<p>You will be able to use bash operator as below</p> bash_operator<pre><code>from brickflow import Workflow\nfrom brickflow_plugins import BashOperator\nwf = Workflow(...)\n@wf.task\ndef bash_task():\nreturn BashOperator(task_id=bash_task.__name__, \nbash_command=\"ls -ltr\")  # (1)!\n</code></pre> <ol> <li>Use Bashoperator like how we use in airflow but it has to be returned from task function</li> </ol>"},{"location":"tasks/#task-dependency-sensor","title":"Task Dependency Sensor","text":"<p>Even if you migrate to databricks workflows, brickflow gives you the flexibility to have a dependency on the airflow job</p> task_dependency_sensor<pre><code>from brickflow import Workflow, ctx\nfrom brickflow_plugins import TaskDependencySensor, AirflowProxyOktaClusterAuth\nwf = Workflow(...)\n@wf.task\ndef airflow_external_task_dependency_sensor():\nimport base64\nfrom datetime import timedelta\ndata = base64.b64encode(\nctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"okta_conn_id\").encode(\n\"utf-8\"\n)\n).decode(\"utf-8\")\nreturn TaskDependencySensor(\ntask_id=\"sensor\",\ntimeout=180,\nairflow_cluster_auth=AirflowProxyOktaClusterAuth(\noauth2_conn_id=f\"b64://{data}\",\nairflow_cluster_url=\"https://proxy.../.../cluster_id/\",\nairflow_version=\"2.0.2\", # if you are using airflow 1.x please make sure this is the right value, the apis are different between them!\n),\nexternal_dag_id=\"external_airlfow_dag\",\nexternal_task_id=\"hello\",\nallowed_states=[\"success\"],\nexecution_delta=timedelta(hours=-2),\nexecution_delta_json=None,\npoke_interval= 60,\n)\n</code></pre>"},{"location":"tasks/#autosys-sensor","title":"Autosys Sensor","text":"<p>This operator calls an Autosys API and is used to place a dependency on Autosys jobs, when necessary.</p> autosys_sensor<pre><code>from brickflow import Workflow, ctx\nfrom brickflow_plugins import AutosysSensor, AirflowProxyOktaClusterAuth\nwf = Workflow(...)\n@wf.task\ndef airflow_autosys_sensor():\nimport base64\ndata = base64.b64encode(\nctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"okta_conn_id\").encode(\n\"utf-8\"\n)\n).decode(\"utf-8\")\nreturn AutosysSensor(\ntask_id=\"sensor\",\nurl=\"https://autosys.../.../api/\",\nairflow_cluster_auth=AirflowProxyOktaClusterAuth(\noauth2_conn_id=f\"b64://{data}\",\nairflow_cluster_url=\"https://autosys.../.../api/\",\nairflow_version=\"2.0.2\", \n),\npoke_interval=200,\njob_name=\"hello\",\ntime_delta={\"days\": 0},\n)\n</code></pre>"},{"location":"tasks/#workflow-dependency-sensor","title":"Workflow Dependency Sensor","text":"<p>Wait for a workflow to finish before kicking off the current workflow's tasks</p> workflow_dependency_sensor<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import WorkflowDependencySensor\nwf = Workflow(...)\n@wf.task\ndef wait_on_workflow(*args):\napi_token_key = ctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"api_token_key\")\nsensor = WorkflowDependencySensor(\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ndependency_job_id=job_id,\npoke_interval=20,\ntimeout=60,\ndelta=timedelta(days=1)\n)\nsensor.execute()\n</code></pre>"},{"location":"workflows/","title":"Workflows","text":"<p>A Workflow is similar to an Airflow dag that lets you encapsulate a set of tasks. </p> <p>Here is an example of a workflow.  Click the plus buttons to understand all the parts of the workflow file.</p> workflow.py<pre><code>from datetime import timedelta\nfrom brickflow import Workflow, Cluster, WorkflowPermissions, User, \\\n    TaskSettings, EmailNotifications, PypiTaskLibrary, MavenTaskLibrary\nwf = Workflow(  # (1)!\n\"wf_test\",  # (2)!\ndefault_cluster=Cluster.from_existing_cluster(\"your_existing_cluster_id\"),  # (3)!\n# Optional parameters below\nschedule_quartz_expression=\"0 0/20 0 ? * * *\",  # (4)!\ntimezone=\"UTC\",  # (5)!\nschedule_pause_status=\"PAUSED\",  # (15)!\ndefault_task_settings=TaskSettings(  # (6)!\nemail_notifications=EmailNotifications(\non_start=[\"email@nike.com\"],\non_success=[\"email@nike.com\"],\non_failure=[\"email@nike.com\"],\non_duration_warning_threshold_exceeded=[\"email@nike.com\"]\n),\ntimeout_seconds=timedelta(hours=2).seconds\n),\nlibraries=[  # (7)!\nPypiTaskLibrary(package=\"requests\"),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\n],\ntags={  # (8)!\n\"product_id\": \"brickflow_demo\",\n\"slack_channel\": \"nike-sole-brickflow-support\"\n},\nmax_concurrent_runs=1,  # (9)!\npermissions=WorkflowPermissions(  # (10)!\ncan_manage_run=[User(\"abc@abc.com\")],\ncan_view=[User(\"abc@abc.com\")],\ncan_manage=[User(\"abc@abc.com\")],\n),\nprefix=\"feature-jira-xxx\",  # (11)!\nsuffix=\"_qa1\",  # (12)!\ncommon_task_parameters={  # (13)!\n\"catalog\": \"development\",\n\"database\": \"your_database\"\n},\nhealth = { # (16)!\n\"metric\": \"RUN_DURATION_SECONDS\",\n\"op\": \"GREATER_THAN\",\n\"value\": 7200\n}\n)\n@wf.task()  # (14)!\ndef task_function(*, test=\"var\"):\nreturn \"hello world\"\n</code></pre> <ol> <li>Workflow definition which constructs the workflow object</li> <li>Define the workflow name</li> <li>The default cluster used for all the tasks in the workflow. This is an all-purpose cluster, but you can also create a job cluster</li> <li>Cron expression in the quartz format</li> <li>Define the timezone for your workflow. It is defaulted to UTC</li> <li>Default task setting that can be used for all the tasks</li> <li>Libraries that need to be installed for all the tasks</li> <li>Tags for the resulting workflow and other objects created during the workflow.</li> <li>Define the maximum number of concurrent runs</li> <li>Define the permissions on the workflow</li> <li>Prefix for the name of the workflow</li> <li>Suffix for the name of the workflow</li> <li>Define the common task parameters that can be used in all the tasks</li> <li>Define a workflow task and associate it to the workflow</li> <li>Define the schedule pause status. It is defaulted to \"UNPAUSED\"</li> <li>Define health check condition that triggers duration warning threshold exceeded notifications</li> </ol>"},{"location":"workflows/#clusters","title":"Clusters","text":"<p>There are two ways to define the cluster for the workflow or a task</p>"},{"location":"workflows/#using-an-existing-cluster","title":"Using an existing cluster","text":"existing_cluster<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster.from_existing_cluster(\"your_existing_cluster_id\")\n</code></pre>"},{"location":"workflows/#use-a-job-cluster","title":"Use a job cluster","text":"job_cluster<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nnode_type_id='m6g.xlarge',\ndriver_node_type_id='m6g.xlarge',\nmin_workers=1,\nmax_workers=3,\nenable_elastic_disk=True,\npolicy_id='your_policy_id',\naws_attributes={\n\"first_on_demand\": 1,\n\"availability\": \"SPOT_WITH_FALLBACK\",\n\"instance_profile_arn\": \"arn:aws:iam::XXXX:instance-profile/XXXX/group/XX\",\n\"spot_bid_price_percent\": 100,\n\"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n\"ebs_volume_count\": 3,\n\"ebs_volume_size\": 100\n}\n)\n</code></pre>"},{"location":"workflows/#use-a-job-cluster-with-driver-and-worker-instances-from-the-same-pool","title":"Use a job cluster with driver and worker instances from the same pool","text":"job_cluster_driver_workers_same_pool<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nmin_workers=1,\nmax_workers=3,\ninstance_pool_id=\"your_instance_pool_id\",\npolicy_id='policy_id_of_pool',\n)\n</code></pre>"},{"location":"workflows/#use-a-job-cluster-with-driver-and-worker-instances-from-different-pools","title":"Use a job cluster with driver and worker instances from different pools","text":"job_cluster_driver_workers_same_pool<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nmin_workers=1,\nmax_workers=3,\ninstance_pool_id=\"your_workers_instance_pool_id\",\ndriver_instance_pool_id=\"your_driver_instance_pool_id\",\npolicy_id='policy_id_of_pools',\n)\n</code></pre>"},{"location":"workflows/#permissions","title":"Permissions","text":"<p>Brickflow provides an opportunity to manage permissions on the workflows.  You can provide individual users or to a group or to a ServicePrincipal that can help manage, run or  view the workflows.</p> <p>Below example is for reference</p> manage_permissions<pre><code>from brickflow import WorkflowPermissions, User, Group, ServicePrincipal\npermissions=WorkflowPermissions(\ncan_manage_run=[\nUser(\"abc@abc.com\"), \nGroup(\"app.xyz.team.Developer\"), \nServicePrincipal(\"ServicePrinciple_dbx_url.app.xyz.team.Developer\")\n],\ncan_view=[User(\"abc@abc.com\")],\ncan_manage=[User(\"abc@abc.com\")],\n)\n</code></pre>"},{"location":"workflows/#tags","title":"Tags","text":"<p>Using brickflow, custom tags can be created on the workflow - but there are also some default tags  that are created while the job is deployed.</p> <p>The defaults tags that gets automatically attached to the workflow are below</p> <ul> <li>\"brickflow_project_name\" : Brickflow Project Name that is referred from the entrypoint.py file</li> <li>\"brickflow_version\" : Brickflow Version that is used to deploy the workflow</li> <li>\"databricks_tf_provider_version\" : Databricks terraform provider version that is used to deploy the workflow</li> <li>\"deployed_by\" : Email id of the profile that is used to deploy the workflow.     It can be a user or a service principle. Whichever id is used to deploy the workflow, automatically becomes the    owner of the workflow</li> <li>\"environment\" : Environment to which the workflow is identified to</li> </ul> <p>Use the below reference to define more tags and attach to the workflow. These can be used for collecting various metrics and build dashboards.</p> configure_tags<pre><code>tags={\n\"product_id\": \"brickflow_demo\",\n\"slack_channel\": \"nike-sole-brickflow-support\"\n}\n</code></pre>"},{"location":"workflows/#schedule","title":"Schedule","text":"<p>Databricks workflows uses Quartz cron expression unlike airflow's unix based cron scheduler. A typical Quartz cron expression have six or seven fields, seperated by spaces</p> <p><pre><code>second minute hour day_of_month month day_of_week year(optional)\n</code></pre> Below is a sample</p> quartz_cron_expression<pre><code>schedule_quartz_expression=\"0 0/20 0 ? * * *\"\n</code></pre>"},{"location":"workflows/#tasksettings","title":"Tasksettings","text":"<p>Task setting at workflow level can be used to have common setting defined that will be applicable for all the tasks. Below is a sample that can be used for reference and all the parameters in TaskSettings are optional task_settings<pre><code>from datetime import timedelta\nfrom brickflow import TaskSettings, EmailNotifications\ndefault_task_settings=TaskSettings(\nemail_notifications=EmailNotifications(\non_start=[\"email@nike.com\"],\non_success=[\"email@nike.com\"],\non_failure=[\"email@nike.com\"],\non_duration_warning_threshold_exceeded=[\"email@nike.com\"]      \n),\ntimeout_seconds=timedelta(hours=2).seconds,\nmax_retries=2,\nmin_retry_interval_millis=60000,\nretry_on_timeout=True\n)\n</code></pre></p>"},{"location":"workflows/#libraries","title":"Libraries","text":"<p>Brickflow allows to specify libraries that are need to be installed and used across different tasks. There are many ways to install library from different repositories/sources</p> libraries<pre><code>from brickflow import PypiTaskLibrary, MavenTaskLibrary, StorageBasedTaskLibrary, \\\n    JarTaskLibrary, EggTaskLibrary, WheelTaskLibrary\nlibraries=[\nPypiTaskLibrary(package=\"requests\"),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\nStorageBasedTaskLibrary(\"s3://...\"),\nStorageBasedTaskLibrary(\"dbfs://...\"),\nJarTaskLibrary(\"s3://...\"),\nJarTaskLibrary(\"dbfs://...\"),\nEggTaskLibrary(\"s3://...\"),\nEggTaskLibrary(\"dbfs://...\"),\nWheelTaskLibrary(\"s3://...\"),\nWheelTaskLibrary(\"dbfs://...\"),\n]\n</code></pre>"},{"location":"workflows/#common-task-parameters","title":"Common task parameters","text":"<p>Define the common parameters that can be used in all the tasks. Example could be database name, secrets_id etc</p> common_task_parameters<pre><code>common_task_parameters={\n\"catalog\": \"development\",\n\"database\": \"your_database\"\n}\n</code></pre>"},{"location":"faq/faq/","title":"FAQ","text":"","boost":3},{"location":"faq/faq/#how-do-i-enable-airflow-features","title":"How do I enable airflow features?","text":"<p>Warning</p> <p>Only certain operators are supported so please make sure you read the documentation before using them. If your operator is not supported  please raise a new issue in github.</p> <p>Supported Operators:</p> <ul> <li>BranchPythonOperator</li> <li>PythonOperator</li> <li>BashOperator</li> <li>ShortCircuitOperator</li> <li>TaskDependencySensor</li> </ul> <p>To enable the usage of airflow operators, you need to set the <code>enable_plugins</code> flag to <code>True</code> in the <code>Project</code> constructor.</p>","boost":3},{"location":"faq/faq/#how-do-i-run-only-one-task-in-a-workflow","title":"How do I run only one task in a workflow?","text":"<p>Databricks and Airflow use different task scheduling mechanisms. Due to the way airflow manages state in a database, it is possible to run only one task in a workflow. Though this works very differently at Databricks as our job scheduler is very different and needs to scale to much large volume of tasks and workflows.</p> <p>To provide this capability, brickflow offers a parameter to do this:</p> <ol> <li>Go to workflow UI</li> <li>On the top right click the drop down next to <code>Run Now</code></li> <li>Select <code>Run now with different parameters</code></li> <li>Then select switch to legacy parameters</li> <li>Scroll down to: <code>brickflow_internal_only_run_tasks</code> and then type the tasks you want to run as a CSV.</li> <li>Please do not modify any other parameter!</li> </ol>","boost":3},{"location":"faq/faq/#how-do-i-wait-for-a-workflow-to-finish-before-kicking-off-my-own-workflows-tasks","title":"How do I wait for a workflow to finish before kicking off my own workflow's tasks?","text":"<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import WorkflowDependencySensor\nwf = Workflow(...)\n@wf.task\ndef wait_on_workflow(*args):\napi_token_key = ctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"api_token_key\")\nsensor = WorkflowDependencySensor(\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ndependency_job_id=job_id,\npoke_interval=20,\ntimeout=60,\ndelta=timedelta(days=1)\n)\nsensor.execute()\n</code></pre>","boost":3},{"location":"upgrades/upgrade-pre-0-10-0-to-0-10-0/","title":"Upgrading to v0.10.x","text":"","boost":2},{"location":"upgrades/upgrade-pre-0-10-0-to-0-10-0/#upgrade-checklist","title":"Upgrade checklist","text":"<ul> <li> <p> The package has been renamed from <code>brickflow</code> to <code>brickflows</code>. Please run:</p> <pre><code>pip uninstall brickflow\n</code></pre> <p>and then</p> <pre><code>pip install brickflows&gt;=0.10.0\nbf --version\n</code></pre> </li> <li> <p> If you are upgrading from a CDKTF version of brickflow then do not worry, the existing workflows as long as you do   not change their names will be imported.</p> </li> <li> <p> Start using project configurations following the quickstart guide.</p> </li> <li> <p> Confirm the existence of the following files:</p> </li> <li> <p>brickflow-multi-project.yml</p> </li> <li>brickflow-project-root.yml</li> <li> <p>Please reference concepts     and initialize project for more details.</p> </li> <li> <p> RelativePathPackageResolver has been removed from the project to offer a seamless import   as long as you import brickflow at the top.</p> </li> <li> <p> Ensure import for brickflow is at the top of your entrypoint.py</p> </li> <li> <p> Ensure import for brickflow is at the top of your entrypoint.py</p> </li> <li> <p> Ensure your entrypoint looks like this. Make sure to click the plus buttons and read the highlighted sections:</p> </li> </ul> <pre><code># Databricks notebook source\n# COMMAND ----------\nfrom brickflow import Project # (1)!\nimport workflows # (2)!\ndef main() -&gt; None:\n\"\"\"Project entrypoint\"\"\"\nwith Project(\n\"product_abc_workflows_2\",\ngit_repo=\"https://github.com/stikkireddy/mono-repo-test\",\nprovider=\"github\",\nlibraries=[  # (3)!\n# PypiTaskLibrary(package=\"spark-expectations==0.5.0\"), # Uncomment if spark-expectations is needed\n],\nenable_plugins=True, # (4)!\n) as f:\nf.add_pkg(workflows)\nif __name__ == \"__main__\":\nmain()\n</code></pre> <ol> <li>Make sure brickflow is at the top of your imports! This will help resolve paths and allow other libraries to be    imported correctly.</li> <li>Import your modules after brickflow has been imported! Make sure your optimize imports doesnt reorder your imports!</li> <li>Make sure you remove brickflow and brickflow plugins and cron utils from this list.</li> <li>Make sure you have enable_plugins=True. This will enable the plugins to be loaded to support airflow operators, etc.    Disable this if you dont want to install airflow.</li> </ol>","boost":2}]}