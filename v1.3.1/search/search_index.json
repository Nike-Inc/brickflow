{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BrickFlow","text":"<p>BrickFlow is a CLI tool for development and deployment of Python based Databricks Workflows in a declarative way.</p>"},{"location":"#concept","title":"Concept","text":"<p><code>brickflow</code> aims to improve development experience for building any pipelines on databricks via:</p> <ul> <li>Providing a declarative way to describe workflows via decorators</li> <li>Provide intelligent defaults to compute targets</li> <li>Provide a code and git first approach to managing and deploying workflows</li> <li>Use databricks asset bundles to deploy workflows seamlessly. It is powered using terraform which helps manage state   across deployments.</li> <li>CLI tool helps facilitate setting up a projects</li> <li>Provides additional functionality through the context library to be able to do additional things for workflows.</li> </ul>"},{"location":"#feedback","title":"Feedback","text":"<p>Issues with <code>brickflow</code>? Found a  bug? Have a great idea for an addition? Want to improve the documentation? Please feel free to file an issue.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>To contribute please fork and create a pull request. Here is a guide to help you through this process.</p>"},{"location":"bundles-quickstart/","title":"Brickflow Projects","text":"","boost":3},{"location":"bundles-quickstart/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install Locally (optional):</p> <ol> <li>Python &gt;= 3.8</li> </ol> </li> <li> <p>Configure the databricks cli cfg file. <code>pip install databricks-cli</code> and then <code>databricks configure -t</code> which    will configure the databricks cli with a token. </p> <pre><code>pip install databricks-cli\ndatabricks configure -t\n</code></pre> </li> <li> <p>Install brickflow cli</p> <pre><code>pip install brickflows\n</code></pre> </li> </ol>","boost":3},{"location":"bundles-quickstart/#confirming-the-installation","title":"Confirming the installation","text":"<ul> <li> <p>To confirm the setup run the following command:</p> <pre><code>bf --help\n</code></pre> </li> <li> <p>Also confirm the connectivity to databricks:</p> <p><pre><code>databricks workspace list /\n</code></pre>   or if you have specific profile</p> <pre><code>databricks workspace list /  --profile &lt;profile&gt;\n</code></pre> </li> </ul>","boost":3},{"location":"bundles-quickstart/#brickflow-projects-setup","title":"Brickflow Projects Setup","text":"<p>Brickflow introduced projects in version 0.9.2 for managing mono repos with multiple projects or workflows that need to be deployed in groups. It helps with the following things:</p> <ol> <li>It helps manage statefiles and simplifies deployment.</li> <li>It helps you manage clean up of state, etc.</li> <li>It also helps the framework resolve imports for python modules, etc in your repo.</li> </ol>","boost":3},{"location":"bundles-quickstart/#concepts","title":"Concepts","text":"<ol> <li>Project - A project is a collection of workflows that are deployed together. A project is a folder with a    entrypoint and a set of workflows.</li> <li>Workflow - A workflow is a collection of tasks that are deployed together which may be DLT pipelines, notebooks,    wheels, jars, etc.</li> </ol>","boost":3},{"location":"bundles-quickstart/#monorepo-style","title":"Monorepo Style","text":"<p>A monorepo style project is a repository that has multiple folders and modules that can contain multiple brickflow projects. Learn more here.</p> <p>Folder structure:</p> <pre><code>repo-root/\n\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 projects/\n\u2502   \u251c\u2500\u2500 project_abc/\n\u2502   \u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 shared_functions.py\n\u2502   \u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow_abc.py\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u2514\u2500\u2500 .brickflow-project-root.yml\n\u2502   \u2514\u2500\u2500 project_xyz/\n\u2502       \u251c\u2500\u2500 workflows_geo_b/\n\u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n\u2502       \u251c\u2500\u2500 workflows_geo_a/\n\u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n\u2502       \u2514\u2500\u2500 .brickflow-project-root.yml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 brickflow-multi-project.yml\n\u2514\u2500\u2500 README.md\n</code></pre> <ol> <li>entrypoint.py: This is the entrypoint for your project. It is the file that will be used to identify all the    workflows to be deployed.</li> <li>brickflow-multi-project.yml: This is the project file that will be generated by brickflow. It will contain the    list of projects and a path to the project root config. This will be created in the git repository root (where your    .git folder is).</li> </ol> <p>Example for monorepo with multiple projects:</p> <pre><code>```yaml\nproject_roots:\n  project_abc:\n    root_yaml_rel_path: projects/project_abc\n  project_xyz_geo_a:\n    root_yaml_rel_path: projects/project_xyz\n  project_xyz_geo_b:\n    root_yaml_rel_path: projects/project_xyz\nversion: v1\n```\n</code></pre> <ol> <li>brickflow-project-root.yml: This is the project root config file. It will contain the list of workflows and a    path to the workflows root config.</li> </ol> <p>Example for monorepo with multiple projects for <code>repo-root/projects/project_xyz/.brickflow-project-root.yml</code>:</p> <pre><code>```yaml\n# DO NOT MODIFY THIS FILE - IT IS AUTO GENERATED BY BRICKFLOW AND RESERVED FOR FUTURE USAGE\nprojects:\n  project_xyz_geo_a:\n    brickflow_version: auto # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_xyz_geo_a\n    path_from_repo_root_to_project_root: projects/project_xyz # path from the repo root (where your .git folder is) to the project root\n    path_project_root_to_workflows_dir: workflows_geo_a\n  project_xyz_geo_b:\n    brickflow_version: auto  # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_xyz_geo_b\n    path_from_repo_root_to_project_root: projects/project_xyz\n    path_project_root_to_workflows_dir: workflows_geo_b\nversion: v1\n```\n</code></pre> <p>The important fields are:</p> <ul> <li>path_from_repo_root_to_project_root: This is the path from the repo root to the project root. This is the path   that will be used to find the entrypoint file.</li> <li>path_project_root_to_workflows_dir: This is the path from the project .git root and is used to find and load   modules into python<ul> <li>This is what helps you make your imports work in your notebooks. It is the path from the project root to the   workflows directory.</li> </ul> </li> </ul>","boost":3},{"location":"bundles-quickstart/#polyrepo-style","title":"Polyrepo Style","text":"<p>A polyrepo style project is a repository that has multiple repositories that can contain multiple brickflow projects.</p> <p>Folder structure</p> <pre><code>repo-root/\n\u251c\u2500\u2500 .git\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 shared_functions.py\n\u2502   \u251c\u2500\u2500 workflows_a/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502   \u2502   \u2514\u2500\u2500 workflow_a.py\n\u2502   \u251c\u2500\u2500 workflows_b/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n\u2502   \u2502   \u2514\u2500\u2500 workflow_b.py\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .brickflow-project-root.yml\n\u251c\u2500\u2500 brickflow-multi-project.yml\n\u2514\u2500\u2500 README.md\n</code></pre> <ol> <li>entrypoint.py: This is the entrypoint for your project. It is the file that will be used to identify all the    workflows to be deployed.</li> <li>brickflow-multi-project.yml: This is the project file that will be generated by brickflow. It will contain the    list of projects and a path to the project root config. This will be created in the git repository root (where your    .git folder is).</li> </ol> <p>Example for polyrepo with multiple projects:</p> <pre><code>```yaml\nproject_roots:\n  project_abc:\n    root_yaml_rel_path: .\n  project_abc_workflows_2:\n    root_yaml_rel_path: .\n  project_xyz:\n    root_yaml_rel_path: .\nversion: v1\n```\n</code></pre> <ol> <li>brickflow-project-root.yml: This is the project root config file. It will contain the list of workflows and a    path to the workflows root config.</li> </ol> <p>Example for polyrepo with multiple projects:</p> <pre><code>```yaml\n# DO NOT MODIFY THIS FILE - IT IS AUTO GENERATED BY BRICKFLOW AND RESERVED FOR FUTURE USAGE\nprojects:\n  project_abc:\n    brickflow_version: auto # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_abc\n    path_from_repo_root_to_project_root: . # path from the repo root (where your .git folder is) to the project root\n    path_project_root_to_workflows_dir: workflows\n  project_abc_workflows_2:\n    brickflow_version: auto  # automatically determine the brickflow version based on cli version\n    deployment_mode: bundle\n    name: project_abc_workflows_2\n    path_from_repo_root_to_project_root: .\n    path_project_root_to_workflows_dir: workflows2\nversion: v1\n```\n</code></pre> <p>The important fields are:</p> <pre><code>* path_from_repo_root_to_project_root: This is the path from the repo root to the project root. This is the path\n  that will be used to find the entrypoint file.\n* path_project_root_to_workflows_dir: This is the path from the project .git root and is used to find and load\n  modules into python\n    * This is what helps you make your imports work in your notebooks. It is the path from the project root to the\n      workflows directory.\n</code></pre>","boost":3},{"location":"bundles-quickstart/#initialize-project","title":"Initialize Project","text":"<p>The first step is to create a new project.</p> <p>Warning</p> <p>Make sure you are in repository root (where your .git folder is) to do this! Otherwise you will run into validation issues.</p> <p>Note</p> <p>Please note that if you are an advanced user and understand the concepts of both files described above,  you can manually create the files thats brickflow projects add creates.</p> <ol> <li>Run the following command: <pre><code>bf projects add\n</code></pre></li> <li> <p>Update your .gitignore file with the correct directories to ignore. <code>.databricks</code> and <code>bundle.yml</code> should be ignored.</p> </li> <li> <p>It will prompt you for the:</p> </li> </ol> <pre><code>Project Name: # (1)!\nPath from repo root to project root (optional) [.]: # (2)!\nPath from project root to workflows dir: # (3)!\nGit https url: # (4)!\nBrickflow version [auto]: # (5)!\nSpark expectations version [0.8.0]: # (6)!  \nSkip entrypoint [y/N]: # (7)!\n</code></pre> <ol> <li>A name thats not already used please only use alphanumeric characters</li> <li>If you have a polyrepo leave this a <code>.</code>. Look above for polyrepo sections    and monorepo sections for guidance.</li> <li>Look above for polyrepo sections and monorepo sections for guidance.</li> <li>Used to populate entrypoint and used for deployment to higher environments</li> <li>Auto or hard code specific version to be shipped with the project during deployment</li> <li>If you want to use spark expectations. Visit spark-expectations for    more information.</li> <li>If you already have an entrypoint in that folder you can skip this step.</li> </ol>","boost":3},{"location":"bundles-quickstart/#validating-your-project","title":"Validating your project","text":"<ul> <li> <p>To test your configuration run the following command:</p> <pre><code>bf projects synth --project &lt;project_name&gt; --profile &lt;profile&gt; # profile is optional its your databricks profile\n</code></pre> </li> <li> <p>This will generate the following output at the end:</p> <pre><code>SUCCESSFULLY SYNTHESIZED BUNDLE.YML FOR PROJECT: &lt;project_name&gt;\n</code></pre> </li> <li> <p>This should create a bundle.yml file in your project root and it should contain all the information for your workflow.</p> </li> <li> <p>Anything else would indicate an error.</p> </li> </ul>","boost":3},{"location":"bundles-quickstart/#gitignore","title":"gitignore","text":"<ul> <li> <p>For now all the bundle.yml files will be code generated so you can add the following to your .gitignore file:</p> <pre><code>**/bundle.yml\n</code></pre> </li> </ul>","boost":3},{"location":"bundles-quickstart/#deploying-your-project","title":"Deploying your Project","text":"<ul> <li> <p>To deploy the workflow run the following command</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> </ul> <p>By default this will deploy to local.</p> <p>Important</p> <p>Keep in mind that environments are logical, your profile controls where the workflows are deployed and your code  may have business logic based on which environment you are on.</p> <p>If you want to deploy to a higher environment you can use the following command:</p> <ul> <li> <p>dev:</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e dev --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> <li> <p>test:</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> <li> <p>prod:</p> <pre><code>bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e prod --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> </ul>","boost":3},{"location":"bundles-quickstart/#deployments-by-release-candidates-or-prs","title":"Deployments By Release Candidates or PRs","text":"<p>Sometimes you may want to deploy multiple RC branches into the same \"test\" environment. Your objective will be to:</p> <ol> <li>Deploy the workflows</li> <li>Run and test the workflows</li> <li>Destroy the workflows after confirming the tests pass</li> </ol> <p>To do this you can use the <code>BRICKFLOW_WORKFLOW_PREFIX</code> and <code>BRICKFLOW_WORKFLOW_SUFFIX</code> environment variables.</p> <ul> <li>Doing it based on release candidates</li> </ul> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-rc1\" bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> <ul> <li>Doing it based on PRs</li> </ul> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-pr34\" bf projects deploy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> <p>Make sure when using the suffix and prefix that you destroy them, they are considered independent deployments and have their own state.</p> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-rc1\" bf projects destroy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre> <ul> <li>Doing it based on PRs</li> </ul> <pre><code>BRICKFLOW_WORKFLOW_SUFFIX=\"0.1.0-pr34\" bf projects destroy --project &lt;project&gt; -p &lt;profile&gt; -e test --force-acquire-lock # force acquire lock is optional\n</code></pre>","boost":3},{"location":"bundles-quickstart/#destroying-your-project","title":"Destroying your project","text":"<ul> <li> <p>To destroy the workflow run the following command</p> <pre><code>bf projects destroy --project &lt;project&gt; -p &lt;profile&gt; --force-acquire-lock # force acquire lock is optional\n</code></pre> </li> </ul>","boost":3},{"location":"environment-variables/","title":"ENV Variables","text":""},{"location":"environment-variables/#environment-variables","title":"Environment Variables","text":"Environment Variable Default Value Description BRICKFLOW_ENV local The environment name for Brickflow BRICKFLOW_GIT_REPO N/A The URL of the Git repository for Brickflow BRICKFLOW_GIT_REF N/A The Git reference (branch, tag, commit) for Brickflow BRICKFLOW_GIT_PROVIDER github The Git provider (e.g., GitHub, GitLab) for Brickflow DATABRICKS_CONFIG_PROFILE default The profile name for Databricks configuration BRICKFLOW_DEPLOY_ONLY_WORKFLOWS N/A List of workflows seperated by \",\" (e.g. wf1,wf2) to deploy exclusively BRICKFLOW_WORKFLOW_PREFIX N/A Prefix to add to workflow names during deployment BRICKFLOW_WORKFLOW_SUFFIX N/A Suffix to add to workflow names during deployment BRICKFLOW_INTERACTIVE_MODE True Flag indicating whether to enable interactive mode BRICKFLOW_BUNDLE_BASE_PATH /Users/${workspace.current_user.userName} The base path for the bundle in the S3 backend BRICKFLOW_BUNDLE_OBJ_NAME .brickflow_bundles The name of the folder post appended to your base path BRICKFLOW_BUNDLE_CLI_EXEC databricks The executable command for bundle execution. By default it will be downloaded on the fly. BRICKFLOW_BUNDLE_NO_DOWNLOAD False Flag indicating whether to skip downloading the databricks bundle cli. Useful if you are in locked down network. BRICKFLOW_BUNDLE_CLI_VERSION 0.200.0 The version of the bundle CLI tool BRICKFLOW_MONOREPO_PATH_TO_BUNDLE_ROOT N/A The path to the bundle root directory in a monorepo. Default assumes you are not using a monorepo BRICKFLOW_PROJECT_PARAMS N/A Runtime/Deployment time parameters seperated by \",\" (e.g. k1=v1,k2=v2) to pass to workflow tasks"},{"location":"environment-variables/#runtime-project-parameters","title":"Runtime project parameters","text":"<p>This allows for passing runtime parameters to the workflow tasks for given project.</p>"},{"location":"environment-variables/#environment-variable","title":"Environment Variable","text":"<p>The parameters are passed as a comma separated list when setting through environment variable. The key-value pairs are separated by an equal sign.</p> <pre><code>export BRICKFLOW_PROJECT_PARAMS=\"key1=value1,key2=value2\"\nbf projects deploy -p DEFAULT -e local\n</code></pre>"},{"location":"environment-variables/#cli","title":"CLI","text":"<p>Provide the runtime key-value parameters, each key-value separated by space when using CLI.</p> <pre><code>bf projects deploy -p DEFAULT -e local -kv key1=value1 -kv key2=value2\n</code></pre>"},{"location":"environment-variables/#accessing-the-parameters-in-the-workflow-tasks","title":"Accessing the parameters in the workflow tasks","text":"<p>The parameters can be accessed in the workflow tasks using the brickflow context object.</p> <pre><code>from brickflow.context import ctx\nctx.get_project_parameter(\"key1\") ## returns value1\nctx.get_project_parameter(\"key2\") ## returns value2\n</code></pre>"},{"location":"environment-variables/#runtime-project-tags","title":"Runtime project tags","text":"<p>This allows for passing runtime tags to be applied to workflows. After deployment, the tags will be applied to the workflows in the Databricks workspace.</p>"},{"location":"environment-variables/#environment-variable_1","title":"Environment Variable","text":"<p>The tags are passed as a comma separated list when setting through environment variable. The key-value pairs are separated by an equal sign.</p> <pre><code>export BRICKFLOW_PROJECT_TAGS=\"tag1=value1,tag2=value2\"\nbf projects deploy -p DEFAULT -e local\n</code></pre>"},{"location":"environment-variables/#cli_1","title":"CLI","text":"<p>Provide the runtime key-value tags, each key-value separated by space when using CLI.</p> <pre><code>bf projects deploy -p DEFAULT -e local --tag tag1=value1 --tag tag2=value2\n</code></pre>"},{"location":"environment-variables/#workflow-prefixing-or-suffixing","title":"Workflow prefixing or suffixing","text":"<p>This allows for adding suffixes or prefixes in the name of the workflow:</p> <ul> <li>BRICKFLOW_WORKFLOW_PREFIX</li> <li>BRICKFLOW_WORKFLOW_SUFFIX</li> </ul> <p>Setting the above is semantically the same as doing this in code:</p> <pre><code>wf = Workflow(\n\"thanks\",\nprefix=\"so_long_\",  # same as BRICKFLOW_WORKFLOW_PREFIX\nsuffix=\"_and_thanks_for_all_the_fish\"  # same as BRICKFLOW_WORKFLOW_SUFFIX\n)\n</code></pre> <p><code>wf.name</code> would then result in \"so_long_and_thanks_for_all_the_fish\"</p> <p>this is to allow 'unique' names while deploying the same workflow to same environments while still needing to keep them separate.</p> <p>For example, consider this scenario:</p> <ul> <li>You have a workflow named <code>inventory_upsert</code>;</li> <li>Two features are being developed on in parallel in the DEV environment, let's name these <code>feature_1</code> and <code>feature_2</code>;</li> <li>If you don't have the ability to uniquely set the name for a workflow, the workflow you are creating in dev (no matter   in which feature/branch they originate from) will always be named <code>dev_inventory_upsert</code>;</li> <li>with using the prefix/suffix mechanism, we can set a ENV variable and end up with unique names for each feature,   i.e. <code>dev_inventory_upsert_feature_1</code> and <code>dev_inventory_upsert_feature_2</code>.</li> </ul> <p>Ideal usage for this is in CI/CD pipelines.</p>"},{"location":"highlevel/","title":"HighLevel","text":""},{"location":"highlevel/#brickflow-overview","title":"Brickflow Overview","text":"<p>The objective of Brickflow is to provide a thin layer on top of databricks workflows to help deploy  and manage workflows in Databricks. It also provides plugins/extras to be able to run airflow  operators directly in the workflows.</p>"},{"location":"highlevel/#brickflow-to-airflow-term-mapping","title":"Brickflow to Airflow Term Mapping","text":"Object Airflow Brickflow Collection of Workflows Airflow Cluster (Airflow Dag Bag) Project/Entrypoint Workflow Airflow Dag Workflow Task Airflow Operator Task Schedule Unix Cron Quartz Cron Inter Task Communication XComs Task Values Managing Connections to External Services Airflow Connections Mocked Airflow connections or Databricks Secrets Variables to Tasks Variables Task Parameters [ctx.get_parameter(key, default)] Context values (execution_date, etc.) Airflow Macros, context[\"ti\"] ctx.&lt;task parameter&gt;"},{"location":"how-imports-work/","title":"Importing Modules","text":""},{"location":"how-imports-work/#how-do-imports-work","title":"How do imports work?","text":"<p>Warning</p> <p>This is very important to understand how imports work for mono repos. Please read this carefully. Otherwise you might run into issues during deployments.</p> <p>When using brickflow projects every project will have a <code>.brickflow-project-root.yml</code> file. When you import brickflow, which you will in your entrypoint or workflows, brickflow will inspect all paths all stackframes during the import and recursively go up the path until it finds the <code>.brickflow-project-root.yml</code> file. The first instance of brickflow-project-root.yml will be added to the sys.path to help with module imports.</p> <p>Let us take a quick example of how to get imports to properly work!</p> <p>Let us say you have a project structure like this:</p> <pre><code>    repo-root/\n    \u251c\u2500\u2500 .git\n    \u251c\u2500\u2500 projects/\n    \u2502   \u251c\u2500\u2500 project_abc/\n    \u2502   \u2502   \u251c\u2500\u2500 lib/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 shared_functions.py\n    \u2502   \u2502   \u251c\u2500\u2500 workflows/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 entrypoint.py\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow_abc.py\n    \u2502   \u2502   \u251c\u2500\u2500 setup.py\n    \u2502   \u2502   \u2514\u2500\u2500 .brickflow-project-root.yml\n    \u2502   \u2514\u2500\u2500 project_xyz/\n    \u2502       \u251c\u2500\u2500 workflows_geo_b/\n    \u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n    \u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n    \u2502       \u251c\u2500\u2500 workflows_geo_a/\n    \u2502       \u2502   \u251c\u2500\u2500 entrypoint.py\n    \u2502       \u2502   \u2514\u2500\u2500 workflow_xyz.py\n    \u2502       \u2514\u2500\u2500 .brickflow-project-root.yml\n    \u251c\u2500\u2500 .gitignore\n    \u251c\u2500\u2500 brickflow-multi-project.yml\n    \u2514\u2500\u2500 README.md\n</code></pre> <p>If let us say you are looking at adding imports from lib into <code>workflow_abc.py</code>, you need to:</p> <pre><code>from lib import share_functions\nshare_functions.some_function(....)\n</code></pre> <p>Since in the project structure the <code>.brickflow-project-root.yml</code> is at <code>repo-root/projects/project_abc</code> then everything in that <code>project_abc</code> folder is added to sys.path in python. So you can import any of the folders under there.</p>"},{"location":"projects/","title":"Projects","text":"<p>The project is similar to a map cluster it can be composed of various different Workflows or dags.</p> <p>Here is an example of an entrypoint.  Click the plus buttons to understand all the parts of the entrypoint file.</p> entrypoint.py<pre><code># Databricks notebook source  (1)\nimport examples.brickflow_examples.workflows\nfrom brickflow import Project, PypiTaskLibrary, MavenTaskLibrary\ndef main() -&gt; None:\n\"\"\"Project entrypoint\"\"\"\nwith Project(\n\"brickflow-demo\",  # (3)!\ngit_repo=\"https://github.com/nike-inc/brickflow\",  # (4)!\nprovider=\"github\",  # (5)!\nlibraries=[  # (6)!\nPypiTaskLibrary(package=\"networkx\"),\n],\n) as f:\nf.add_pkg(examples.brickflow_examples.workflows)  # (7)!\nif __name__ == \"__main__\":  # (2)!\nmain()\n</code></pre> <ol> <li>Uploading this Python file into databricks with this comment on the first line treats the python file     as a notebook.</li> <li>This makes sure this only runs when this file is run via python entrypoint.py</li> <li>This is the project name you provided when you do <code>bf projects add</code></li> <li>This is the git repo that is introspected when running <code>bf projects add</code></li> <li>This is the github provider that you decide on.</li> <li>You can provide a list of packages that need to be installed in all of your clusters when running ETL.</li> <li>You can add multiple packages in your project where you are defining workflows.</li> </ol>"},{"location":"tasks/","title":"Tasks","text":"<p>A task in Databricks workflows refers to a single unit of work that is executed as part of a larger data processing pipeline. Tasks are typically designed to perform a specific set of operations on data, such as loading data from a source, transforming the data, and storing it in a destination. In brickflow, tasks as designed in such a way that</p> <p>Assuming, that this is already read - workflow and workflow object is created</p>"},{"location":"tasks/#task","title":"Task","text":"<p>Databricks workflow task can be created by decorating a python function with brickflow's task function</p> task<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task  # (1)!\ndef start():\npass\n@wf.task(name=\"custom_end\")  # (2)!\ndef end():\npass\n</code></pre> <ol> <li>Create a task using a decorator pattern. The task name would default to the python function name. So a task will be     created with the name \"start\"</li> <li>Creating a task and defining the task name explicitly instead of using the function name \"end\". The task will be    created with the new name \"custom_end\"</li> </ol>"},{"location":"tasks/#task-dependency","title":"Task dependency","text":"<p>Define task dependency by using a variable \"depends_on\" in the task function. You can provide the dependent tasks as direct python callables or string or list of callables/strings</p> task_dependency<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task\ndef start():\npass\n@wf.task(depends_on=start)  # (1)!\ndef bronze_layer():\npass\n@wf.task(depends_on=\"bronze_layer\")  # (2)!\ndef x_silver():\npass\n@wf.task(depends_on=bronze_layer)\ndef y_silver():\npass\n@wf.task(depends_on=[x_silver, y_silver])  # (3)!\ndef xy_gold():\npass\n@wf.task(name=\"custom_z_gold\", depends_on=[x_silver, \"y_silver\"])  # (4)!\ndef z_gold():\npass\n@wf.task(depends_on=[\"xy_gold\", \"custom_z_gold\"])  # (5)!\ndef end():\npass\n</code></pre> <ol> <li>Create dependency on task \"start\" and it is passed as callable</li> <li>Create dependency on task \"bronze_layer\" and it is passed as a string</li> <li>Create dependency on multiple tasks using list and the tasks are callables</li> <li>Create dependency on multiple tasks using list but one task is a callable and another is a string</li> <li>Create dependency on multiple tasks using list and tasks are passed as string. \"custom_z_gold\" is the task name that    is explicitly defined - should not use \"z_gold\" which is a function name</li> </ol>"},{"location":"tasks/#task-parameters","title":"Task parameters","text":"<p>Task parameters can be defined as key value pairs in the function definition on which task is defined</p> task_parameters<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task\ndef task_function(*, test=\"var\", test1=\"var1\"):  # (1)!\nprint(test)\nprint(test1)\n</code></pre> <ol> <li>To pass the task specific parameters, need to start with \"*\" and then key value pairs start</li> </ol>"},{"location":"tasks/#common-task-parameters","title":"Common task parameters","text":"<p>In the workflows section, we saw how the common task parameters are created at the workflow level. Now in this section, we shall see how to use the common task parameters</p> use_common_task_parameters<pre><code>from brickflow import Workflow, ctx\nwf = Workflow(...)\n@wf.task\ndef common_params():\nimport some_pyspark_function  # (1)!\ncatalog_env = ctx.get_parameter(key=\"catalog\", debug=\"local\")  # (2)!\nsome_pyspark_function(catalog_env)  # (3)!\n</code></pre> <ol> <li>It is recommended to use localized imports in tasks rather than the global imports</li> <li>Brickflow provides the context using which we can fetch the task parameters that are defined. Providing debug is    mandatory or else there will be a compilation error while deploying</li> <li>The extracted task_parameter_value can be used as any python variable. In this example, we are just passing the    variable to \"some_pyspark_function\"</li> </ol>"},{"location":"tasks/#inbuilt-task-parameters","title":"Inbuilt task parameters","text":"<p>There are many inbuilt task parameters that be accessed using brickflow context like above</p> inbuilt_task_parameters<pre><code>from brickflow import Workflow, ctx\nwf = Workflow(...)\n@wf.task\ndef inbuilt_params():\nprint(ctx.get_parameter(\nkey=\"brickflow_env\",  # (1)! \ndebug=\"local\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_run_id\",  # (2)! \ndebug=\"788868\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_job_id\",  # (3)! \ndebug=\"987987987987987\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_start_date\",  # (4)! \ndebug=\"2023-05-03\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_start_time\",  # (5)! \ndebug=\"1683102411626\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_task_retry_count\",  # (6)! \ndebug=\"2\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_parent_run_id\",  # (7)! \ndebug=\"788869\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_task_key\",  # (8)! \ndebug=\"inbuilt_params\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_workflow_name\",  # (9)! \ndebug=\"Sample_Workflow\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_task_name\",  # (10)! \ndebug=\"inbuilt_params\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_workflow_prefix\",  # (11)! \ndebug=\"inbuilt_params\"))\nprint(ctx.get_parameter(\nkey=\"brickflow_internal_workflow_suffix\",  # (12)! \ndebug=\"inbuilt_params\"))\n</code></pre> <ol> <li>\"brickflow_env\" holds the value of the --env variable which was used when brickflow is deployed</li> <li>\"brickflow_run_id\" holds the value of the current task run id</li> <li>\"brickflow_job_id\" holds the value of the current workflow job id</li> <li>\"brickflow_start_date\" holds the value of the current workflow start date</li> <li>\"brickflow_start_time\" holds the value of the current task start time</li> <li>\"brickflow_task_retry_count\" holds the value of number of retries a task can run, when a failure occurs</li> <li>\"brickflow_parent_run_id\" hold the value of the current workflow run_id</li> <li>\"brickflow_task_key\" holds the value of the current task name</li> <li>\"brickflow_internal_workflow_name\" holds the value of the current workflow name</li> <li>\"brickflow_internal_task_name\" holds the value of the current task name</li> <li>\"brickflow_internal_workflow_prefix\" holds the value of the prefix used for the current workflow name</li> <li>\"brickflow_internal_workflow_suffix\" holds the value of the suffix used for the current workflow name</li> </ol>"},{"location":"tasks/#clusters","title":"Clusters","text":"<p>There is a flexibility to use different clusters for each task or assign custom clusters</p> clusters<pre><code>from brickflow import Workflow, Cluster\nwf = Workflow(...)\n@wf.task(cluster=Cluster(...))  # (1)!\ndef custom_cluster():\npass\n</code></pre> <ol> <li>You will be able to create a job cluster or use existing cluster. Refer to this section in    the workflows to understand how to implement</li> </ol>"},{"location":"tasks/#libraries","title":"Libraries","text":"<p>There is a flexibility to use specific libraries for a particular task</p> libraries<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task(libraries=[...])  # (1)!\ndef custom_libraries():\npass\n</code></pre> <ol> <li>You will be able to install libraries that are specific to a task. Refer to this section in    the workflows to understand how to implement</li> </ol>"},{"location":"tasks/#task-types","title":"Task types","text":"<p>There are different task types that are supported by brickflow right now. The default task type that is used by brickflow is NOTEBOOK</p> task_types<pre><code>from brickflow import Workflow, TaskType, BrickflowTriggerRule, TaskResponse\nwf = Workflow(...)\n@wf.task\ndef notebook_task():\npass\n@wf.task(task_type=TaskType.DLT)\ndef dlt_task():\npass\n</code></pre> <ol> <li>Provide the task type that is to be used for this task. Default is a notebook task</li> <li>Trigger rule can be attached. It can be ALL_SUCCESS or NONE_FAILED. In this case, this task will be triggered, if all    the upstream tasks are at-least run and completed.</li> </ol>"},{"location":"tasks/#notebook-task","title":"Notebook Task","text":"<p>The <code>Notebook Task</code> is used as a decorator in conjunction with the <code>notebook_task</code> method of a <code>Workflow</code> instance. This method registers the task within the workflow.</p> <p>Here's an example of how to use the <code>Notebook</code> Task type:</p> <pre><code>@wf.task\ndef notebook_task():\npass\n@wf.notebook_task\n# this task runs a databricks notebook\ndef example_notebook():\nreturn NotebookTask(\nnotebook_path=\"notebooks/example_notebook.py\",\nbase_parameters={\n\"some_parameter\": \"some_value\",  # in the notebook access these via dbutils.widgets.get(\"some_parameter\")\n},\n)\n</code></pre> <p>NotebookTask class can accept the following as inputs: base_parameters: Optional[Dict[str, str]] = parameters to pass to notebook and can be accessed through dbutils widgets notebook_path:'The path of the notebook to be run in the Databricks workspace or remote repository.For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash.For notebooks stored in a remote repository, the path must be relative.,      source: Optional[str] :'Optional location type of the Python file. When set to <code>WORKSPACE</code> or not specified, the file will be retrieved from the local  workspace or cloud location (if the <code>python_file</code> has a URI format). When set to <code>GIT</code>,the Python file will be retrieved from a Git repository defined in <code>git_source</code>.<code>WORKSPACE</code>: The Python file is located in a  workspace or at a cloud filesystem URI. <code>GIT</code>: The Python file is located in a remote Git repository.',"},{"location":"tasks/#run-job-task","title":"Run Job Task","text":"<p>The <code>Run Job Task</code> is used as a decorator in conjunction with the <code>run_job_task</code> method of a <code>Workflow</code> instance. This method registers the task within the workflow.</p> <p>Here's an example of how to use the <code>Run Job</code> Task type:</p> <pre><code>from brickflow import RunJobTask\n@wf.run_job_task\ndef run_job_task_a():\nreturn RunJobTask(job_name=\"run_job_task\")\n# we can also pass task type as parameter\n@wf.task(task_type=TaskType.RUN_JOB_TASK)\ndef run_job_task_a():\nreturn RunJobTask(job_name=\"run_job_task\")\n</code></pre> <p>RunJobTask class can accept the following as inputs:</p> <p> job_name: The name of the job (case-insensitive). host [Optional]: The URL of the Databricks workspace. token [Optional]: The Databricks API token.</p> <p>Important</p> <p>Databricks does not natively support triggering the job run in the remote workspace. Only set <code>host</code> and <code>token</code> parameters when remote trigger is required, it will envoke RunJobInRemoteWorkspace plugin which will transparently  substitute the native execution. No extra action will be required from the user.</p>"},{"location":"tasks/#jar-task","title":"JAR Task","text":"<p>The <code>JAR Task</code> is used as a decorator in conjunction with the <code>spark_jar_task</code> method of a <code>Workflow</code> instance. This method registers the task within the workflow. Make sure to upload <code>JAR</code> file into <code>dbfs</code> (or) <code>S3</code> paths and provide relative path of the uploaded jar. Here's an example of how to use the <code>JAR</code> Task type:</p> <pre><code># Example:1\n@wf.spark_jar_task(\nlibraries=[\nJarTaskLibrary(\njar=\"dbfs:/Volumes/development/global_sustainability_dev/raju_spark_jar_test/PrintArgs.jar\"\n)\n]\n)\ndef spark_jar_task_a():\nreturn SparkJarTask(\nmain_class_name=\"PrintArgs\",\nparameters=[\"Hello\", \"World!\"],\n) \n# Example: 2\n@wf.spark_jar_task(\nlibraries=[\nJarTaskLibrary(\njar=\"s3:/Volumes/development/global_sustainability_dev/raju_spark_jar_test/PrintArgs.jar\"\n)\n]\n)\ndef spark_jar_task_test():\nreturn SparkJarTask(\nmain_class_name=\"PrintArgs\",\n)\n# Example: 3\n# we can also pass task type as parameter\n@wf.task(task_type=TaskType.SPARK_JAR_TASK, libraries=[\nJarTaskLibrary(\njar=\"dbfs:/Volumes/development/global_sustainability_dev/raju_spark_jar_test/PrintArgs.jar\"\n)])\ndef run_job_task_a():\nreturn SparkJarTask(main_class_name=\"MainClass\")\n</code></pre> <p>JarTask class can accept the following as inputs:</p> <p> main_class_name: The full name of the class containing the main method to be executed. parameters [Optional]: Parameters passed to the main method.</p>"},{"location":"tasks/#python-task","title":"PYTHON Task","text":"<p>The <code>PYTHON Task</code> is used as a decorator in conjunction with the <code>spark_python_task</code> method of a <code>Workflow</code> instance. This method registers the task within the workflow. Make sure to provide correct path to  <code>PYTHON</code> file from workspace or relative path from git repo. Here's an example of how to use the <code>PYTHON</code> Task type:</p> <pre><code># Example:1\n@wf.spark_python_task(\nlibraries=[\nPypiTaskLibrary(\npackage=\"koheesio\"\n)\n]\n)\ndef spark_python_task_a():\nreturn SparkPythonTask(\npython_file=\"path/to/python/file.py\",\nsource=\"GIT\",\nparameters=[\"--param1\", \"World!\"],\n)\n# Example: 2\n# we can also pass task type as parameter\n@wf.task(task_type=TaskType.SPARK_PYTHON_TASK, libraries=[\nPypiTaskLibrary(\npackage=\"koheesio\"\n)\n]\ndef run_job_task_a():\nreturn SparkPythonTask(\npython_file=\"path/to/python/file.py\",\nsource=\"GIT\",\nparameters=[\"--param1\", \"World!\"],\n)\n</code></pre> <p>PythonTask class can accept the following as inputs:</p> <p> python_file: The Python file to be executed. Cloud file URIs (such as dbfs:/, s3:/, adls:/, gcs:/) and workspace paths are supported. For python files stored in the Databricks workspace, the path must be absolute and begin with <code>/</code>. For files stored in a remote repository, the path must be relative. This field is required.'  source: When set to <code>WORKSPACE</code> or not specified, the file will be retrieved from the local Databricks workspace or cloud location (if the <code>python_file</code> has a URI format). When set to <code>GIT</code>,\\nthe Python file will be retrieved from a Git repository defined in <code>git_source</code>.\\n\\n* <code>WORKSPACE</code>: The Python file is located in a Databricks workspace or at a cloud filesystem URI.\\n* <code>GIT</code>: The Python file is located in a remote Git repository.. parameters [Optional]: Parameters passed to the main method.</p>"},{"location":"tasks/#sql-task","title":"SQL Task","text":"<p>The SqlTask class is used to create SQL tasks in the workflow. It can be used to create tasks with a query ID, file path, alert ID, and dashboard ID. The <code>SQL Task</code> is used as a decorator in conjunction with the <code>sql_task</code> method of a <code>Workflow</code> instance. This method registers the task within the workflow.</p> <p>SQLTask class can accept the following as inputs: query_id[Optional]: A string representing the ID of the query. file_path[Optional]: A string representing the path to the SQL file. alert_id[Optional]: A string representing the ID of the alert. pause_subscriptions[Optional]: A boolean indicating whether to pause subscriptions or not. subscriptions[Optional]: A dictionary containing usernames and destination IDs for subscriptions. dashboard_id[Optional]: A string representing the ID of the dashboard. dashboard_custom_subject[Optional]: A string representing the custom subject of the dashboard.\u00a0 warehouse_id: A string representing the ID of the warehouse.</p> <p>Here's an example of how to use the <code>SQL</code> Task type:</p> <pre><code>@wf.sql_task\ndef sample_sql_task_query():\nreturn SqlTask(\nquery_id=\"your_sql_query_id\", warehouse_id=\"your_warehouse_id\"\n)\n@wf.sql_task\ndef sample_sql_task_file() -&gt; any:\nreturn SqlTask(file_path=\"products/brickflow_test/src/sql/sql_task_file_test.sql\", warehouse_id=\"your_warehouse_id\")\n@wf.sql_task\ndef sample_sql_alert() -&gt; any:\nreturn SqlTask(\nalert_id=\"Your_Alert_ID\",\npause_subscriptions=False,\nsubscriptions={\n\"usernames\": [\"YOUR_USERNAME\", \"YOUR_USERNAME\"]\n},\nwarehouse_id=\"your_warehouse_id\",\n)\n@wf.sql_task\ndef sample_sql_dashboard() -&gt; any:\nreturn SqlTask(\ndashboard_id=\"Your_Dashboard_ID\",\ndashboard_custom_subject=\"Raju Legacy Dashboard Test\",\npause_subscriptions=True,\nsubscriptions={\n\"usernames\": [\"YOUR_USERNAME\", \"YOUR_USERNAME\"],\n\"destination_id\": [\"your_destination_id\"],\n},\nwarehouse_id=\"your_warehouse_id\",\n)\n# we can also pass task type as parameter\n@wf.task(task_type=TaskType.SQL)\ndef sample_sql_dashboard_task() -&gt; any:\nreturn SqlTask(\ndashboard_id=\"Your_Dashboard_ID\",\ndashboard_custom_subject=\"Raju Legacy Dashboard Test\",\npause_subscriptions=True,\nsubscriptions={\n\"usernames\": [\"YOUR_USERNAME\", \"YOUR_USERNAME\"],\n\"destination_id\": [\"your_destination_id\"],\n},\nwarehouse_id=\"your_warehouse_id\",\n)\n</code></pre>"},{"location":"tasks/#ifelse-task","title":"If/Else Task","text":"<p>The <code>IfElseConditionTask</code> class is used to create conditional tasks in the workflow. It can be used to create tasks with a left operand, a right operand, and an operator.</p> <p>The <code>IfElseConditionTask</code> is used as a decorator in conjunction with the <code>if_else_condition_task</code> method of a <code>Workflow</code> instance. This method registers the task within the workflow.</p> <p><code>IfElseConditionTask</code> class can accept the following as inputs:</p> <ul> <li>left[Optional]: A string representing the left operand in the condition.</li> <li>right[Optional]: A string representing the right operand in the condition.</li> <li>operator[Optional]: A string representing the operator used in the condition. It can be one of the following: \"==\", \"!=\", \"&gt;\", \"&lt;\", \"&gt;=\", \"&lt;=\".</li> </ul> <p>Here's an example of how to use the <code>IfElseConditionTask</code> type:</p> <pre><code># Example 1: creating a if/else task with some params\n@wf.if_else_condition_task\ndef sample_if_else_condition_task():\nreturn IfElseConditionTask(\nleft=\"value1\", right=\"value2\", op=\"==\"\n)\n# Let me walk you through how we can make use of if/else condition task. we created a task with name `sample_if_else_condition_task` and it will return either true ot false. Now based on the returned bool, now we're going to decide which task to run. check the below examples.\n# Example 2: creating a if/else task that depends on example 1 and this task only triggers if example 1 returns true.\n@wf.if_else_condition_task(depends_on=\"sample_if_else_condition_task\", name=\"new_conditon_task\", if_else_outcome={\"sample_if_else_condition_task\":\"true\"})\ndef sample_condition_true():\nreturn IfElseConditionTask(\nleft='{{job.id}}',\nop=\"==\",\nright='{{job.id}}')\n'''\n Now i created on more condition task (you can create any task type), since my new task named `new_conditon_task` is dependent on `sample_if_else_condition_task` (if/else task). Now, If my parent tasks runs sucessfully (returns true) then only this task will trigger, cz i mentioned \nif_else_outcome={\"sample_if_else_condition_task\":\"true\"}, to seee the false case see the example below.\n'''\n#Example 3: this task will trigger only when example1 task fails (returns false).\n@wf.if_else_condition_task(depends_on=\"sample_if_else_condition_task\", name=\"example_task_3\", if_else_outcome={\"sample_if_else_condition_task\":\"false\"})\ndef sample_condition_false():\nreturn IfElseConditionTask(\nleft='{{job.trigger.type}}',\nop=\"==\",\nright='{{job.trigger.type}}')\n# Note: As we can have multiple deps same way we can keep multiple deps for if_else_outcome:\n# Ex: if_else_outcome={\"task1\":\"false\", \"task2\":\"true\"}\n# Example 4: creating a SQL Alert Task that depends on example_task_3, now the below sql task will trigger only if above tasks returns true.\n@wf.sql_task(depends_on=\"example_task_3\", if_else_outcome={\"example_task_3\":\"true\"})\ndef sample_sql_alert() -&gt;any:\n# it automatically validates user emails\nreturn SqlTask(alert_id=\"ALERT_ID\", pause_subscriptions=False, subscriptions={\"usernames\":[\"YOUR_EMAIL\", 'YOUR_EMAIL']} ,warehouse_id=\"WAREHOUSE_ID\")\n# Note: Since SQL task doesn't return any bool, we can't make use of if_else_outcome params for the tasks that depends on sql Task\n</code></pre>"},{"location":"tasks/#for-each-task","title":"For each Task","text":"<p>The for each task is used to iterate the execution of a task over a set of input values. Iteration of a task over a set of input values can be achieved decorating your task function with the <code>for_each_task</code> method of the <code>Workflow</code> instance.</p> <p>Iteration of tasks is possible only for task types:</p> <ul> <li>Notebook</li> <li>Spark Jar</li> <li>Python</li> <li>Run Job</li> <li>Sql</li> </ul> <p>The <code>for_each_task</code> decorator can be configured by providing in the <code>for_each_task_conf</code> param a <code>JobsTasksForEachTaskConfigs</code> config in which you specify:</p> <ul> <li>inputs: Optional[str]: the list of input values to iterate over. This can be a python iterable, or a string representing a JSON formatted array of values.</li> <li>for_each_task_concurrency: Optional[int]: the number of concurrent executions of the task. Default is 1.</li> </ul> <p>A reference to the current input value we are iterating on can be accessed using <code>{{input}}</code> databricks workflow parameter.</p> <p>Here are some examples of how to use the for each task type:</p> <pre><code>@wf.for_each_task(\ndepends_on=example_task,\nfor_each_task_conf=JobsTasksForEachTaskConfigs(\n# Inputs can be provided by either a python iterable or a json-string\ninputs=[\n\"AZ\",\n\"CA\",\n\"IL\",\n],\nconcurrency=3,\n),\n)\ndef example_notebook():\nreturn NotebookTask(\nnotebook_path=\"notebooks/example_notebook.py\",\nbase_parameters={\"looped_parameter\": \"{{input}}\"},\n)\n@wf.for_each_task(\ndepends_on=example_task,\nfor_each_task_conf=JobsTasksForEachTaskConfigs(\ninputs='[\"1\", \"2\", \"3\"]', concurrency=3\n),\n)\ndef example_brickflow_task(*, test_param=\"{{input}}\"):\nprint(f\"Test param: {test_param}\")\nparam = ctx.get_parameter(\"looped_parameter\")\nprint(f\"Nested brickflow task running with input: {param}\")\n@wf.for_each_task(\ndepends_on=example_task,\nlibraries=[\nJarTaskLibrary(\njar=\"&lt;dbfs:/some/path/to/The.jar&gt;\"\n) \n],\nfor_each_task_conf=JobsTasksForEachTaskConfigs(\ninputs=\"[1,2,3]\",\nconcurrency=1,\n),\n)\ndef for_each_spark_jar():\nreturn SparkJarTask(\nmain_class_name=\"com.example.MainClass\", \nparameters=[\"{{input}}\"],\n)\n@wf.for_each_task(\ndepends_on=example_task,\nfor_each_task_conf=JobsTasksForEachTaskConfigs(\ninputs=\"[1,2,3]\",\nconcurrency=1,\n),\n)\ndef for_each_spark_python():\nreturn SparkPythonTask(\npython_file=\"src/python/print_args.py\",\nsource=\"WORKSPACE\",\nparameters=[\"{{input}}\"],\n)\n@wf.for_each_task(\ndepends_on=example_notebook,\nfor_each_task_conf=JobsTasksForEachTaskConfigs(\ninputs=\"[1,2,3]\",\nconcurrency=1,\n),\n)\ndef for_each_sql_task() -&gt; any:\nreturn SqlTask(\nquery_id=\"&lt;some-query-id&gt;\", \nwarehouse_id=\"&lt;some-warehouse-id&gt;\", \nparameters={\"looped_parameter\": \"{{input}}\"},\n)\n</code></pre>"},{"location":"tasks/#trigger-rules","title":"Trigger rules","text":"<p>There are two types of trigger rules that can be applied on a task. It can be either ALL_SUCCESS or NONE_FAILED</p> task_types<pre><code>from brickflow import Workflow, BrickflowTriggerRule\nwf = Workflow(...)\n@wf.task(\ntrigger_rule=BrickflowTriggerRule.NONE_FAILED  # (1)!\n)\ndef none_failed_task():\npass\n@wf.task(\ntrigger_rule=BrickflowTriggerRule.ALL_SUCCESS  # (2)!\n)\ndef all_success_task():\npass\n</code></pre> <ol> <li>NONE_FAILED - use this if you want to trigger the task irrespective of the upstream tasks success or failure state</li> <li>ALL_SUCCESS - use this if you want to trigger the task only if all the upstream tasks are all having success state</li> </ol>"},{"location":"tasks/#tasks-conditional-run","title":"Tasks conditional run","text":"<p>Adding condition for task running based on result of parent tasks</p> task_conditional_run<pre><code>from brickflow import Workflow, TaskRunCondition, TaskSettings\nwf = Workflow(...)\n@wf.task(\ntask_settings=TaskSettings(run_if=TaskRunCondition.AT_LEAST_ONE_FAILED)\n)\ndef none_failed_task():\npass\n</code></pre> <p>This option is determining whether the task is run once its dependencies have been completed. Available options:</p> <ol> <li><code>ALL_SUCCESS</code>: All dependencies have executed and succeeded</li> <li><code>AT_LEAST_ONE_SUCCESS</code>: At least one dependency has succeeded</li> <li><code>NONE_FAILED</code>: None of the dependencies have failed and at least one was executed</li> <li><code>ALL_DONE</code>: All dependencies completed and at least one was executed</li> <li><code>AT_LEAST_ONE_FAILED</code>: At least one dependency failed</li> <li><code>ALL_FAILED</code>: ALl dependencies have failed</li> </ol>"},{"location":"tasks/#airflow-operators","title":"Airflow Operators","text":"<p>We have adopted/extended certain airflow operators that might be needed to run as a task in databricks workflows. Typically for airflow operators we return the operator and brickflow will execute the operator based on task return type.</p>"},{"location":"tasks/#bash-operator","title":"Bash Operator","text":"<p>You will be able to use bash operator as below</p> bash_operator<pre><code>from brickflow import Workflow\nfrom brickflow_plugins import BashOperator\nwf = Workflow(...)\n@wf.task\ndef bash_task():\nreturn BashOperator(task_id=bash_task.__name__, \nbash_command=\"ls -ltr\")  # (1)!\n</code></pre> <ol> <li>Use Bashoperator like how we use in airflow but it has to be returned from task function</li> </ol>"},{"location":"tasks/#task-dependency-sensor","title":"Task Dependency Sensor","text":"<p>Even if you migrate to databricks workflows, brickflow gives you the flexibility to have a dependency on the airflow job</p> task_dependency_sensor<pre><code>from brickflow import Workflow, ctx\nfrom brickflow_plugins import TaskDependencySensor, AirflowProxyOktaClusterAuth\nwf = Workflow(...)\n@wf.task\ndef airflow_external_task_dependency_sensor():\nimport base64\nfrom datetime import timedelta\ndata = base64.b64encode(\nctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"okta_conn_id\").encode(\n\"utf-8\"\n)\n).decode(\"utf-8\")\nreturn TaskDependencySensor(\ntask_id=\"sensor\",\ntimeout=180,\nairflow_cluster_auth=AirflowProxyOktaClusterAuth(\noauth2_conn_id=f\"b64://{data}\",\nairflow_cluster_url=\"https://proxy.../.../cluster_id/\",\nairflow_version=\"2.0.2\", # if you are using airflow 1.x please make sure this is the right value, the apis are different between them!\n),\nexternal_dag_id=\"external_airlfow_dag\",\nexternal_task_id=\"hello\",\nallowed_states=[\"success\"],\nexecution_delta=timedelta(hours=-2),\nexecution_delta_json=None,\npoke_interval= 60,\n)\n</code></pre>"},{"location":"tasks/#autosys-sensor","title":"Autosys Sensor","text":"<p>This operator calls an Autosys API and is used to place a dependency on Autosys jobs, when necessary.</p> autosys_sensor<pre><code>from brickflow import Workflow, ctx\nfrom brickflow_plugins import AutosysSensor, AirflowProxyOktaClusterAuth\nwf = Workflow(...)\n@wf.task\ndef airflow_autosys_sensor():\nimport base64\ndata = base64.b64encode(\nctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"okta_conn_id\").encode(\n\"utf-8\"\n)\n).decode(\"utf-8\")\nreturn AutosysSensor(\ntask_id=\"sensor\",\nurl=\"https://autosys.../.../api/\",\nairflow_cluster_auth=AirflowProxyOktaClusterAuth(\noauth2_conn_id=f\"b64://{data}\",\nairflow_cluster_url=\"https://autosys.../.../api/\",\nairflow_version=\"2.0.2\", \n),\npoke_interval=200,\njob_name=\"hello\",\ntime_delta={\"days\": 0},\n)\n</code></pre>"},{"location":"tasks/#workflow-dependency-sensor","title":"Workflow Dependency Sensor","text":"<p>Wait for a workflow to finish before kicking off the current workflow's tasks</p> workflow_dependency_sensor<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import WorkflowDependencySensor\nwf = Workflow(...)\n@wf.task\ndef wait_on_workflow(*args):\napi_token_key = ctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"api_token_key\")\nsensor = WorkflowDependencySensor(\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ndependency_job_id=job_id,\npoke_interval=20,\ntimeout=60,\ndelta=timedelta(days=1)\n)\nsensor.execute()\n</code></pre>"},{"location":"tasks/#workflow-task-dependency-sensor","title":"Workflow Task Dependency Sensor","text":"<p>Wait for a specific task in a workflow to finish before kicking off the current workflow's tasks</p> workflow_dependency_sensor<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import WorkflowTaskDependencySensor\nwf = Workflow(...)\n@wf.task\ndef wait_on_workflow(*args):\napi_token_key = ctx.dbutils.secrets.get(\"scope\", \"api_token_key\")\nsensor = WorkflowTaskDependencySensor(\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ndependency_job_id=job_id,\ndependency_task_name=\"foo\",\npoke_interval=20,\ntimeout=60,\ndelta=timedelta(days=1)\n)\nsensor.execute()\n</code></pre>"},{"location":"tasks/#snowflake-operator","title":"Snowflake Operator","text":"<p>Run snowflake queries from the databricks environment</p> <p>As databricks secrets is a key value store, code expects the secret scope to contain the below exact keys</p> <ul> <li><code>username</code> : user id created for connecting to snowflake for ex: sample_user  </li> <li><code>password</code> : password information for about user for ex: P@$$word  </li> <li><code>account</code>  : snowflake account information, not entire url for ex: sample_enterprise  </li> <li><code>warehouse</code>: warehouse/cluster information that user has access for ex: sample_warehouse  </li> <li><code>database</code> : default database that we want to connect for ex: sample_database  </li> <li><code>role</code>     : role to which the user has write access for ex: sample_write_role</li> </ul> <p>SnowflakeOperator can accept the following as inputs</p> <ul> <li><code>secret_scope</code> : (required) databricks secret scope identifier</li> <li><code>query_string</code> : (required) queries separated by semicolon</li> <li><code>sql_file</code> : (optional) path to the sql file</li> <li><code>parameters</code> : (optional) dictionary with variables that can be used to substitute in queries</li> <li><code>fail_on_error</code> : (optional) bool to fail the task if there is a sql error, default is True</li> </ul> <p>Operator only takes one of either query_string or sql_file needs to be passed</p> snowflake_operator using sql queries<pre><code>from brickflow_plugins import SnowflakeOperator\nwf = Workflow(...)\n@wf.task\ndef run_snowflake_queries(*args):\nsf_query_run = SnowflakeOperator(\nsecret_scope = \"your_databricks secrets scope name\",\nquery_string =\"select * from database.$schema.$table where $filter_condition1; select * from sample_schema.test_table\",\nparameters = {\"schema\":\"test_schema\",\"table\":\"sample_table\",\"filter_condition\":\"col='something'\"},\nfail_on_error=True,\n)\nsf_query_run.execute()\n</code></pre> snowflake_operator_using_sql_files<pre><code>#Sql file path is relative from the brickflow project root (Ex: root/products/{product_name})\n@wf.task\ndef run_snowflake_files(*args):\nsf_file_run = SnowflakeOperator(\nsecret_cope=\"sample_scope\",\nsql_file=f\"src/sql/sample.sql\",\nparameters={\"database\": \"sample_db\"},\n)\nsf_file_run.execute()\n</code></pre>"},{"location":"tasks/#uc-to-snowflake-operator","title":"UC to Snowflake Operator","text":"<p>copy data from databricks to snowflake</p> <p>As databricks secrets is a key value store, code expects the secret scope to contain the below exact keys - <code>username</code> : user id created for connecting to snowflake for ex: sample_user - <code>password</code> : password information for about user for ex: P@$$word - <code>account</code>  : snowflake account information, not entire url for ex: sample_enterprise - <code>warehouse</code>: warehouse/cluster information that user has access for ex: sample_warehouse - <code>database</code> : default database that we want to connect for ex: sample_database - <code>role</code>     : role to which the user has write access for ex: sample_write_role</p> <p>UcToSnowflakeOperator can expects the following as inputs to copy data in parameters one of Either dbx_sql or (dbx_catalog, dbx_database, dbx_table ) needs to be provided - <code>load_type</code>: (required) type of data load , acceptable values full or incremental - <code>dbx_catalog</code>: (optional) name of the databricks catalog in which object resides - <code>dbx_database</code>: (optional) name of the databricks schema in which object is available - <code>dbx_table</code>: (optional) name of the databricks object we want to copy to snowflake - <code>dbx_sql</code>: (optional) Custom sql to extract data from databricks Unity Catalog - <code>sf_database</code>: (optional) name of the snowflake database if different from the one in secret_scope - <code>sf_schema</code>: (required) name of the snowflake schema in which we want to copy the data - <code>sf_table</code>: (required) name of the snowflake object to which we want to copy from databricks - <code>incremental_filter</code>: (required for incrmental mode) condition to manage data before writing to snowflake - <code>dbx_data_filter</code>: (optional) filter condition on databricks source for full or incremental (if different from inremental_filter) - <code>sf_grantee_roles</code>: (optional) snowflake roles to which we want to grant select/read access, can be a comma seperated string - <code>sf_cluster_keys</code>: (optional) list of keys we want to cluster our snowflake table. - <code>write_mode</code>: (optional) write mode to write into snowflake table ( overwrite, append etc)</p> uc_to_snowflake_operator<pre><code>from brickflow_plugins import UcToSnowflakeOperator\nwf = Workflow(...)\n@wf.task\ndef run_snowflake_queries(*args):\nuc_to_sf_copy = UcToSnowflakeOperator(\nsecret_scope = \"your_databricks secrets scope name\",\nwrite_mode =\"overwrite\",\nparameters = {'load_type':'incremental','dbx_catalog':'sample_catalog','dbx_database':'sample_schema',\n'dbx_table':'sf_operator_1', 'sf_schema':'stage','sf_table':'SF_OPERATOR_1',\n'sf_grantee_roles':'downstream_read_role1,downstream_read_role2', 'incremental_filter':\"dt='2023-10-22'\",\n'sf_cluster_keys':'', 'dbx_sql':'Custom sql query to read data from UC'}\n)\nuc_to_sf_copy.execute()\n</code></pre>"},{"location":"tasks/#tableau-refresh-operators","title":"Tableau Refresh Operators","text":"<p>Connect to the Tableau server and trigger the refresh of the data sources or workbooks.</p> <p>Tableau client uses object GUIDs  to identify objects on the server. At the same time the server does not enforce unique names for the objects across the server. This means that multiple objects, e.g. data sources, with the same name can exist on the server.</p> <p>To overcome this, operators are using the combination of <code>project</code> and <code>parent_project</code> parameters to uniquely identify the project that owns data source or workbook on the server. Successfull project resolution will be indicated in the logs as follows:</p> <pre><code>INFO - Querying all projects on site\n\nParent project identified:\n Name: My Parent Project\n ID: 2e14e111-036f-409e-b536-fb515ee534b9\nWorking project identified:\n Name: My Project\n ID: 2426e01f-c145-43fc-a7f6-1a7488aceec0\n</code></pre> <p>If project resolution is successful the refresh will be triggered and operator will poll the server for the refresh status:</p> <pre><code>Triggering refresh of 'my-datasource' datasource...\nQuery for information about job c3263ad0-1340-444d-8128-24ad742a943a\nData source 'my-datasource' refresh status: \n   { \n      'job_id': 'c3263ad0-1340-444d-8128-24ad742a943a', \n      'job_status': 'Success', \n      'finish_code': 0, \n      'started_at': '2024-02-05 20:36:03 UTC', \n      'completed_at': '2024-02-05 20:41:32 UTC', \n      'job_status_details': None\n   }!\n</code></pre> <p>If refresh fails, <code>job_status_details</code> will contain the error message retrieved from the server and the operator will fail. If fail behavior is not desired, <code>fail_operator = False</code> can be set in the operator parameters.</p> tableau_refresh_operators<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import TableauRefreshDataSourceOperator, TableauRefreshWorkBookOperator\nwf = Workflow(...)\n@wf.task\ndef tableau_refresh_datasource():\nreturn TableauRefreshDataSourceOperator(\nserver=\"https://my-tableau.com\",\nusername=\"foo\",\npassword=\"bar\",\nsite=\"site\",\nproject=\"project\",\ndata_sources=[\"datasource1\", \"datasource2\"],\n)\n@wf.task\ndef tableau_refresh_workbook():\nreturn TableauRefreshWorkBookOperator(\nserver=\"https://my-tableau.com\",\nusername=\"foo\",\npassword=\"bar\",\nsite=\"site\",\nproject=\"project\",\nworkbooks=[\"workbook1\", \"workbook2\"],\n)\n</code></pre>"},{"location":"tasks/#box-operators","title":"Box Operators","text":"<p>The Box Operator provides a comprehensive solution for authenticating with Box and efficiently managing file transfers between Box and Databricks Unity Catalog (UC) volumes.  It includes classes for downloading and uploading files, leveraging JWT authentication via BoxAuthenticator.</p> <p>To properly authenticate with Box using JWT within the Databricks or Cerberus environments, ensure that your secrets scope contains the following exact keys:</p> <p><code>Dbutils or Cerberus</code> The secrets scope should contain the following keys for Box authentication:</p> <ul> <li><code>client_id</code>: <code>your_client_id</code> The client ID for the Box application.</li> <li><code>client_secret</code>: <code>your_client_secret</code> The client secret for the Box application.</li> <li><code>jwt_key_id</code>: <code>your_jwt_key_id</code> The JWT key ID for the Box application.</li> <li><code>rsa_private_key_data</code>: <code>your_rsa_private_key_data</code> The RSA private key data for the Box application. This is encoded in UTF-8.</li> <li><code>rsa_private_key_passphrase</code>: <code>your_rsa_private_key_passphrase</code> The passphrase for the RSA private key.</li> <li><code>enterprise_id</code>: <code>your_enterprise_id</code> The enterprise ID for the Box application.</li> </ul> <p>The <code>BoxToVolumesOperator</code> downloads files from Box to Databricks Unity Catalog (UC) volume.</p> <p>The <code>VolumesToBoxOperator</code> uploads files from a Databricks Unity Catalog (UC) volume to a Box folder.</p> <p>The <code>BoxOperator</code> manages the high-level operations for interacting with Box (download/upload).</p> <p><code>BoxToVolumesOperator, VolumesToBoxOperator and BoxOperator Parameters:</code></p> <ul> <li><code>secret_scope</code>: (required) The scope within Databricks or Cerberus where the secrets are stored.</li> <li><code>cerberus_client_url</code>: (optional) The URL for the Cerberus client, used to retrieve secrets if not found in the secret_scope.</li> <li><code>folder_id</code>: (required) The ID of the Box folder from which files will be downloaded.</li> <li><code>volume_path</code>: (required) The local path to the volume where files will be downloaded.</li> <li><code>file_names</code>: (optional) A list of specific file names to be downloaded. If not specified, all files in the folder will be downloaded.</li> <li><code>file_pattern</code>: (optional) The pattern to match file names starting with or ending with the given file pattern to be downloaded or uploaded.</li> <li><code>file_id</code>: (optional for BoxToVolumesOperator) The ID of a specific file to be downloaded. If specified, only this file will be downloaded. This parameter is not used in VolumesToBoxOperator.</li> <li><code>operation</code>: (required only for BoxOperator) Specifies the operation to be performed: <code>\"download\"</code> or <code>\"upload\"</code>.</li> </ul> box_operators<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import BoxToVolumesOperator, VolumeToBoxOperator, BoxOperator\nwf = Workflow(...)\n@wf.task\ndef box_to_volume():\nbox_to_volume_copy = BoxToVolumesOperator(\nsecret_scope=\"my_secret_scope\",\ncerberus_client_url=\"https://cerberus-url.com\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\nfile_id=\"678910\",\n)\nbox_to_volume_copy.execute()\n@wf.task\ndef volume_to_box():\nvolumes_to_box_copy = VolumesToBoxOperator(\nsecret_scope=\"my_secret_scope\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\n)\nvolumes_to_box_copy.execute()\n@wf.task\ndef download_box_to_volume():\ndownload_box_to_volume_copy = BoxOperator(\nsecret_scope=\"my_secret_scope\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\nfile_id=\"678910\",\noperation=\"download\",\n)\ndownload_box_to_volume_copy.execute()\n@wf.task\ndef upload_volume_to_box():\nupload_volumes_to_box_copy = BoxOperator(\nsecret_scope=\"my_secret_scope\",\ncerberus_client_url=\"https://cerberus-url.com\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\noperation=\"upload\",\n)\nupload_volumes_to_box_copy.execute()\n</code></pre> <p>Check operator logs for more details on the status of the connection and the refresh.</p>"},{"location":"workflows/","title":"Workflows","text":"<p>A Workflow is similar to an Airflow dag that lets you encapsulate a set of tasks.</p> <p>Here is an example of a workflow. Click the plus buttons to understand all the parts of the workflow file.</p> workflow.py<pre><code>from datetime import timedelta\nfrom brickflow import (Workflow, Cluster, WorkflowPermissions, User, \nTaskSettings, EmailNotifications, PypiTaskLibrary, MavenTaskLibrary, JobsParameters)\nwf = Workflow(  # (1)!\n\"wf_test\",  # (2)!\ndefault_cluster=Cluster.from_existing_cluster(\"your_existing_cluster_id\"),  # (3)!\n# Optional parameters below\nschedule_quartz_expression=\"0 0/20 0 ? * * *\",  # (4)!\ntimezone=\"UTC\",  # (5)!\nschedule_pause_status=\"PAUSED\",  # (15)!\ndefault_task_settings=TaskSettings(  # (6)!\nemail_notifications=EmailNotifications(\non_start=[\"email@nike.com\"],\non_success=[\"email@nike.com\"],\non_failure=[\"email@nike.com\"],\non_duration_warning_threshold_exceeded=[\"email@nike.com\"]\n),\ntimeout_seconds=timedelta(hours=2).seconds\n),\nlibraries=[  # (7)!\nPypiTaskLibrary(package=\"requests\"),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\n],\ntags={  # (8)!\n\"product_id\": \"brickflow_demo\",\n\"slack_channel\": \"nike-sole-brickflow-support\"\n},\nmax_concurrent_runs=1,  # (9)!\npermissions=WorkflowPermissions(  # (10)!\ncan_manage_run=[User(\"abc@abc.com\")],\ncan_view=[User(\"abc@abc.com\")],\ncan_manage=[User(\"abc@abc.com\")],\n),\nprefix=\"feature-jira-xxx\",  # (11)!\nsuffix=\"_qa1\",  # (12)!\ncommon_task_parameters={  # (13)!\n\"catalog\": \"development\",\n\"database\": \"your_database\"\n},\nhealth = { # (16)!\n\"metric\": \"RUN_DURATION_SECONDS\",\n\"op\": \"GREATER_THAN\",\n\"value\": 7200\n},\nparameters=[JobsParameters(default=\"INFO\", name=\"jp_logging_level\")],  # (18)!\n)\n@wf.task()  # (14)!\ndef task_function(*, test=\"var\"):\nreturn \"hello world\"\n</code></pre> <ol> <li>Workflow definition which constructs the workflow object</li> <li>Define the workflow name</li> <li>The default cluster used for all the tasks in the workflow. This is an all-purpose cluster, but you can also create a job cluster</li> <li>Cron expression in the quartz format</li> <li>Define the timezone for your workflow. It is defaulted to UTC</li> <li>Default task setting that can be used for all the tasks</li> <li>Libraries that need to be installed for all the tasks</li> <li>Tags for the resulting workflow and other objects created during the workflow.</li> <li>Define the maximum number of concurrent runs</li> <li>Define the permissions on the workflow</li> <li>Prefix for the name of the workflow</li> <li>Suffix for the name of the workflow</li> <li>Define the common task parameters that can be used in all the tasks</li> <li>Define a workflow task and associate it to the workflow</li> <li>Define the schedule pause status. It is defaulted to \"UNPAUSED\"</li> <li>Define health check condition that triggers duration warning threshold exceeded notifications</li> <li>Define timeout_seconds check condition that triggers workflow failure if duration exceeds threshold</li> <li>Define the parameters on workflow level databricks docs</li> </ol>"},{"location":"workflows/#clusters","title":"Clusters","text":"<p>There are two ways to define the cluster for the workflow or a task</p>"},{"location":"workflows/#using-an-existing-cluster","title":"Using an existing cluster","text":"existing_cluster<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster.from_existing_cluster(\"your_existing_cluster_id\")\n</code></pre>"},{"location":"workflows/#use-a-job-cluster","title":"Use a job cluster","text":"job_cluster<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nnode_type_id='m6g.xlarge',\ndriver_node_type_id='m6g.xlarge',\nmin_workers=1,\nmax_workers=3,\nenable_elastic_disk=True,\npolicy_id='your_policy_id',\naws_attributes={\n\"first_on_demand\": 1,\n\"availability\": \"SPOT_WITH_FALLBACK\",\n\"instance_profile_arn\": \"arn:aws:iam::XXXX:instance-profile/XXXX/group/XX\",\n\"spot_bid_price_percent\": 100,\n\"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n\"ebs_volume_count\": 3,\n\"ebs_volume_size\": 100\n}\n)\n</code></pre>"},{"location":"workflows/#use-a-job-cluster-with-driver-and-worker-instances-from-the-same-pool","title":"Use a job cluster with driver and worker instances from the same pool","text":"job_cluster_driver_workers_same_pool<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nmin_workers=1,\nmax_workers=3,\ninstance_pool_id=\"your_instance_pool_id\",\npolicy_id='policy_id_of_pool',\n)\n</code></pre>"},{"location":"workflows/#use-a-job-cluster-with-driver-and-worker-instances-from-different-pools","title":"Use a job cluster with driver and worker instances from different pools","text":"job_cluster_driver_workers_same_pool<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nmin_workers=1,\nmax_workers=3,\ninstance_pool_id=\"your_workers_instance_pool_id\",\ndriver_instance_pool_id=\"your_driver_instance_pool_id\",\npolicy_id='policy_id_of_pools',\n)\n</code></pre>"},{"location":"workflows/#permissions","title":"Permissions","text":"<p>Brickflow provides an opportunity to manage permissions on the workflows. You can provide individual users or to a group or to a ServicePrincipal that can help manage, run or view the workflows.</p> <p>Below example is for reference</p> manage_permissions<pre><code>from brickflow import WorkflowPermissions, User, Group, ServicePrincipal\npermissions=WorkflowPermissions(\ncan_manage_run=[\nUser(\"abc@abc.com\"), \nGroup(\"app.xyz.team.Developer\"), \nServicePrincipal(\"ServicePrinciple_dbx_url.app.xyz.team.Developer\")\n],\ncan_view=[User(\"abc@abc.com\")],\ncan_manage=[User(\"abc@abc.com\")],\n)\n</code></pre>"},{"location":"workflows/#tags","title":"Tags","text":"<p>Using brickflow, custom tags can be created on the workflow - but there are also some default tags that are created while the job is deployed.</p> <p>The defaults tags that gets automatically attached to the workflow are below</p> <ul> <li>\"brickflow_project_name\" : Brickflow Project Name that is referred from the entrypoint.py file</li> <li>\"brickflow_version\" : Brickflow Version that is used to deploy the workflow</li> <li>\"databricks_tf_provider_version\" : Databricks terraform provider version that is used to deploy the workflow</li> <li>\"deployed_by\" : Email id of the profile that is used to deploy the workflow.    It can be a user or a service principle. Whichever id is used to deploy the workflow, automatically becomes the    owner of the workflow</li> <li>\"environment\" : Environment to which the workflow is identified to</li> </ul> <p>Use the below reference to define more tags and attach to the workflow. These can be used for collecting various metrics and build dashboards.</p> configure_tags<pre><code>tags={\n\"product_id\": \"brickflow_demo\",\n\"slack_channel\": \"nike-sole-brickflow-support\"\n}\n</code></pre>"},{"location":"workflows/#schedule","title":"Schedule","text":"<p>Databricks workflows uses Quartz cron expression unlike airflow's unix based cron scheduler. A typical Quartz cron expression have six or seven fields, seperated by spaces</p> <pre><code>second minute hour day_of_month month day_of_week year(optional)\n</code></pre> <p>Below is a sample</p> quartz_cron_expression<pre><code>schedule_quartz_expression=\"0 0/20 0 ? * * *\"\n</code></pre>"},{"location":"workflows/#tasksettings","title":"Tasksettings","text":"<p>Task setting at workflow level can be used to have common setting defined that will be applicable for all the tasks. Below is a sample that can be used for reference and all the parameters in TaskSettings are optional</p> task_settings<pre><code>from datetime import timedelta\nfrom brickflow import TaskSettings, EmailNotifications\ndefault_task_settings=TaskSettings(\nemail_notifications=EmailNotifications(\non_start=[\"email@nike.com\"],\non_success=[\"email@nike.com\"],\non_failure=[\"email@nike.com\"],\non_duration_warning_threshold_exceeded=[\"email@nike.com\"]      \n),\ntimeout_seconds=timedelta(hours=2).seconds,\nmax_retries=2,\nmin_retry_interval_millis=60000,\nretry_on_timeout=True\n)\n</code></pre>"},{"location":"workflows/#libraries","title":"Libraries","text":"<p>Brickflow allows to specify libraries that are need to be installed and used across different tasks. There are many ways to install library from different repositories/sources</p> libraries<pre><code>from brickflow import PypiTaskLibrary, MavenTaskLibrary, StorageBasedTaskLibrary, \\\n    JarTaskLibrary, EggTaskLibrary, WheelTaskLibrary\nlibraries=[\nPypiTaskLibrary(package=\"requests\"),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\nStorageBasedTaskLibrary(\"s3://...\"),\nStorageBasedTaskLibrary(\"dbfs://...\"),\nJarTaskLibrary(\"s3://...\"),\nJarTaskLibrary(\"dbfs://...\"),\nEggTaskLibrary(\"s3://...\"),\nEggTaskLibrary(\"dbfs://...\"),\nWheelTaskLibrary(\"s3://...\"),\nWheelTaskLibrary(\"dbfs://...\"),\n]\n</code></pre>"},{"location":"workflows/#common-task-parameters","title":"Common task parameters","text":"<p>Define the common parameters that can be used in all the tasks. Example could be database name, secrets_id etc</p> common_task_parameters<pre><code>common_task_parameters={\n\"catalog\": \"development\",\n\"database\": \"your_database\"\n}\n</code></pre>"},{"location":"workflows/#jobs-parameters","title":"Jobs Parameters","text":"<p>You can configure parameters on a job that are passed to any of the job\u2019s tasks that accept key-value parameters, including Python wheel files configured to accept keyword arguments. Parameters set at the job level are added to configured task-level parameters. Job parameters passed to tasks are visible in the task configuration, along with any parameters configured on the task.</p> <p>You can also pass job parameters to tasks that are not configured with key-value parameters such as JAR or Spark Submit tasks. To pass job parameters to these tasks, format arguments as {{job.parameters.[name]}}, replacing [name] with the key that identifies the parameter.</p> <p>Job parameters take precedence over task parameters. If a job parameter and a task parameter have the same key, the job parameter overrides the task parameter.</p> <p>You can override configured job parameters or add new job parameters when you run a job with different parameters or repair a job run.</p> jobs_parameters<pre><code>    parameters=[JobsParameters(default=\"INFO\", name=\"jp_logging_level\")]\n</code></pre>"},{"location":"faq/faq/","title":"FAQ","text":"","boost":3},{"location":"faq/faq/#how-do-i-enable-airflow-features","title":"How do I enable airflow features?","text":"<p>Warning</p> <p>Only certain operators are supported so please make sure you read the documentation before using them. If your operator is not supported  please raise a new issue in github.</p> <p>Supported Operators:</p> <ul> <li>BranchPythonOperator</li> <li>PythonOperator</li> <li>BashOperator</li> <li>ShortCircuitOperator</li> <li>TaskDependencySensor</li> <li>BoxOperator</li> </ul> <p>To enable the usage of airflow operators, you need to set the <code>enable_plugins</code> flag to <code>True</code> in the <code>Project</code> constructor.</p>","boost":3},{"location":"faq/faq/#how-do-i-run-only-one-task-in-a-workflow","title":"How do I run only one task in a workflow?","text":"<p>Databricks and Airflow use different task scheduling mechanisms. Due to the way airflow manages state in a database, it is possible to run only one task in a workflow. Though this works very differently at Databricks as our job scheduler is very different and needs to scale to much large volume of tasks and workflows.</p> <p>To provide this capability, brickflow offers a parameter to do this:</p> <ol> <li>Go to workflow UI</li> <li>On the top right click the drop down next to <code>Run Now</code></li> <li>Select <code>Run now with different parameters</code></li> <li>Then select switch to legacy parameters</li> <li>Scroll down to: <code>brickflow_internal_only_run_tasks</code> and then type the tasks you want to run as a CSV.</li> <li>Please do not modify any other parameter!</li> </ol>","boost":3},{"location":"faq/faq/#how-do-i-wait-for-a-workflow-to-finish-before-kicking-off-my-own-workflows-tasks","title":"How do I wait for a workflow to finish before kicking off my own workflow's tasks?","text":"<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import WorkflowDependencySensor\nwf = Workflow(...)\n@wf.task\ndef wait_on_workflow(*args):\napi_token_key = ctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"api_token_key\")\nsensor = WorkflowDependencySensor(\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ndependency_job_id=job_id,\npoke_interval=20,\ntimeout=60,\ndelta=timedelta(days=1)\n)\nsensor.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-wait-for-a-specific-task-in-a-workflow-to-finish-before-kicking-off-my-own-workflows-tasks","title":"How do I wait for a specific task in a workflow to finish before kicking off my own workflow's tasks?","text":"<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import WorkflowTaskDependencySensor\nwf = Workflow(...)\n@wf.task\ndef wait_on_workflow(*args):\napi_token_key = ctx.dbutils.secrets.get(\"scope\", \"api_token_key\")\nsensor = WorkflowTaskDependencySensor(\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ndependency_job_id=job_id,\ndependency_task_name=\"foo\",\npoke_interval=20,\ntimeout=60,\ndelta=timedelta(days=1)\n)\nsensor.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-monitor-the-sla-on-a-workflow","title":"How do I monitor the SLA on a workflow?","text":"","boost":3},{"location":"faq/faq/#provide-a-datetime-object-with-utc-specified-as-the-timezone","title":"Provide a datetime object with UTC specified as the timezone","text":"<pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import SLASensor\nfrom datetime import datetime, timezone\nwf = Workflow(...)\n@wf.task\ndef sla_sensor(*args):\napi_token_key = ctx.dbutils.secrets.get(\"scope\", \"api_token_key\")\nsensor = SLASensor(\nmonitored_task_name=\"end\",\nenv=\"dev\",\ndata_product=\"product_name\",\nrun_date=\"2024-01-01\",\nsla_sensor_task_names=[\"sla_sensor\"],\nexpected_sla_timestamp=datetime(2024, 1, 1, 10, 0, 0, tzinfo=pytz.utc),\ndependency_job_name=\"target_job_name\",\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ncustom_description=\"message to provide additional context\",\nslack_webhook_url=\"https://hooks.slack.com/your/webhook/url\",\nemail_params={\n\"email_list\": [\"recipient_1@email.com, recipient_2@email.com\"],\n\"sender_address\": \"sender@email.com\",\n\"cc\": [\"cc_1@email.com, cc_2@email.com\"],\n\"port\": 25,\n\"host\": \"your.email.host\",\n\"priority\": \"1\",\n},\ntimeout_seconds=120\n)\n# returns \"SLA Missed\" or \"SLA Met\"\nresponse = sensor.monitor()\n</code></pre>","boost":3},{"location":"faq/faq/#tag-your-workflow-and-pass-the-tag-to-the-sensor","title":"Tag your workflow and pass the tag to the sensor","text":"<p>This approach will only work for explicit times provided to the minute and hour fields. Ranges and intervals in the minute and hour fields are not supported.</p> <pre><code>from brickflow.context import ctx\nfrom brickflow_plugins import SLASensor\nfrom datetime import datetime, timezone\nwf = Workflow(...)\n'''\nValue for tag key must END with a 5-place quartz cron expression, e.g. \"0 10 @ @ @\"\n'''\n@wf.task\ndef sla_sensor(*args):\napi_token_key = ctx.dbutils.secrets.get(\"scope\", \"api_token_key\")\nsensor = SLASensor(\nmonitored_task_name=\"end\",\nenv=\"dev\",\ndata_product=\"product_name\",\nrun_date=\"2024-01-01\",\nsla_sensor_task_names=[\"sla_sensor\"],\nsla_tag_key=\"My_Tag_Key\",\ndependency_job_name=\"target_job_name\",\ndatabricks_host=\"https://your_workspace_url.cloud.databricks.com\",\ndatabricks_token=api_token_key,\ncustom_description=\"message to provide additional context\",\nslack_webhook_url=\"https://hooks.slack.com/your/webhook/url\",\nemail_params={\n\"email_list\": [\"recipient_1@email.com, recipient_2@email.com\"],\n\"sender_address\": \"sender@email.com\",\n\"cc\": [\"cc_1@email.com, cc_2@email.com\"],\n\"port\": 25,\n\"host\": \"your.email.host\",\n\"priority\": \"1\",\n},\ntimeout_seconds=120\n)\n# returns \"SLA Missed\" or \"SLA Met\"\nresponse = sensor.monitor()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-run-a-sql-query-on-snowflake-from-dbx","title":"How do I run a sql query on snowflake from DBX?","text":"<pre><code>from brickflow_plugins import SnowflakeOperator\nwf = Workflow(...)\n@wf.task\ndef run_snowflake_queries(*args):\nsf_query_run = SnowflakeOperator(\nsecret_scope = \"your_databricks secrets scope name\",\nquery_string = \"string of queries separated by semicolon(;)\",\nparameters={\"key1\":\"value1\", \"key2\":\"value2\"},\nfail_on_error=True,\n)\nsf_query_run.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-copy-a-partentire-data-of-a-uc-table-to-snowflake-table","title":"How do I copy a part/entire data of a UC table to snowflake table?","text":"<pre><code>from brickflow_plugins import UcToSnowflakeOperator\nwf = Workflow(...)\n@wf.task\ndef copy_from_uc_sf(*args):\nuc_to_sf_copy = UcToSnowflakeOperator(\nsecret_scope = \"your_databricks secrets scope name\",\nparameters = {'load_type':'incremental','dbx_catalog':'sample_catalog','dbx_database':'sample_schema',\n'dbx_table':'sf_operator_1', 'sf_schema':'stage','sf_table':'SF_OPERATOR_1',\n'sf_grantee_roles':'downstream_read_role1,downstream_read_role2', 'incremental_filter':\"dt='2023-10-22'\",\n'sf_cluster_keys':''}\n)\nuc_to_sf_copy.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-copy-files-or-folders-from-box-to-uc-volumes","title":"How do I copy files or folders from Box to UC Volumes?","text":"<pre><code>from brickflow_plugins import BoxToVolumesOperator\nwf = Workflow(...)\n@wf.task\ndef copy_from_box_to_volumnes(*args):\nbox_to_volumnes_copy = BoxToVolumesOperator(\nsecret_scope=\"my_secret_scope\",\ncerberus_client_url=\"https://my-cerberus-url.com\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\nfile_id = \"box_file_id\",\n)\nbox_to_volumnes_copy.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-copy-files-or-folders-from-uc-volumes-to-box","title":"How do I copy files or folders from UC Volumes to Box?","text":"<pre><code>from brickflow_plugins import VolumesToBoxOperator\nwf = Workflow(...)\n@wf.task\ndef copy_from_volumnes_to_box(*args):\nvolumnes_to_box_copy = VolumesToBoxOperator(\nsecret_scope=\"my_secret_scope\",\ncerberus_client_url=\"https://my-cerberus-url.com\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\n)\nvolumnes_to_box_copy.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-copy-files-or-folders-from-box-to-uc-volumes-and-uc-volumes-to-box","title":"How do I copy files or folders from Box to UC Volumes and UC Volumes to Box?","text":"<pre><code>from brickflow_plugins import BoxOperator\nwf = Workflow(...)\n@wf.task\ndef copy_from_box_to_volumnes(*args):\nbox_to_volumnes_copy = BoxOperator(\nsecret_scope=\"my_secret_scope\",\ncerberus_client_url=\"https://my-cerberus-url.com\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\noperation=\"download\",\n)\nbox_to_volumnes_copy.execute()\n@wf.task\ndef copy_from_volumnes_to_box(*args):\nvolumnes_to_box_copy = BoxOperator(\nsecret_scope=\"my_secret_scope\",\ncerberus_client_url=\"https://my-cerberus-url.com\",\nfolder_id=\"12345\",\nvolume_path=\"/path/to/local/volume\",\nfile_names=[\"file1.txt\", \"file2.txt\"],\nfile_pattern=\".txt\",\noperation=\"upload\",\n)\nvolumnes_to_box_copy.execute()\n</code></pre>","boost":3},{"location":"faq/faq/#how-do-i-use-serverless-compute-for-my-tasks","title":"How do I use serverless compute for my tasks?","text":"<p>Serverless compute is supported for: Brickflow entrypoint task, Notebook task and Python task. 1. Remove <code>default_cluster</code> configuration from the workflow and tasks. 2. Configure dependencies:       - For Brickflow entrypoint task use MAGIC commands by adding the below to the top of the notebook:</p> <pre><code>    ```python\n       # Databricks notebook source\n       # `brickflows` dependency is mandatory!\n       # It should always point to the `brickflows` version with serverless support or the wheel file with the same\n       # MAGIC %pip install brickflows==x.x.x\n       # MAGIC %pip install my-dependency==x.x.x\n       # MAGIC %restart_python\n\n       # COMMAND ----------\n    ```\n\n  - For Notebook task use the MAGIC commands, but `brickflows` dependency is not required:\n       ```python\n       # Databricks notebook source\n       # MAGIC %pip install my-dependency==x.x.x\n       # MAGIC %restart_python\n\n       # COMMAND ----------\n    ```\n\n  - For Python set the dependencies as usual on workflow level::\n    ```python\n     wf = Workflow(\n         \"brickflow-serverless-demo\",\n         schedule_quartz_expression=\"0 0/20 0 ? * * *\",\n         libraries=[\n             PypiTaskLibrary(package=\"my-package==x.x.x\"),\n             WheelTaskLibrary(whl=\"/path/to/wheel.whl\")\n         ],\n     )\n    ```\n</code></pre> <p>Refer to the full workflow example in <code>/examples/brickflow_serverless_examples</code> folder.</p>","boost":3},{"location":"upgrades/upgrade-pre-0-10-0-to-0-10-0/","title":"Upgrading to v0.10.x","text":"","boost":2},{"location":"upgrades/upgrade-pre-0-10-0-to-0-10-0/#upgrade-checklist","title":"Upgrade checklist","text":"<ul> <li> <p> The package has been renamed from <code>brickflow</code> to <code>brickflows</code>. Please run:</p> <pre><code>pip uninstall brickflow\n</code></pre> <p>and then</p> <pre><code>pip install brickflows&gt;=0.10.0\nbf --version\n</code></pre> </li> <li> <p> If you are upgrading from a CDKTF version of brickflow then do not worry, the existing workflows as long as you do   not change their names will be imported.</p> </li> <li> <p> Start using project configurations following the quickstart guide.</p> </li> <li> <p> Confirm the existence of the following files:</p> </li> <li> <p>brickflow-multi-project.yml</p> </li> <li>brickflow-project-root.yml</li> <li> <p>Please reference concepts     and initialize project for more details.</p> </li> <li> <p> RelativePathPackageResolver has been removed from the project to offer a seamless import   as long as you import brickflow at the top.</p> </li> <li> <p> Ensure import for brickflow is at the top of your entrypoint.py</p> </li> <li> <p> Ensure import for brickflow is at the top of your entrypoint.py</p> </li> <li> <p> Ensure your entrypoint looks like this. Make sure to click the plus buttons and read the highlighted sections:</p> </li> </ul> <pre><code># Databricks notebook source\n# COMMAND ----------\nfrom brickflow import Project # (1)!\nimport workflows # (2)!\ndef main() -&gt; None:\n\"\"\"Project entrypoint\"\"\"\nwith Project(\n\"product_abc_workflows_2\",\ngit_repo=\"https://github.com/stikkireddy/mono-repo-test\",\nprovider=\"github\",\nlibraries=[  # (3)!\n# PypiTaskLibrary(package=\"spark-expectations==0.5.0\"), # Uncomment if spark-expectations is needed\n],\nenable_plugins=True, # (4)!\n) as f:\nf.add_pkg(workflows)\nif __name__ == \"__main__\":\nmain()\n</code></pre> <ol> <li>Make sure brickflow is at the top of your imports! This will help resolve paths and allow other libraries to be    imported correctly.</li> <li>Import your modules after brickflow has been imported! Make sure your optimize imports doesnt reorder your imports!</li> <li>Make sure you remove brickflow and brickflow plugins and cron utils from this list.</li> <li>Make sure you have enable_plugins=True. This will enable the plugins to be loaded to support airflow operators, etc.    Disable this if you dont want to install airflow.</li> </ol>","boost":2}]}