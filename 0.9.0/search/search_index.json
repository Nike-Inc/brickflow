{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BrickFlow","text":"<p>BrickFlow is a CLI tool for development and deployment of Python based Databricks Workflows in a declarative way.</p>"},{"location":"#concept","title":"Concept","text":"<p><code>brickflow</code> aims to improve development experience for building any pipelines on databricks via:</p> <ul> <li>Providing a declarative way to describe workflows via decorators</li> <li>Provide intelligent defaults to compute targets</li> <li>Provide a code and git first approach to managing and deploying workflows</li> <li>Use IAC such as terraform to manage the state and deploy jobs and their infrastructure.</li> <li>CLI tool helps facilitate setting up a projects</li> <li>Provides additional functionality through the context library to be able to do additional things for workflows.</li> </ul>"},{"location":"#feedback","title":"Feedback","text":"<p>Issues with <code>brickflow</code>? Found a  bug? Have a great idea for an addition? Want to improve the documentation? Please feel free to file an issue.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>To contribute please fork and create a pull request.</p>"},{"location":"bundles-quickstart/","title":"Bundles (Recommended)","text":""},{"location":"bundles-quickstart/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Locally (optional):</li> <li>Python &gt;= 3.8</li> <li>Configure the databricks cli cfg file. <code>pip install databricks-cli</code> and then <code>databricks configure -t</code> which    will configure the databricks cli with a token. </li> </ol>"},{"location":"bundles-quickstart/#setup-project","title":"Setup Project","text":"<ul> <li> <p>The first step is to initialize the project. It will do the following:</p> <ol> <li>Create the entrypoint.py file in your workflows module.</li> <li>Update your .gitignore file with the correct directories to ignore.</li> </ol> </li> <li> <p>To initialize the project inside the bfs shell run:</p> <pre><code>bf init\n</code></pre> </li> <li> <p>It will prompt you for the:</p> <ol> <li>Project Name</li> <li>Git https url of your project</li> <li>Workflows Directory Path</li> <li>Brickflow Version </li> <li>Spark Expectations Version</li> </ol> </li> </ul>"},{"location":"bundles-quickstart/#gitignore","title":"gitignore","text":"<ul> <li> <p>For now all the bundle.yml files will be code generated so you can add the following to your .gitignore file:</p> <pre><code>**/bundle.yml\n</code></pre> </li> </ul>"},{"location":"bundles-quickstart/#post-setup","title":"Post Setup","text":"<ul> <li> <p>To deploy run the following command</p> <pre><code>bf deploy --deploy-mode=bundle -p \"&lt;profile&gt;\" -wd &lt;workflows directory&gt;\n</code></pre> </li> <li> <p>To destroy run the following command</p> <pre><code>bf destroy --deploy-mode=bundle -p \"&lt;profile&gt;\" -wd &lt;workflows directory&gt;\n</code></pre> </li> </ul>"},{"location":"environment-variables/","title":"ENV Variables","text":""},{"location":"environment-variables/#environment-variables","title":"Environment Variables","text":""},{"location":"environment-variables/#note-cdktf-is-deprecated-please-keep-in-mind-as-you-read-the-list","title":"Note: CDKTF is deprecated please keep in mind as you read the list","text":"Environment Variable Default Value Deploment Mode Support Description BRICKFLOW_ENV local bundle &amp; cdktf (deprecated) The environment name for Brickflow BRICKFLOW_FORCE_DEPLOY False cdktf (deprecated) Flag indicating whether to force deployment BRICKFLOW_DEPLOYMENT_MODE cdktf (deprecated) bundle &amp; cdktf (deprecated) The deployment mode for Brickflow (cdktf, bundles) BRICKFLOW_GIT_REPO N/A bundle &amp; cdktf (deprecated) The URL of the Git repository for Brickflow BRICKFLOW_GIT_REF N/A bundle &amp; cdktf (deprecated) The Git reference (branch, tag, commit) for Brickflow BRICKFLOW_GIT_PROVIDER github bundle &amp; cdktf (deprecated) The Git provider (e.g., GitHub, GitLab) for Brickflow DATABRICKS_CONFIG_PROFILE default bundle &amp; cdktf (deprecated) The profile name for Databricks configuration BRICKFLOW_DEPLOY_ONLY_WORKFLOWS N/A bundle &amp; cdktf (deprecated) List of workflows to deploy exclusively BRICKFLOW_WORKFLOW_PREFIX N/A bundle &amp; cdktf (deprecated) Prefix to add to workflow names during deployment BRICKFLOW_WORKFLOW_SUFFIX N/A bundle &amp; cdktf (deprecated) Suffix to add to workflow names during deployment BRICKFLOW_S3_BACKEND_BUCKET N/A cdktf (deprecated) The name of the S3 bucket for Brickflow backend BRICKFLOW_S3_BACKEND_KEY N/A cdktf (deprecated) The key or path in the S3 bucket for Brickflow backend BRICKFLOW_S3_BACKEND_REGION N/A cdktf (deprecated) The AWS region for the S3 backend BRICKFLOW_S3_BACKEND_DYNAMODB_TABLE N/A cdktf (deprecated) The DynamoDB table name for tracking S3 backend BRICKFLOW_INTERACTIVE_MODE True bundle &amp; cdktf (deprecated) Flag indicating whether to enable interactive mode BRICKFLOW_BUNDLE_BASE_PATH /Users/${workspace.current_user.userName} bundle The base path for the bundle in the S3 backend BRICKFLOW_BUNDLE_OBJ_NAME .brickflow_bundles bundle The name of the folder post appended to your base path BRICKFLOW_BUNDLE_CLI_EXEC databricks bundle The executable command for bundle execution. By default it will be downloaded on the fly. BRICKFLOW_BUNDLE_NO_DOWNLOAD False bundle Flag indicating whether to skip downloading the databricks bundle cli. Useful if you are in locked down network. BRICKFLOW_BUNDLE_CLI_VERSION 0.200.0 bundle The version of the bundle CLI tool BRICKFLOW_MONOREPO_PATH_TO_BUNDLE_ROOT N/A bundle &amp; cdktf (deprecated) The path to the bundle root directory in a monorepo. Default assumes you are not using a monorepo"},{"location":"environment-variables/#workflow-prefixing-or-suffixing","title":"Workflow prefixing or suffixing","text":"<p>This allows for adding suffixes or prefixes in the name of the workflow:</p> <ul> <li>BRICKFLOW_WORKFLOW_PREFIX</li> <li>BRICKFLOW_WORKFLOW_SUFFIX</li> </ul> <p>Setting the above is semantically the same as doing this in code:</p> <pre><code>wf = Workflow(\n\"thanks\",\nprefix=\"so_long_\",  # same as BRICKFLOW_WORKFLOW_PREFIX\nsuffix=\"_and_thanks_for_all_the_fish\"  # same as BRICKFLOW_WORKFLOW_SUFFIX\n)\n</code></pre> <p><code>wf.name</code> would then result in \"so_long_and_thanks_for_all_the_fish\"</p> <p>this is to allow 'unique' names while deploying the same workflow to same environments while still needing to keep them separate.</p> <p>For example, consider this scenario:</p> <ul> <li>You have a workflow named <code>inventory_upsert</code>;</li> <li>Two features are being developed on in parallel in the DEV environment, let's name these <code>feature_1</code> and <code>feature_2</code>;</li> <li>If you don't have the ability to uniquely set the name for a workflow, the workflow you are creating in dev (no matter   in which feature/branch they originate from) will always be named <code>dev_inventory_upsert</code>;</li> <li>with using the prefix/suffix mechanism, we can set a ENV variable and end up with unique names for each feature,   i.e. <code>dev_inventory_upsert_feature_1</code> and <code>dev_inventory_upsert_feature_2</code>.</li> </ul> <p>Ideal usage for this is in CI/CD pipelines.</p>"},{"location":"highlevel/","title":"HighLevel","text":""},{"location":"highlevel/#brickflow-overview","title":"Brickflow Overview","text":"<p>The objective of Brickflow is to provide a thin layer on top of databricks workflows to help deploy  and manage workflows in Databricks. It also provides plugins/extras to be able to run airflow  operators directly in the workflows.</p>"},{"location":"highlevel/#brickflow-to-airflow-term-mapping","title":"Brickflow to Airflow Term Mapping","text":"Object Airflow Brickflow Collection of Workflows Airflow Cluster (Airflow Dag Bag) Project/Entrypoint Workflow Airflow Dag Workflow Task Airflow Operator Task Schedule Unix Cron Quartz Cron Inter Task Communication XComs Task Values Connections to External Services Airflow Connections Cerberus Connection Builder Variables to Tasks Variables Task Parameters Context values (execution_date, etc.) Airflow Macros, context[\"ti\"] ctx.&lt;task parameter&gt;"},{"location":"limitations/","title":"Limitations","text":"<ul> <li> Docs (WIP)</li> <li> Python wheel tasks</li> <li> Support for DBSQL tasks using SQL Warehouses</li> <li> CLI for visualizing workflow locally using a graphing tool</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>The project is similar to a map cluster it can be composed of various different Workflows or dags.</p> <p>Here is an example of an entrypoint.  Click the plus buttons to understand all the parts of the entrypoint file.</p> entrypoint.py<pre><code># Databricks notebook source  # (1)!\nimport examples.brickflow_examples.workflows\nfrom brickflow import Project, PypiTaskLibrary, MavenTaskLibrary\nARTIFACTORY = \"\"\ndef main() -&gt; None:\n\"\"\"Project entrypoint\"\"\"\nwith Project(\n\"brickflow-demo\",  # (3)!\ngit_repo=\"https://github.com/Nike-Inc/brickflow\",  # (4)!\nprovider=\"github\",  # (5)!\nlibraries=[  # (6)!\nPypiTaskLibrary(package=\"brickflow==1.0.0 --extra-index-url \" + ARTIFACTORY),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\n],\n) as f:\nf.add_pkg(examples.brickflow_examples.workflows)  # (7)!\nif __name__ == \"__main__\":  # (2)!\nmain()\n</code></pre> <ol> <li>Uploading this Python file into databricks with this comment on the first line treats the python file     as a notebook.</li> <li>This makes sure this only runs when this file is run via python entrypoint.py</li> <li>This is the project name you provided when you do <code>bf init</code></li> <li>This is the git repo that is introspected when running <code>bf init</code></li> <li>This is the github provider that you decide on.</li> <li>You can provide a list of packages that need to be installed in all of your clusters when running ETL.</li> <li>You can add multiple packages in your project where you are defining workflows.</li> </ol>"},{"location":"quickstart/","title":"CDKTF (Deprecated)","text":""},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ol> <li>You need either of the following installed:</li> <li>Install via docker:<ol> <li>Docker installed on your laptop</li> </ol> </li> <li>Install Locally (optional):<ol> <li>Python &gt;= 3.8</li> <li>Install nodejs == 18.14.0</li> <li>Install terraform 1.3.1</li> <li>Install cerberus-python-client</li> </ol> </li> <li>Configure your github integration to your repos using SSH.</li> <li>Configure the databricks cli cfg file. <code>pip install databricks-cli</code> and then <code>databricks configure -t</code> which    will configure the databricks cli with a token. </li> </ol>"},{"location":"quickstart/#install-via-docker","title":"Install Via Docker","text":"<p>We recommend to use docker container for development purposes as it's easier to have version upgrades by changing the docker version.</p> <ul> <li> <p>Add the following alias to your profile or zsh_profile:</p> <pre><code>alias bfs='docker run -it --rm --name brickflow -v \"$PWD\":/usr/src/brickflow -v ~/.databrickscfg:/root/.databrickscfg:ro -v ~/.ssh:/root/.ssh:ro -w /usr/src/brickflow &lt;DOCKERHUB_URL_REPLACE&gt;/brickflow:latest'\n</code></pre> </li> <li> <p>Please change your directory to the root of your project. Then run the <code>bfs</code> command.</p> <pre><code>bfs\n</code></pre> </li> <li> <p>This will launch the bash shell inside the container. It will do the following:</p> <ol> <li>Mount your current working directory as read-write to the working directory in the container.</li> <li>Mount your <code>~/.ssh</code> directory as read-only to the <code>~/.ssh</code> in the container.</li> <li>Mount your <code>~/.databrickscfg</code> file as read-only to the <code>~/.databrickscfg</code> in the container.</li> </ol> </li> <li> <p>You will also need to install any required packages of your respective project inside the docker container.</p> </li> </ul>"},{"location":"quickstart/#upgrade-the-brickflow-container","title":"Upgrade the brickflow container","text":"<ul> <li> <p>If the brickflow version in your container is outdated and needed to upgrade then run the below command in your shell which pull the latest docker image</p> <pre><code>docker pull &lt;DOCKERHUB_URL_REPLACE&gt;/brickflow:latest\n</code></pre> </li> </ul>"},{"location":"quickstart/#install-locally-optional-if-you-choose-not-to-use-docker","title":"Install locally (optional if you choose not to use docker)","text":"<p>Alternatively instead of docker you can install locally but you will need to resolve all the deps.</p> <p>The project relies on terraform and cdktf to deploy your python projects.</p> <ol> <li>Install brew if not installed already using - brew-install</li> <li>Install node using <code>brew install node</code></li> <li>Install cdktf-cli via <code>npm install -g cdktf-cli</code></li> <li>Install the brickflow package via <code>pip install brickflow[deploy]</code></li> <li>Install the cerberus if needed via <code>pip install brickflow[cerberus]</code></li> <li>Install the airflow if needed via <code>pip install brickflow[airflow]</code></li> </ol>"},{"location":"quickstart/#setup-project","title":"Setup Project","text":"<ul> <li> <p>The first step is to initialize the project. It will do the following:</p> <ol> <li>Create the entrypoint.py file in your workflows module.</li> <li>Update your .gitignore file with the correct directories to ignore.</li> </ol> </li> <li> <p>To initialize the project inside the bfs shell run:</p> <pre><code>bf init\n</code></pre> </li> <li> <p>It will prompt you for the:</p> <ol> <li>Project Name</li> <li>Git https url of your project</li> <li>Workflows Directory Path</li> <li>Brickflow Version </li> <li>Spark Expectations Version</li> </ol> </li> </ul>"},{"location":"tasks/","title":"Tasks","text":"<p>A task in Databricks workflows refers to a single unit of work that is executed as part of a larger data processing  pipeline. Tasks are typically designed to perform a specific set of operations on data, such as loading data from a  source, transforming the data, and storing it in a destination. In brickflow, tasks as designed in such a way that </p> <p>Assuming, that this is already read - workflow and workflow object is created</p>"},{"location":"tasks/#task","title":"Task","text":"<p>Databricks workflow task can be created by decorating a python function with brickflow's task function</p> task<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task  # (1)!\ndef start():\npass\n@wf.task(name=\"custom_end\")  # (2)!\ndef end():\npass\n</code></pre> <ol> <li>Create a task using a decorator pattern. The task name would default to the python function name. So a task will be      created with the name \"start\"</li> <li>Creating a task and defining the task name explicitly instead of using the function name \"end\". The task will be    created with the new name \"custom_end\"</li> </ol>"},{"location":"tasks/#task-dependency","title":"Task dependency","text":"<p>Define task dependency by using a variable \"depends_on\" in the task function. You can provide the dependent tasks as direct python callables or string or list of callables/strings</p> task_dependency<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task\ndef start():\npass\n@wf.task(depends_on=start)  # (1)!\ndef bronze_layer():\npass\n@wf.task(depends_on=\"bronze_layer\")  # (2)!\ndef x_silver():\npass\n@wf.task(depends_on=bronze_layer)\ndef y_silver():\npass\n@wf.task(depends_on=[x_silver, y_silver])  # (3)!\ndef xy_gold():\npass\n@wf.task(name=\"custom_z_gold\", depends_on=[x_silver, \"y_silver\"])  # (4)!\ndef z_gold():\npass\n@wf.task(depends_on=[\"xy_gold\", \"custom_z_gold\"])  # (5)!\ndef end():\npass\n</code></pre> <ol> <li>Create dependency on task \"start\" and it is passed as callable</li> <li>Create dependency on task \"bronze_layer\" and it is passed as a string</li> <li>Create dependency on multiple tasks using list and the tasks are callables</li> <li>Create dependency on multiple tasks using list but one task is a callable and another is a string</li> <li>Create dependency on multiple tasks using list and tasks are passed as string. \"custom_z_gold\" is the task name that    is explicitly defined - should not use \"z_gold\" which is a function name</li> </ol>"},{"location":"tasks/#task-parameters","title":"Task parameters","text":"<p>Task parameters can be defined as key value pairs in the function definition on which task is defined</p> task_parameters<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task\ndef task_function(*, test=\"var\", test1=\"var1\"):  # (1)!\nprint(test)\nprint(test1)\n</code></pre> <ol> <li>To pass the task specific parameters, need to start with \"*\" and then key value pairs start</li> </ol>"},{"location":"tasks/#common-task-parameters","title":"Common task parameters","text":"<p>In the workflows section, we saw how the common task parameters are created at  the workflow level. Now in this section, we shall see how to use the common task parameters</p> use_common_task_parameters<pre><code>from brickflow import Workflow, ctx\nwf = Workflow(...)\n@wf.task\ndef common_params():\nimport some_pyspark_function  # (1)!\ncatalog_env = ctx.dbutils_widget_get_or_else(key=\"catalog\", debug=\"local\")  # (2)!\nsome_pyspark_function(catalog_env)  # (3)!\n</code></pre> <ol> <li>It is recommended to use localized imports in tasks rather than the global imports</li> <li>Brickflow provides the context using which we can fetch the task parameters that are defined. Providing debug is    mandatory or else there will be a compilation error while deploying</li> <li>The extracted task_parameter_value can be used as any python variable. In this example, we are just passing the    variable to \"some_pyspark_function\"</li> </ol>"},{"location":"tasks/#inbuilt-task-parameters","title":"Inbuilt task parameters","text":"<p>There are many inbuilt task parameters that be accessed using brickflow context like above</p> inbuilt_task_parameters<pre><code>from brickflow import Workflow, ctx\nwf = Workflow(...)\n@wf.task\ndef inbuilt_params():\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_env\",  # (1)! \ndebug=\"local\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_run_id\",  # (2)! \ndebug=\"788868\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_job_id\",  # (3)! \ndebug=\"987987987987987\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_start_date\",  # (4)! \ndebug=\"2023-05-03\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_start_time\",  # (5)! \ndebug=\"1683102411626\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_task_retry_count\",  # (6)! \ndebug=\"2\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_parent_run_id\",  # (7)! \ndebug=\"788869\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_task_key\",  # (8)! \ndebug=\"inbuilt_params\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_internal_workflow_name\",  # (9)! \ndebug=\"Sample_Workflow\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_internal_task_name\",  # (10)! \ndebug=\"inbuilt_params\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_internal_workflow_prefix\",  # (11)! \ndebug=\"inbuilt_params\"))\nprint(ctx.dbutils_widget_get_or_else(\nkey=\"brickflow_internal_workflow_suffix\",  # (12)! \ndebug=\"inbuilt_params\"))\n</code></pre> <ol> <li>\"brickflow_env\" holds the value of the --env variable which was used when brickflow is deployed</li> <li>\"brickflow_run_id\" holds the value of the current task run id</li> <li>\"brickflow_job_id\" holds the value of the current workflow job id</li> <li>\"brickflow_start_date\" holds the value of the current workflow start date</li> <li>\"brickflow_start_time\" holds the value of the current task start time</li> <li>\"brickflow_task_retry_count\" holds the value of number of retries a task can run, when a failure occurs</li> <li>\"brickflow_parent_run_id\" hold the value of the current workflow run_id</li> <li>\"brickflow_task_key\" holds the value of the current task name</li> <li>\"brickflow_internal_workflow_name\" holds the value of the current workflow name</li> <li>\"brickflow_internal_task_name\" holds the value of the current task name</li> <li>\"brickflow_internal_workflow_prefix\" holds the value of the prefix used for the current workflow name</li> <li>\"brickflow_internal_workflow_suffix\" holds the value of the suffix used for the current workflow name</li> </ol>"},{"location":"tasks/#clusters","title":"Clusters","text":"<p>There is a flexibility to use different clusters for each task or assign custom clusters</p> clusters<pre><code>from brickflow import Workflow, Cluster\nwf = Workflow(...)\n@wf.task(cluster=Cluster(...))  # (1)!\ndef custom_cluster():\npass\n</code></pre> <ol> <li>You will be able to create a job cluster or use existing cluster. Refer to this section in     the workflows to understand how to implement</li> </ol>"},{"location":"tasks/#libraries","title":"Libraries","text":"<p>There is a flexibility to use specific libraries for a particular task</p> libraries<pre><code>from brickflow import Workflow\nwf = Workflow(...)\n@wf.task(libraries=[...])  # (1)!\ndef custom_libraries():\npass\n</code></pre> <ol> <li>You will be able to install libraries that are specific to a task. Refer to this section in    the workflows to understand how to implement</li> </ol>"},{"location":"tasks/#task-types","title":"Task types","text":"<p>There are different task types that are supported by brickflow right now. The default task type that is used by  brickflow is NOTEBOOK</p> task_types<pre><code>from brickflow import Workflow, TaskType, BrickflowTriggerRule, TaskResponse\nwf = Workflow(...)\n@wf.task\ndef notebook_task():\npass\n@wf.task(task_type=TaskType.DLT)\ndef dlt_task():\npass\n@wf.task(\ntask_type=TaskType.CUSTOM_PYTHON_TASK,  # (1)!\ntrigger_rule=BrickflowTriggerRule.NONE_FAILED,  # (2)!\ncustom_execute_callback=lambda x: TaskResponse(x.name, \npush_return_value=True),  # (3)!\n)\ndef custom_python_task():\npass\n</code></pre> <ol> <li>Provide the task type that is to be used for this task. Default is a notebook task</li> <li>Trigger rule can be attached. It can be ALL_SUCCESS or NONE_FAILED. In this case, this task will be triggered, if all    the upstream tasks are at-least run and completed.</li> <li>Custom function that have to be executed as a call back. \"push_return_value\" will assign the output to task values.    Task values can be compared to xcom values in airflow</li> </ol>"},{"location":"tasks/#trigger-rules","title":"Trigger rules","text":"<p>There are two types of trigger rules that can be applied on a task. It can be either ALL_SUCCESS or NONE_FAILED</p> task_types<pre><code>from brickflow import Workflow, BrickflowTriggerRule\nwf = Workflow(...)\n@wf.task(\ntrigger_rule=BrickflowTriggerRule.NONE_FAILED  # (1)!\n)\ndef none_failed_task():\npass\n@wf.task(\ntrigger_rule=BrickflowTriggerRule.ALL_SUCCESS  # (2)!\n)\ndef all_success_task():\npass\n</code></pre> <ol> <li>NONE_FAILED - use this if you want to trigger the task irrespective of the upstream tasks success or failure state</li> <li>ALL_SUCCESS - use this if you want to trigger the task only if all the upstream tasks are all having success state</li> </ol>"},{"location":"tasks/#operators","title":"Operators","text":"<p>We have adopted/extended certain airflow operators that might be needed to run as a task in databricks workflows. Typically for airflow operators we return the operator and brickflow will execute the operator based on task return type.</p>"},{"location":"tasks/#bash-operator","title":"Bash Operator","text":"<p>You will be able to use bash operator as below</p> bash_operator<pre><code>from brickflow import Workflow\nfrom brickflow_plugins import BashOperator\nwf = Workflow(...)\n@wf.task\ndef bash_task():\nreturn BashOperator(task_id=bash_task.__name__, \nbash_command=\"ls -ltr\")  # (1)!\n</code></pre> <ol> <li>Use Bashoperator like how we use in airflow but it has to be returned from task function</li> </ol>"},{"location":"tasks/#task-dependency-sensor","title":"Task Dependency Sensor","text":"<p>Even if you migrate to databricks workflows, brickflow gives you the flexibility to have a dependency on the airflow job</p> task_dependency_sensor<pre><code>from brickflow import Workflow, ctx\nfrom brickflow_plugins import TaskDependencySensor\nwf = Workflow(...)\n@wf.task\ndef airflow_external_task_dependency_sensor():\nimport base64\ndata = base64.b64encode(\nctx.dbutils.secrets.get(\"brickflow-demo-tobedeleted\", \"okta_conn_id\").encode(\n\"utf-8\"\n)\n).decode(\"utf-8\")\nreturn TaskDependencySensor(\ntask_id=\"sensor\",\ntimeout=180,\nokta_conn_id=f\"b64://{data}\",\nexternal_dag_id=\"external_airlfow_dag\",\nexternal_task_id=\"hello\",\nallowed_states=[\"success\"],\nexecution_delta=None,\nexecution_delta_json=None,\ncluster_id=\"your_cluster_id\",\n)\n</code></pre>"},{"location":"workflows/","title":"Workflows","text":"<p>A Workflow is similar to an Airflow dag that lets you encapsulate a set of tasks. </p> <p>Here is an example of a workflow.  Click the plus buttons to understand all the parts of the workflow file.</p> workflow.py<pre><code>from datetime import timedelta\nfrom brickflow import Workflow, Cluster, WorkflowPermissions, User, \\\n    TaskSettings, EmailNotifications, PypiTaskLibrary, MavenTaskLibrary\nwf = Workflow(  # (1)!\n\"wf_test\",  # (2)!\ndefault_cluster=Cluster.from_existing_cluster(\"your_existing_cluster_id\"),  # (3)!\n# Optional parameters below\nschedule_quartz_expression=\"0 0/20 0 ? * * *\",  # (4)!\ntimezone=\"UTC\",  # (5)!\ndefault_task_settings=TaskSettings(  # (6)!\nemail_notifications=EmailNotifications(\non_start=[\"email@nike.com\"],\non_success=[\"email@nike.com\"],\non_failure=[\"email@nike.com\"]\n),\ntimeout_seconds=timedelta(hours=2).seconds\n),\nlibraries=[  # (7)!\nPypiTaskLibrary(package=\"requests\"),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\n],\ntags={  # (8)!\n\"product_id\": \"brickflow_demo\",\n\"slack_channel\": \"nike-sole-brickflow-support\"\n},\nmax_concurrent_runs=1,  # (9)!\npermissions=WorkflowPermissions(  # (10)!\ncan_manage_run=[User(\"abc@abc.com\")],\ncan_view=[User(\"abc@abc.com\")],\ncan_manage=[User(\"abc@abc.com\")],\n),\nprefix=\"feature-jira-xxx\",  # (11)!\nsuffix=\"_qa1\",  # (12)!\ncommon_task_parameters={  # (13)!\n\"catalog\": \"development\",\n\"database\": \"your_database\"\n},\n)\n@wf.task()  # (14)!\ndef task_function(*, test=\"var\"):\nreturn \"hello world\"\n</code></pre> <ol> <li>Workflow definition which constructs the workflow object</li> <li>Define the workflow name</li> <li>The default cluster used for all the tasks in the workflow. This is an all-purpose cluster, but you can also create a job cluster</li> <li>Cron expression in the quartz format</li> <li>Define the timezone for your workflow. It is defaulted to UTC</li> <li>Default task setting that can be used for all the tasks</li> <li>Libraries that need to be installed for all the tasks</li> <li>Tags for the resulting workflow and other objects created during the workflow.</li> <li>Define the maximum number of concurrent runs</li> <li>Define the permissions on the workflow</li> <li>Prefix for the name of the workflow</li> <li>Suffix for the name of the workflow</li> <li>Define the common task parameters that can be used in all the tasks</li> <li>Define a workflow task and associate it to the workflow</li> </ol>"},{"location":"workflows/#clusters","title":"Clusters","text":"<p>There are two ways to define the cluster for the workflow or a task</p>"},{"location":"workflows/#using-an-existing-cluster","title":"Using an existing cluster","text":"existing_cluster<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster.from_existing_cluster(\"your_existing_cluster_id\")\n</code></pre>"},{"location":"workflows/#use-a-job-cluster","title":"Use a job cluster","text":"job_cluster<pre><code>from brickflow import Cluster\ndefault_cluster=Cluster(\nname=\"your_cluster_name\",\nspark_version='11.3.x-scala2.12',\nnode_type_id='m6g.xlarge',\ndriver_node_type_id='m6g.xlarge',\nmin_workers=1,\nmax_workers=3,\nenable_elastic_disk=True,\npolicy_id='your_policy_id',\naws_attributes={\n\"first_on_demand\": 1,\n\"availability\": \"SPOT_WITH_FALLBACK\",\n\"instance_profile_arn\": \"arn:aws:iam::XXXX:instance-profile/XXXX/group/XX\",\n\"spot_bid_price_percent\": 100,\n\"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n\"ebs_volume_count\": 3,\n\"ebs_volume_size\": 100\n}\n)\n</code></pre>"},{"location":"workflows/#permissions","title":"Permissions","text":"<p>Brickflow provides an opportunity to manage permissions on the workflows.  You can provide individual users or to a group or to a ServicePrincipal that can help manage, run or  view the workflows.</p> <p>Below example is for reference</p> manage_permissions<pre><code>from brickflow import WorkflowPermissions, User, Group, ServicePrincipal\npermissions=WorkflowPermissions(\ncan_manage_run=[\nUser(\"abc@abc.com\"), \nGroup(\"app.xyz.team.Developer\"), \nServicePrincipal(\"ServicePrinciple_dbx_url.app.xyz.team.Developer\")\n],\ncan_view=[User(\"abc@abc.com\")],\ncan_manage=[User(\"abc@abc.com\")],\n)\n</code></pre>"},{"location":"workflows/#tags","title":"Tags","text":"<p>Using brickflow, custom tags can be created on the workflow - but there are also some default tags  that are created while the job is deployed.</p> <p>The defaults tags that gets automatically attached to the workflow are below</p> <ul> <li>\"brickflow_project_name\" : Brickflow Project Name that is referred from the entrypoint.py file</li> <li>\"brickflow_version\" : Brickflow Version that is used to deploy the workflow</li> <li>\"databricks_tf_provider_version\" : Databricks terraform provider version that is used to deploy the workflow</li> <li>\"deployed_by\" : Email id of the profile that is used to deploy the workflow.     It can be a user or a service principle. Whichever id is used to deploy the workflow, automatically becomes the    owner of the workflow</li> <li>\"environment\" : Environment to which the workflow is identified to</li> </ul> <p>Use the below reference to define more tags and attach to the workflow. These can be used for collecting various metrics and build dashboards.</p> configure_tags<pre><code>tags={\n\"product_id\": \"brickflow_demo\",\n\"slack_channel\": \"nike-sole-brickflow-support\"\n}\n</code></pre>"},{"location":"workflows/#schedule","title":"Schedule","text":"<p>Databricks workflows uses Quartz cron expression unlike airflow's unix based cron scheduler. A typical Quartz cron expression have six or seven fields, seperated by spaces</p> <p><pre><code>second minute hour day_of_month month day_of_week year(optional)\n</code></pre> Below is a sample</p> quartz_cron_expression<pre><code>schedule_quartz_expression=\"0 0/20 0 ? * * *\"\n</code></pre>"},{"location":"workflows/#tasksettings","title":"Tasksettings","text":"<p>Task setting at workflow level can be used to have common setting defined that will be applicable for all the tasks. Below is a sample that can be used for reference and all the parameters in TaskSettings are optional task_settings<pre><code>from datetime import timedelta\nfrom brickflow import TaskSettings, EmailNotifications\ndefault_task_settings=TaskSettings(\nemail_notifications=EmailNotifications(\non_start=[\"email@nike.com\"],\non_success=[\"email@nike.com\"],\non_failure=[\"email@nike.com\"]\n),\ntimeout_seconds=timedelta(hours=2).seconds,\nmax_retries=2,\nmin_retry_interval_millis=60000,\nretry_on_timeout=True\n)\n</code></pre></p>"},{"location":"workflows/#libraries","title":"Libraries","text":"<p>Brickflow allows to specify libraries that are need to be installed and used across different tasks. There are many ways to install library from different repositories/sources</p> libraries<pre><code>from brickflow import PypiTaskLibrary, MavenTaskLibrary, StorageBasedTaskLibrary, \\\n    JarTaskLibrary, EggTaskLibrary, WheelTaskLibrary\nlibraries=[\nPypiTaskLibrary(package=\"requests\"),\nMavenTaskLibrary(coordinates=\"com.cronutils:cron-utils:9.2.0\"),\nStorageBasedTaskLibrary(\"s3://...\"),\nStorageBasedTaskLibrary(\"dbfs://...\"),\nJarTaskLibrary(\"s3://...\"),\nJarTaskLibrary(\"dbfs://...\"),\nEggTaskLibrary(\"s3://...\"),\nEggTaskLibrary(\"dbfs://...\"),\nWheelTaskLibrary(\"s3://...\"),\nWheelTaskLibrary(\"dbfs://...\"),\n]\n</code></pre>"},{"location":"workflows/#common-task-parameters","title":"Common task parameters","text":"<p>Define the common parameters that can be used in all the tasks. Example could be database name, secrets_id etc</p> common_task_parameters<pre><code>common_task_parameters={\n\"catalog\": \"development\",\n\"database\": \"your_database\"\n}\n</code></pre>"},{"location":"api/airflow_external_task_dependency/","title":"AirflowTaskDependencySensor","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks-attributes","title":"Attributes","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks-classes","title":"Classes","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagSchedule","title":"<code>brickflow_plugins.airflow.operators.external_tasks.MapDagSchedule</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagSchedule-functions","title":"Functions","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagSchedule.get_schedule","title":"<code>get_schedule(wf_id: str, **args: str)</code>","text":"<p>Function that the sensors defined while deriving this class should override.</p> Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_schedule(self, wf_id: str, **args):\n\"\"\"\n    Function that the sensors defined while deriving this class should\n    override.\n    \"\"\"\nraise Exception(\"Override me.\")\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagSchedule.get_task_run_status","title":"<code>get_task_run_status(wf_id: str, task_id: str, run_date: str = None, cluster_id: str = None, **args: str)</code>","text":"<p>Function that the sensors defined while deriving this class should override.</p> Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_task_run_status(\nself, wf_id: str, task_id: str, run_date=None, cluster_id=None, **args\n):\n\"\"\"\n    Function that the sensors defined while deriving this class should\n    override.\n    \"\"\"\nraise Exception(\"Override me.\")\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper","title":"<code>brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper(okta_conn_id: str)</code>","text":"<p>             Bases: <code>MapDagSchedule</code></p> Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def __init__(self, okta_conn_id: str):\nself._okta_conn: Connection = Connection.get_connection_from_secrets(\nokta_conn_id\n)\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper-functions","title":"Functions","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_access_token","title":"<code>get_access_token() -&gt; str</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_access_token(self) -&gt; str:\nokta_url = self.get_okta_url()\nclient_id = self.get_okta_client_id()\nclient_secret = self.get_okta_client_secret()\nokta_url = os.getenv(\"OKTA_URL\", okta_url)\npayload = (\n\"client_id=\"\n+ client_id\n+ \"&amp;client_secret=\"\n+ client_secret\n+ \"&amp;grant_type=client_credentials\"\n)\nheaders = {\n\"Content-Type\": \"application/x-www-form-urlencoded\",\n\"cache-control\": \"no-cache\",\n}\nresponse = requests.post(okta_url, data=payload, headers=headers, timeout=600)\nif (\nresponse.status_code &lt; HTTPStatus.OK\nor response.status_code &gt; HTTPStatus.PARTIAL_CONTENT\n):\nlog.error(\n\"Failed request to Okta for JWT status_code={} response={} client_id={}\".format(\nresponse.status_code, response.text, client_id\n)\n)\ntoken_data = response.json()[\"access_token\"]\nreturn token_data\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_airflow_api_url","title":"<code>get_airflow_api_url(cluster_id: str) -&gt; str</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_airflow_api_url(self, cluster_id: str) -&gt; str:\n# TODO: templatize this to a env variable\nbase_api_url = f\"https://proxy.us-east-1.map.nike.com/{cluster_id}\"\nreturn base_api_url\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_okta_client_id","title":"<code>get_okta_client_id() -&gt; str</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_okta_client_id(self) -&gt; str:\nreturn self._okta_conn.login\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_okta_client_secret","title":"<code>get_okta_client_secret() -&gt; str</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_okta_client_secret(self) -&gt; str:\nreturn self._okta_conn.get_password()\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_okta_url","title":"<code>get_okta_url() -&gt; str</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_okta_url(self) -&gt; str:\nconn_type = self._okta_conn.conn_type\nhost = self._okta_conn.host\nschema = self._okta_conn.schema\nreturn f\"{conn_type}://{host}/{schema}\"\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_schedule","title":"<code>get_schedule(wf_id: str, **kwargs: str)</code>","text":"<p>get work flow schedule cron syntax</p> Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_schedule(self, wf_id: str, **kwargs):\n\"\"\"\n    get work flow schedule cron syntax\n    \"\"\"\nraise Exception(\"Do not have implementation\")\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_task_run_status","title":"<code>get_task_run_status(wf_id: str, task_id: str, run_date: str = None, cluster_id: str = None, **args: str)</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_task_run_status(\nself, wf_id: str, task_id: str, run_date=None, cluster_id=None, **args\n):\ntoken_data = self.get_access_token()\napi_url = self.get_airflow_api_url(cluster_id)\nversion_nr = self.get_version(cluster_id)\ndag_id = wf_id\nheaders = {\n\"Content-Type\": \"application/json\",\n\"cache-control\": \"no-cache\",\n\"Authorization\": \"Bearer \" + token_data,\n}\no_task_status = \"UKN\"\nsession = requests.Session()\nretries = Retry(\ntotal=5, backoff_factor=1, status_forcelist=[502, 503, 504, 500]\n)\nsession.mount(\"https://\", HTTPAdapter(max_retries=retries))\nif version_nr.startswith(\"1.\"):\nlog.info(\"this is 1.x cluster\")\nurl = (\napi_url\n+ \"/api/experimental\"\n+ \"/dags/\"\n+ dag_id\n+ \"/dag_runs/\"\n+ run_date\n+ \"/tasks/\"\n+ task_id\n)\nelse:\nurl = (\napi_url\n+ \"/api/v1/dags/\"\n+ dag_id\n+ \"/dagRuns/scheduled__\"\n+ run_date\n+ \"/taskInstances/\"\n+ task_id\n)\nlog.info(f\"url= {url.replace(' ', '')}\")\nresponse = session.get(url.replace(\" \", \"\"), headers=headers)\nlog.info(\nf\"response.status_code= {response.status_code} response.text= {response.text}\"\n)\nif response.status_code == 200:\nlog.info(f\"response= {response.text}\")\njson_obj = json.loads(response.text)\nif type(json_obj) == dict:\no_task_status = json_obj[\"state\"]\nreturn o_task_status\nreturn o_task_status\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.MapDagScheduleHelper.get_version","title":"<code>get_version(cluster_id: str) -&gt; str</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def get_version(self, cluster_id: str) -&gt; str:\nsession = requests.Session()\nretries = Retry(\ntotal=10, backoff_factor=1, status_forcelist=[502, 503, 504, 500]\n)\nsession.mount(\"https://\", HTTPAdapter(max_retries=retries))\nversion_check_url = (\nself.get_airflow_api_url(cluster_id) + \"/admin/rest_api/api?api=version\"\n)\nlogging.info(version_check_url)\notoken = self.get_access_token()\nheaders = {\"Authorization\": \"Bearer \" + otoken, \"Accept\": \"application/json\"}\nout_version = \"UKN\"\nresponse = session.get(version_check_url, headers=headers, verify=False)\nif response.status_code == HTTPStatus.OK:\nout_version = response.json()[\"output\"]\nlog.info(response.text.encode(\"utf8\"))\nsession.close()\nreturn out_version\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor","title":"<code>brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor(external_dag_id, external_task_id, okta_conn_id, allowed_states = None, execution_delta = None, execution_delta_json = None, cluster_id = None, *args, **kwargs)</code>","text":"<p>             Bases: <code>BaseSensorOperator</code></p> Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def __init__(\nself,\nexternal_dag_id,\nexternal_task_id,\nokta_conn_id,\nallowed_states=None,\nexecution_delta=None,\nexecution_delta_json=None,\ncluster_id=None,\n*args,\n**kwargs,\n):\nsuper(TaskDependencySensor, self).__init__(*args, **kwargs)\nself.okta_conn_id = okta_conn_id\nself.allowed_states = allowed_states or [\"success\"]\nif execution_delta_json and execution_delta:\nraise Exception(\n\"Only one of `execution_date` or `execution_delta_json` maybe provided to Sensor; not more than one.\"\n)\nself.external_dag_id = external_dag_id\nself.external_task_id = external_task_id\nself.allowed_states = allowed_states\nself.execution_delta = execution_delta\nself.execution_delta_json = execution_delta_json\nself.cluster_id = cluster_id\nself._poke_count = 0\nself.dbx_wf_id = kwargs.get(\"dbx_wf_id\")\n</code></pre>"},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor-attributes","title":"Attributes","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.allowed_states","title":"<code>allowed_states = allowed_states</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.cluster_id","title":"<code>cluster_id = cluster_id</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.dbx_wf_id","title":"<code>dbx_wf_id = kwargs.get('dbx_wf_id')</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.execution_delta","title":"<code>execution_delta = execution_delta</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.execution_delta_json","title":"<code>execution_delta_json = execution_delta_json</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.external_dag_id","title":"<code>external_dag_id = external_dag_id</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.external_task_id","title":"<code>external_task_id = external_task_id</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.okta_conn_id","title":"<code>okta_conn_id = okta_conn_id</code>  <code>instance-attribute</code>","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor-functions","title":"Functions","text":""},{"location":"api/airflow_external_task_dependency/#brickflow_plugins.airflow.operators.external_tasks.TaskDependencySensor.poke","title":"<code>poke(context)</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/external_tasks.py</code> <pre><code>def poke(self, context):\nlog.info(f\"executing poke.. {self._poke_count}\")\nself._poke_count = self._poke_count + 1\nlogging.info(\"Poking.. {0} round\".format(str(self._poke_count)))\nexec_time = context[\"execution_date\"]\ntask_status = MapDagScheduleHelper(self.okta_conn_id).get_task_run_status(\nwf_id=self.external_dag_id,\ntask_id=self.external_task_id,\nrun_date=exec_time,\ncluster_id=self.cluster_id,\n)\nlog.info(f\"task_status= {task_status}\")\nif task_status not in self.allowed_states:\ncount = 0\nelse:\ncount = 1\nreturn count\n</code></pre>"},{"location":"api/airflow_native_operators/","title":"AirflowNativeOperators","text":""},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators-attributes","title":"Attributes","text":""},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators-classes","title":"Classes","text":""},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.BashOperatorModifier","title":"<code>brickflow_plugins.airflow.operators.native_operators.BashOperatorModifier</code>","text":"<p>             Bases: <code>OperatorModifier</code></p>"},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.BashOperatorModifier-functions","title":"Functions","text":""},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.BashOperatorModifier.modify","title":"<code>modify(operator: BashOperator, task: Task, workflow: Workflow) -&gt; Optional[BashOperator]</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/native_operators.py</code> <pre><code>@check_if(BashOperator)\ndef modify(\nself, operator: BashOperator, task: Task, workflow: Workflow\n) -&gt; Optional[\"BashOperator\"]:\nf = types.MethodType(_bash_execute, operator)\noperator.execute = f\noperator.on_kill = _bash_empty_on_kill\nreturn operator\n</code></pre>"},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.BranchPythonOperatorModifier","title":"<code>brickflow_plugins.airflow.operators.native_operators.BranchPythonOperatorModifier</code>","text":"<p>             Bases: <code>OperatorModifier</code></p>"},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.BranchPythonOperatorModifier-functions","title":"Functions","text":""},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.BranchPythonOperatorModifier.modify","title":"<code>modify(operator: BranchPythonOperator, task: Task, workflow: Workflow) -&gt; Optional[BranchPythonOperator]</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/native_operators.py</code> <pre><code>@check_if(BranchPythonOperator)\ndef modify(\nself, operator: BranchPythonOperator, task: Task, workflow: Workflow\n) -&gt; Optional[\"BranchPythonOperator\"]:\nf = types.MethodType(_skip_all_except, operator)\noperator.skip_all_except = f\nreturn operator\n</code></pre>"},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.ShortCircuitOperatorModifier","title":"<code>brickflow_plugins.airflow.operators.native_operators.ShortCircuitOperatorModifier</code>","text":"<p>             Bases: <code>OperatorModifier</code></p>"},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.ShortCircuitOperatorModifier-functions","title":"Functions","text":""},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators.ShortCircuitOperatorModifier.modify","title":"<code>modify(operator: ShortCircuitOperator, task: Task, workflow: Workflow) -&gt; Optional[ShortCircuitOperator]</code>","text":"Source code in <code>brickflow_plugins/airflow/operators/native_operators.py</code> <pre><code>@check_if(ShortCircuitOperator)\ndef modify(\nself, operator: ShortCircuitOperator, task: Task, workflow: Workflow\n) -&gt; Optional[\"ShortCircuitOperator\"]:\nf = types.MethodType(_short_circuit_execute, operator)\noperator.execute = f\nreturn operator\n</code></pre>"},{"location":"api/airflow_native_operators/#brickflow_plugins.airflow.operators.native_operators-functions","title":"Functions","text":""},{"location":"api/cli/","title":"CLI","text":""},{"location":"api/cli/#brickflow.cli-attributes","title":"Attributes","text":""},{"location":"api/cli/#brickflow.cli-classes","title":"Classes","text":""},{"location":"api/cli/#brickflow.cli.CdktfCmd","title":"<code>brickflow.cli.CdktfCmd</code>","text":"<p>             Bases: <code>click.Group</code></p>"},{"location":"api/cli/#brickflow.cli.CdktfCmd-functions","title":"Functions","text":""},{"location":"api/cli/#brickflow.cli.CdktfCmd.get_command","title":"<code>get_command(ctx: click.Context, cmd_name: str) -&gt; Optional[click.Command]</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def get_command(self, ctx: click.Context, cmd_name: str) -&gt; Optional[click.Command]:\n# during entry of command enable logging\n_ilog.setLevel(logging.INFO)\nif cmd_name == BrickflowDeployMode.CDKTF.value:\nreturn cdktf_command()\nelif cmd_name == BrickflowDeployMode.BUNDLE.value:\nreturn bundles_proxy_command()\n# elif cmd_name in [\"deploy\", \"diff\"]:\n#     return cdktf_command(cmd_name)\nelse:\nrv = click.Group.get_command(self, ctx, cmd_name)\nif rv is not None:\nreturn rv\nraise ctx.fail(f\"No such command '{cmd_name}'.\")\n</code></pre>"},{"location":"api/cli/#brickflow.cli-functions","title":"Functions","text":""},{"location":"api/cli/#brickflow.cli.bundle","title":"<code>brickflow.cli.bundle() -&gt; None</code>","text":"<p>CLI for proxying to databricks bundles cli.</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\ndef bundle() -&gt; None:\n\"\"\"CLI for proxying to databricks bundles cli.\"\"\"\n# Hack for having bundle show up as a command in brickflow\n# with documentation.\npass  # pragma: no cover\n</code></pre>"},{"location":"api/cli/#brickflow.cli.bundles_proxy_command","title":"<code>brickflow.cli.bundles_proxy_command() -&gt; click.Command</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def bundles_proxy_command() -&gt; click.Command:\ndef run_bundle_command(args: Optional[List[str]] = None, **_: Any) -&gt; None:\nbundle_cli_setup()\nbundle_cli = config(\nBrickflowEnvVars.BRICKFLOW_BUNDLE_CLI_EXEC.value, \"databricks\"\n)\nlog_important_versions(bundle_cli)\nexec_command(bundle_cli, \"bundle\", args or [])\n@click.command(\nname=\"bundles_cmd\",\nshort_help=\"CLI for proxying to databricks bundles cli..\",\ncontext_settings={\"ignore_unknown_options\": True},\nadd_help_option=False,\n)\n@click.argument(\"args\", nargs=-1)\ndef cmd(args: List[str]) -&gt; None:\n# check to make sure you are in project root and then set python path to whole dir\nrun_bundle_command(args=args)\nreturn cmd\n</code></pre>"},{"location":"api/cli/#brickflow.cli.cdktf","title":"<code>brickflow.cli.cdktf() -&gt; None</code>","text":"<p>CLI for proxying to cdktf cli.</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\ndef cdktf() -&gt; None:\n\"\"\"CLI for proxying to cdktf cli.\"\"\"\n# Hack for having cdktf show up as a command in brickflow\n# with documentation.\npass  # pragma: no cover\n</code></pre>"},{"location":"api/cli/#brickflow.cli.cdktf_command","title":"<code>brickflow.cli.cdktf_command(base_command: Optional[str] = None) -&gt; click.Command</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def cdktf_command(base_command: Optional[str] = None) -&gt; click.Command:\n@click.command(\nname=\"cdktf_cmd\",\nshort_help=\"CLI for proxying to CDKTF cli.\",\ncontext_settings={\"ignore_unknown_options\": True},\nadd_help_option=False,\ndeprecated=True,\n)\n@click.argument(\"args\", nargs=-1)\ndef cmd(args: Tuple[str]) -&gt; None:\n# check to make sure you are in project root and then set python path to whole dir\nexec_cdktf_command(base_command, args)\nreturn cmd\n</code></pre>"},{"location":"api/cli/#brickflow.cli.cdktf_env_set_options","title":"<code>brickflow.cli.cdktf_env_set_options(f: Callable) -&gt; Callable</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def cdktf_env_set_options(f: Callable) -&gt; Callable:\ndef local_mode_callback(ctx: click.Context, param: str, value: Any) -&gt; None:  # noqa\n# pylint: disable=unused-argument\nif value is not None and value is True:\n_ilog.info(\n\"Configuring environment to %s...\",\nBrickflowDefaultEnvs.LOCAL.value,\n)\nos.environ[\nBrickflowEnvVars.BRICKFLOW_ENV.value\n] = BrickflowDefaultEnvs.LOCAL.value\ndef deploy_only_workflows(\nctx: click.Context, param: str, value: Any\n) -&gt; None:  # noqa\n# pylint: disable=unused-argument\nif value:\nfor file in value:\nif file[-3:] != \".py\":\nraise ClickException(\"Should pass only python files as workflows\")\n_ilog.info(\"Brickflow will only deploy workflows: %s\", \", \".join(value))\nif (\nclick.confirm(\n\"This can delete all of your other workflows that are already deployed? Are you sure?\"\n)\nis False\n):\nctx.exit(0)\nos.environ[\nBrickflowEnvVars.BRICKFLOW_DEPLOY_ONLY_WORKFLOWS.value\n] = \",\".join(value)\ndef set_up_cdktf_for_workflow_dir(\nctx: click.Context, param: str, value: Any  # noqa\n) -&gt; None:\nif value is not None:\nreturn value\noptions = [\nclick.option(\n\"--local-mode\",\n\"-l\",\nis_flag=True,\ncallback=local_mode_callback,\nhelp=\"Set the environment flag to local and other components [TBD] are disabled in local mode.\",\n),\nclick.option(\n\"--workflows-dir\",\n\"-wd\",\ntype=click.Path(exists=True, file_okay=False),\nprompt=INTERACTIVE_MODE,\ncallback=set_up_cdktf_for_workflow_dir,\nhelp=\"Provide the workflow directory that has to be deployed\",\n),\nclick.option(\n\"--workflow\",\n\"-w\",\ntype=str,\nmultiple=True,\ncallback=deploy_only_workflows,\nhelp=\"\"\"Provide the workflow file names which you want to deploy, each file name separated by space!\n                    Example: bf deploy -p DEFAULT -l -w wf1.py -w wf2.py\"\"\",\n),\nclick.option(\n\"--env\",\n\"-e\",\ndefault=BrickflowDefaultEnvs.LOCAL.value,\ntype=str,\ncallback=bind_env_var(BrickflowEnvVars.BRICKFLOW_ENV.value),\nhelp=\"Set the environment value, certain tags [TBD] get added to the workflows based on this value.\",\n),\nclick.option(\n\"--repo-url\",\n\"-r\",\ndefault=None,\ntype=str,\ncallback=bind_env_var(BrickflowEnvVars.BRICKFLOW_GIT_REPO.value),\nhelp=\"The github url in which to run brickflow with.\",\n),\nclick.option(\n\"--git-ref\",\ndefault=None,\ntype=str,\ncallback=bind_env_var(BrickflowEnvVars.BRICKFLOW_GIT_REF.value),\nhelp=\"The commit/tag/branch to use in github.\",\n),\nclick.option(\n\"--git-provider\",\ndefault=None,\ntype=str,\ncallback=bind_env_var(BrickflowEnvVars.BRICKFLOW_GIT_PROVIDER.value),\nhelp=\"The github provider for brickflow this is used for configuring github on DBX jobs.\",\n),\nclick.option(\n\"--profile\",\n\"-p\",\ndefault=None,\ntype=str,\ncallback=bind_env_var(\nBrickflowEnvVars.BRICKFLOW_DATABRICKS_CONFIG_PROFILE.value\n),\nhelp=\"The databricks profile to use for authenticating to databricks during deployment.\",\n),\n]\nfor option in options:\nf = option(f)\nreturn f\n</code></pre>"},{"location":"api/cli/#brickflow.cli.cli","title":"<code>brickflow.cli.cli() -&gt; None</code>","text":"<p>CLI for managing Databricks Workflows</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@click.group(invoke_without_command=True, no_args_is_help=True, cls=CdktfCmd)\n@click.version_option(prog_name=\"brickflow\")\ndef cli() -&gt; None:\n\"\"\"CLI for managing Databricks Workflows\"\"\"\n</code></pre>"},{"location":"api/cli/#brickflow.cli.deploy","title":"<code>brickflow.cli.deploy(**kwargs: Any) -&gt; None</code>","text":"<p>CLI for deploying workflow projects.</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\n@click.option(\n\"--auto-approve\",\ntype=bool,\nis_flag=True,\nshow_default=True,\ndefault=False,\nhelp=\"Auto approve brickflow pipeline without being prompted to approve.\",\n)\n@click.option(\n\"--deploy-mode\",\ntype=click.Choice([\"cdktf\", \"bundle\"]),\nshow_default=True,\ndefault=\"cdktf\",\nhelp=\"Which deployment framework to use to deploy.\",\n)\n@click.option(\n\"--force-acquire-lock\",\ntype=bool,\nis_flag=True,\nshow_default=True,\ndefault=False,\nhelp=\"Force acquire lock for databricks bundles deploy.\",\n)\n@cdktf_env_set_options\ndef deploy(**kwargs: Any) -&gt; None:\n\"\"\"CLI for deploying workflow projects.\"\"\"\n# Hack for having cdktf show up as a command in brickflow\n# with documentation.\ndeploy_mode = get_deployment_mode(**kwargs)\nif deploy_mode == BrickflowDeployMode.CDKTF:\nmake_cdktf_json(**kwargs)\nexec_cdktf_command(\"deploy\", get_cdktf_specific_args(**kwargs))\nelse:\ndisable_project_name_in_env()\nbundle_deploy(**kwargs)\n</code></pre>"},{"location":"api/cli/#brickflow.cli.destroy","title":"<code>brickflow.cli.destroy(**kwargs: Any) -&gt; None</code>","text":"<p>CLI for destroying workflow projects.</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\n@click.option(\n\"--auto-approve\",\ntype=bool,\nis_flag=True,\nshow_default=True,\ndefault=False,\nhelp=\"Auto approve brickflow pipeline without being prompted to approve.\",\n)\n@click.option(\n\"--deploy-mode\",\ntype=click.Choice([\"cdktf\", \"bundle\"]),\nshow_default=True,\ndefault=\"cdktf\",\nhelp=\"Which deployment framework to use to deploy.\",\n)\n@click.option(\n\"--force-acquire-lock\",\ntype=bool,\nis_flag=True,\nshow_default=True,\ndefault=False,\nhelp=\"Force acquire lock for databricks bundles destroy.\",\n)\n@cdktf_env_set_options\ndef destroy(**kwargs: Any) -&gt; None:\n\"\"\"CLI for destroying workflow projects.\"\"\"\n# Hack for having cdktf show up as a command in brickflow\n# with documentation.\ndeploy_mode = get_deployment_mode(**kwargs)\nif deploy_mode == BrickflowDeployMode.CDKTF:\nmake_cdktf_json(**kwargs)\nexec_cdktf_command(\"destroy\", get_cdktf_specific_args(**kwargs))\nelse:\ndisable_project_name_in_env()\nbundle_destroy(**kwargs)\n</code></pre>"},{"location":"api/cli/#brickflow.cli.diff","title":"<code>brickflow.cli.diff(**kwargs: Any) -&gt; None</code>","text":"<p>CLI for identifying diff in projects (only cdktf supported).</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\n@cdktf_env_set_options\ndef diff(**kwargs: Any) -&gt; None:\n\"\"\"CLI for identifying diff in projects (only cdktf supported).\"\"\"\n# Hack for having cdktf show up as a command in brickflow\n# with documentation.\nmake_cdktf_json(**kwargs)\nexec_cdktf_command(\"diff\", [])\n</code></pre>"},{"location":"api/cli/#brickflow.cli.disable_project_name_in_env","title":"<code>brickflow.cli.disable_project_name_in_env() -&gt; None</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def disable_project_name_in_env() -&gt; None:\n# TODO: delete this when deploy commands are gone\n# used for legacy bundles deploy and destroy commands\n# disable multiple projects in same directory\nos.environ[BrickflowEnvVars.BRICKFLOW_USE_PROJECT_NAME.value] = \"False\"\n</code></pre>"},{"location":"api/cli/#brickflow.cli.docs","title":"<code>brickflow.cli.docs() -&gt; None</code>","text":"<p>Use to open docs in your browser...</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\ndef docs() -&gt; None:\n\"\"\"Use to open docs in your browser...\"\"\"\ndocs_site = \"https://verbose-garbanzo-6b8a1ae2.pages.github.io/\"\nwebbrowser.open(docs_site, new=2)\nclick.echo(f\"Opening browser for docs... site: {docs_site}\")\n</code></pre>"},{"location":"api/cli/#brickflow.cli.get_cdktf_specific_args","title":"<code>brickflow.cli.get_cdktf_specific_args(**kwargs: Dict[str, Any]) -&gt; List[str]</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def get_cdktf_specific_args(**kwargs: Dict[str, Any]) -&gt; List[str]:\nargs = []\nif kwargs.get(\"auto_approve\", False) is True:\nargs.append(\"--auto-approve\")\nreturn args\n</code></pre>"},{"location":"api/cli/#brickflow.cli.get_deployment_mode","title":"<code>brickflow.cli.get_deployment_mode(**kwargs: Dict[str, Any]) -&gt; BrickflowDeployMode</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def get_deployment_mode(**kwargs: Dict[str, Any]) -&gt; BrickflowDeployMode:\n# set deployment mode for cdktf or bundle\nos.environ[BrickflowEnvVars.BRICKFLOW_DEPLOYMENT_MODE.value] = str(\nkwargs.get(\"deploy_mode\", BrickflowDeployMode.CDKTF.value)\n)\nif (\nkwargs.get(\"deploy_mode\", BrickflowDeployMode.CDKTF.value)\n== BrickflowDeployMode.CDKTF.value\n):\nreturn BrickflowDeployMode.CDKTF\nelse:\nreturn BrickflowDeployMode.BUNDLE\n</code></pre>"},{"location":"api/cli/#brickflow.cli.make_cdktf_json","title":"<code>brickflow.cli.make_cdktf_json(**kwargs: Any) -&gt; None</code>","text":"Source code in <code>brickflow/cli/__init__.py</code> <pre><code>def make_cdktf_json(**kwargs: Any) -&gt; None:\nwd: Optional[str] = kwargs.get(\"workflows_dir\")\nif wd is None:\nraise ValueError(\n\"workflows_dir not set, please set it using --workflows-dir or -wd\"\n)\nidempotent_cdktf_out(wd)\n</code></pre>"},{"location":"api/cli/#brickflow.cli.sync","title":"<code>brickflow.cli.sync(**kwargs: Any) -&gt; None</code>","text":"<p>Synchronize your bundle tree to databricks workspace (only supported by bundle deployment mode).</p> Source code in <code>brickflow/cli/__init__.py</code> <pre><code>@cli.command\n@click.option(\n\"--deploy-mode\",\ntype=click.Choice([\"bundle\"]),\nshow_default=True,\ndefault=\"bundle\",\nhelp=\"Which deployment framework to use to deploy.\",\n)\n@click.option(\n\"--watch\",\ntype=bool,\nis_flag=True,\nshow_default=True,\ndefault=False,\nhelp=\"Enable filewatcher to sync files over.\",\n)\n@click.option(\n\"--full\",\ntype=bool,\nis_flag=True,\nshow_default=True,\ndefault=False,\nhelp=\"Run a full sync.\",\n)\n@click.option(\n\"--interval-duration\",\ntype=str,\nshow_default=True,\ndefault=None,\nhelp=\"File system polling interval (for --watch).\",\n)\n@click.option(\n\"--debug\",\ntype=str,\nshow_default=True,\ndefault=None,\nhelp=\"File system polling interval (for --watch).\",\n)\n@cdktf_env_set_options\ndef sync(**kwargs: Any) -&gt; None:\n\"\"\"Synchronize your bundle tree to databricks workspace (only supported by bundle deployment mode).\"\"\"\ndeploy_mode = get_deployment_mode(**kwargs)\nif deploy_mode == BrickflowDeployMode.BUNDLE:\nbundle_sync(**kwargs)\nelse:\nraise ClickException(\n\"Unsupported deploy mode for sync; currently only supports bundle deploy mode.\"\n)\n</code></pre>"},{"location":"api/compute/","title":"Compute","text":""},{"location":"api/compute/#brickflow.engine.compute-classes","title":"Classes","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster","title":"<code>brickflow.engine.compute.Cluster</code>  <code>dataclass</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster-attributes","title":"Attributes","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.aws_attributes","title":"<code>aws_attributes: Optional[Dict[str, Any]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.custom_tags","title":"<code>custom_tags: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.data_security_mode","title":"<code>data_security_mode: str = DataSecurityMode.SINGLE_USER</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.dlt_auto_scale_mode","title":"<code>dlt_auto_scale_mode: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.driver_instance_pool_id","title":"<code>driver_instance_pool_id: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.driver_node_type_id","title":"<code>driver_node_type_id: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.enable_elastic_disk","title":"<code>enable_elastic_disk: Optional[bool] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.existing_cluster_id","title":"<code>existing_cluster_id: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.init_scripts","title":"<code>init_scripts: Optional[List[Dict[str, str]]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.instance_pool_id","title":"<code>instance_pool_id: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.is_new_job_cluster","title":"<code>is_new_job_cluster: bool</code>  <code>property</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.job_task_field_dict","title":"<code>job_task_field_dict: Dict[str, str]</code>  <code>property</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.max_workers","title":"<code>max_workers: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.min_workers","title":"<code>min_workers: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.node_type_id","title":"<code>node_type_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.num_workers","title":"<code>num_workers: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.policy_id","title":"<code>policy_id: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.runtime_engine","title":"<code>runtime_engine: Optional[Literal['STANDARD', 'PHOTON']] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.spark_conf","title":"<code>spark_conf: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.spark_env_vars","title":"<code>spark_env_vars: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.spark_version","title":"<code>spark_version: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster-functions","title":"Functions","text":""},{"location":"api/compute/#brickflow.engine.compute.Cluster.__hash__","title":"<code>__hash__() -&gt; int</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>def __hash__(self) -&gt; int:\n# dedupe dicts and lists which are default un hashable. Easiest way to identify dupes.\nreturn hash(json.dumps(self.as_dict()))\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Cluster.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>def __post_init__(self) -&gt; None:\nself.validate()\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Cluster.as_dict","title":"<code>as_dict(is_dlt_cluster: bool = False, allowed_fields: Optional[List[str]] = None, remove_fields: Optional[List[str]] = None) -&gt; Dict[str, Any]</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>def as_dict(\nself,\nis_dlt_cluster: bool = False,\nallowed_fields: Optional[List[str]] = None,\nremove_fields: Optional[List[str]] = None,\n) -&gt; Dict[str, Any]:\nd = dataclasses.asdict(self)\nd = {**d, **self.autoscale(is_dlt_cluster=is_dlt_cluster)}\n# if allowed fields are provided and check if value is in set\nself.cleanup(d, allowed_fields=allowed_fields, remove_fields=remove_fields)\nreturn d\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Cluster.autoscale","title":"<code>autoscale(is_dlt_cluster: bool = False) -&gt; Dict[str, Any]</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>def autoscale(self, is_dlt_cluster: bool = False) -&gt; Dict[str, Any]:\nif self.min_workers is not None and self.max_workers is not None:\nresp: Dict[str, Dict[str, Optional[str | int]]] = {\n\"autoscale\": {\n\"min_workers\": self.min_workers,\n\"max_workers\": self.max_workers,\n}\n}\nif is_dlt_cluster is True:\nresp[\"autoscale\"][\"mode\"] = self.dlt_auto_scale_mode\nreturn resp\nreturn {}\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Cluster.cleanup","title":"<code>cleanup(d: Dict[str, Any], allowed_fields: Optional[List[str]] = None, remove_fields: Optional[List[str]] = None) -&gt; None</code>  <code>staticmethod</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>@staticmethod\ndef cleanup(\nd: Dict[str, Any],\nallowed_fields: Optional[List[str]] = None,\nremove_fields: Optional[List[str]] = None,\n) -&gt; None:\nd.pop(\"min_workers\", None)\nd.pop(\"max_workers\", None)\nd.pop(\"dlt_auto_scale_mode\", None)\nd.pop(\"existing_cluster_id\", None)\nremove_fields = remove_fields or []\nfor k in list(d.keys()):\n# if allowed fields are provided and check if value is in set\nif allowed_fields and k not in allowed_fields:\nd.pop(k, None)\nif k in remove_fields:\nd.pop(k, None)\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Cluster.from_existing_cluster","title":"<code>from_existing_cluster(existing_cluster_id: str) -&gt; 'Cluster'</code>  <code>classmethod</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>@classmethod\ndef from_existing_cluster(cls, existing_cluster_id: str) -&gt; \"Cluster\":\n# just some stub value\nreturn Cluster(\nexisting_cluster_id,\nexisting_cluster_id,\nexisting_cluster_id,\nexisting_cluster_id=existing_cluster_id,\n)\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Cluster.validate","title":"<code>validate() -&gt; None</code>","text":"Source code in <code>brickflow/engine/compute.py</code> <pre><code>def validate(self) -&gt; None:\nassert not (\nself.num_workers is not None\nand self.min_workers is not None\nand self.max_workers is not None\n), \"Num workers should not be provided with min and max workers\"\nassert not (\n(self.min_workers is None and self.max_workers is not None)\nor (self.min_workers is not None and self.max_workers is None)\n), \"Both min workers and max workers should be present if one is provided\"\n# noinspection PyTypeChecker\nassert not (\n(self.min_workers is not None and self.max_workers is not None)\nand (self.min_workers &gt; self.max_workers)\n), \"Min workers should be less than max workers\"\n</code></pre>"},{"location":"api/compute/#brickflow.engine.compute.Runtimes","title":"<code>brickflow.engine.compute.Runtimes</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes-attributes","title":"Attributes","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_4_X_AARCH64_PHOTON_SCALA2_12_LTS","title":"<code>RUNTIME_10_4_X_AARCH64_PHOTON_SCALA2_12_LTS = '10.4.x-aarch64-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_4_X_AARCH64_SCALA2_12_LTS","title":"<code>RUNTIME_10_4_X_AARCH64_SCALA2_12_LTS = '10.4.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_4_X_CPU_ML_SCALA2_12_LTS","title":"<code>RUNTIME_10_4_X_CPU_ML_SCALA2_12_LTS = '10.4.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_4_X_GPU_ML_SCALA2_12_LTS","title":"<code>RUNTIME_10_4_X_GPU_ML_SCALA2_12_LTS = '10.4.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_4_X_PHOTON_SCALA2_12_LTS","title":"<code>RUNTIME_10_4_X_PHOTON_SCALA2_12_LTS = '10.4.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_4_X_SCALA2_12_LTS","title":"<code>RUNTIME_10_4_X_SCALA2_12_LTS = '10.4.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_5_X_AARCH64_PHOTON_SCALA2_12","title":"<code>RUNTIME_10_5_X_AARCH64_PHOTON_SCALA2_12 = '10.5.x-aarch64-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_5_X_AARCH64_SCALA2_12","title":"<code>RUNTIME_10_5_X_AARCH64_SCALA2_12 = '10.5.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_5_X_CPU_ML_SCALA2_12","title":"<code>RUNTIME_10_5_X_CPU_ML_SCALA2_12 = '10.5.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_5_X_GPU_ML_SCALA2_12","title":"<code>RUNTIME_10_5_X_GPU_ML_SCALA2_12 = '10.5.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_5_X_PHOTON_SCALA2_12","title":"<code>RUNTIME_10_5_X_PHOTON_SCALA2_12 = '10.5.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_10_5_X_SCALA2_12","title":"<code>RUNTIME_10_5_X_SCALA2_12 = '10.5.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_0_X_AARCH64_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_0_X_AARCH64_PHOTON_SCALA2_12 = '11.0.x-aarch64-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_0_X_AARCH64_SCALA2_12","title":"<code>RUNTIME_11_0_X_AARCH64_SCALA2_12 = '11.0.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_0_X_CPU_ML_SCALA2_12","title":"<code>RUNTIME_11_0_X_CPU_ML_SCALA2_12 = '11.0.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_0_X_GPU_ML_SCALA2_12","title":"<code>RUNTIME_11_0_X_GPU_ML_SCALA2_12 = '11.0.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_0_X_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_0_X_PHOTON_SCALA2_12 = '11.0.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_0_X_SCALA2_12","title":"<code>RUNTIME_11_0_X_SCALA2_12 = '11.0.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_1_X_AARCH64_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_1_X_AARCH64_PHOTON_SCALA2_12 = '11.1.x-aarch64-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_1_X_AARCH64_SCALA2_12","title":"<code>RUNTIME_11_1_X_AARCH64_SCALA2_12 = '11.1.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_1_X_CPU_ML_SCALA2_12","title":"<code>RUNTIME_11_1_X_CPU_ML_SCALA2_12 = '11.1.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_1_X_GPU_ML_SCALA2_12","title":"<code>RUNTIME_11_1_X_GPU_ML_SCALA2_12 = '11.1.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_1_X_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_1_X_PHOTON_SCALA2_12 = '11.1.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_1_X_SCALA2_12","title":"<code>RUNTIME_11_1_X_SCALA2_12 = '11.1.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_2_X_AARCH64_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_2_X_AARCH64_PHOTON_SCALA2_12 = '11.2.x-aarch64-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_2_X_AARCH64_SCALA2_12","title":"<code>RUNTIME_11_2_X_AARCH64_SCALA2_12 = '11.2.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_2_X_CPU_ML_SCALA2_12","title":"<code>RUNTIME_11_2_X_CPU_ML_SCALA2_12 = '11.2.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_2_X_GPU_ML_SCALA2_12","title":"<code>RUNTIME_11_2_X_GPU_ML_SCALA2_12 = '11.2.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_2_X_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_2_X_PHOTON_SCALA2_12 = '11.2.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_2_X_SCALA2_12","title":"<code>RUNTIME_11_2_X_SCALA2_12 = '11.2.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_3_X_AARCH64_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_3_X_AARCH64_PHOTON_SCALA2_12 = '11.3.x-aarch64-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_3_X_AARCH64_SCALA2_12","title":"<code>RUNTIME_11_3_X_AARCH64_SCALA2_12 = '11.3.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_3_X_CPU_ML_SCALA2_12","title":"<code>RUNTIME_11_3_X_CPU_ML_SCALA2_12 = '11.3.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_3_X_GPU_ML_SCALA2_12","title":"<code>RUNTIME_11_3_X_GPU_ML_SCALA2_12 = '11.3.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_3_X_PHOTON_SCALA2_12","title":"<code>RUNTIME_11_3_X_PHOTON_SCALA2_12 = '11.3.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_11_3_X_SCALA2_12","title":"<code>RUNTIME_11_3_X_SCALA2_12 = '11.3.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_7_3_X_CPU_ML_SCALA2_12_LTS","title":"<code>RUNTIME_7_3_X_CPU_ML_SCALA2_12_LTS = '7.3.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_7_3_X_GPU_ML_SCALA2_12_LTS","title":"<code>RUNTIME_7_3_X_GPU_ML_SCALA2_12_LTS = '7.3.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_7_3_X_HLS_SCALA2_12_LTS","title":"<code>RUNTIME_7_3_X_HLS_SCALA2_12_LTS = '7.3.x-hls-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_7_3_X_SCALA2_12_LTS","title":"<code>RUNTIME_7_3_X_SCALA2_12_LTS = '7.3.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_9_1_X_AARCH64_SCALA2_12_LTS","title":"<code>RUNTIME_9_1_X_AARCH64_SCALA2_12_LTS = '9.1.x-aarch64-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_9_1_X_CPU_ML_SCALA2_12_LTS","title":"<code>RUNTIME_9_1_X_CPU_ML_SCALA2_12_LTS = '9.1.x-cpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_9_1_X_GPU_ML_SCALA2_12_LTS","title":"<code>RUNTIME_9_1_X_GPU_ML_SCALA2_12_LTS = '9.1.x-gpu-ml-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_9_1_X_PHOTON_SCALA2_12_LTS","title":"<code>RUNTIME_9_1_X_PHOTON_SCALA2_12_LTS = '9.1.x-photon-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/compute/#brickflow.engine.compute.Runtimes.RUNTIME_9_1_X_SCALA2_12_LTS","title":"<code>RUNTIME_9_1_X_SCALA2_12_LTS = '9.1.x-scala2.12'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/","title":"Context","text":""},{"location":"api/context/#brickflow.context.context-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.BRANCH_SKIP_EXCEPT","title":"<code>brickflow.context.context.BRANCH_SKIP_EXCEPT = 'branch_skip_except'</code>  <code>module-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.RETURN_VALUE_KEY","title":"<code>brickflow.context.context.RETURN_VALUE_KEY = 'return_value'</code>  <code>module-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.SKIP_EXCEPT_HACK","title":"<code>brickflow.context.context.SKIP_EXCEPT_HACK = 'brickflow_hack_skip_all'</code>  <code>module-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.T","title":"<code>brickflow.context.context.T = TypeVar('T')</code>  <code>module-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.ctx","title":"<code>brickflow.context.context.ctx = Context()</code>  <code>module-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context-classes","title":"Classes","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables","title":"<code>brickflow.context.context.BrickflowBuiltInTaskVariables</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.job_id","title":"<code>job_id = 'brickflow_job_id'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.parent_run_id","title":"<code>parent_run_id = 'brickflow_parent_run_id'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.run_id","title":"<code>run_id = 'brickflow_run_id'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.start_date","title":"<code>start_date = 'brickflow_start_date'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.start_time","title":"<code>start_time = 'brickflow_start_time'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.task_key","title":"<code>task_key = 'brickflow_task_key'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowBuiltInTaskVariables.task_retry_count","title":"<code>task_retry_count = 'brickflow_task_retry_count'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables","title":"<code>brickflow.context.context.BrickflowInternalVariables</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables.env","title":"<code>env = BrickflowEnvVars.BRICKFLOW_ENV.value.lower()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables.only_run_tasks","title":"<code>only_run_tasks = 'brickflow_internal_only_run_tasks'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables.task_id","title":"<code>task_id = 'brickflow_internal_task_name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables.workflow_id","title":"<code>workflow_id = 'brickflow_internal_workflow_name'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables.workflow_prefix","title":"<code>workflow_prefix = 'brickflow_internal_workflow_prefix'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowInternalVariables.workflow_suffix","title":"<code>workflow_suffix = 'brickflow_internal_workflow_suffix'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs","title":"<code>brickflow.context.context.BrickflowTaskComs</code>  <code>dataclass</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs.dbutils","title":"<code>dbutils: Optional[Any] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs.storage","title":"<code>storage: Dict[str, Any] = field(init=False, default_factory=lambda : {})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs-functions","title":"Functions","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs.get","title":"<code>get(task_id: str, key: Optional[str] = None) -&gt; Any</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def get(self, task_id: str, key: Optional[str] = None) -&gt; Any:\nif key is None:\nreturn BrickflowTaskComsDict(task_id=task_id, task_coms=self)\nif self.dbutils is not None:\nencoded_value = self.dbutils.jobs.taskValues.get(\nkey=key, taskKey=task_id, debugValue=\"debug\"\n)\nreturn BrickflowTaskComsObject.from_encoded_value(encoded_value).value\nelse:\n# TODO: logging using local task coms\nencoded_value = self.storage[self._key(task_id, key)]\nreturn BrickflowTaskComsObject.from_encoded_value(encoded_value).value\n</code></pre>"},{"location":"api/context/#brickflow.context.context.BrickflowTaskComs.put","title":"<code>put(task_id: str, key: str, value: Any) -&gt; None</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def put(self, task_id: str, key: str, value: Any) -&gt; None:\nencoded_value = BrickflowTaskComsObject(value).to_encoded_value\nif self.dbutils is not None:\nself.dbutils.jobs.taskValues.set(key, encoded_value)\nelse:\n# TODO: logging using local task coms\nself.storage[self._key(task_id, key)] = encoded_value\n</code></pre>"},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsDict","title":"<code>brickflow.context.context.BrickflowTaskComsDict</code>  <code>dataclass</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsDict-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsDict.task_coms","title":"<code>task_coms: BrickflowTaskComs</code>  <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsDict.task_id","title":"<code>task_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsObject","title":"<code>brickflow.context.context.BrickflowTaskComsObject</code>  <code>dataclass</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsObject-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsObject.to_encoded_value","title":"<code>to_encoded_value: str</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsObject.value","title":"<code>value: Any</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsObject-functions","title":"Functions","text":""},{"location":"api/context/#brickflow.context.context.BrickflowTaskComsObject.from_encoded_value","title":"<code>from_encoded_value(encoded_value: Union[str, bytes]) -&gt; BrickflowTaskComsObject</code>  <code>classmethod</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>@classmethod\ndef from_encoded_value(\ncls, encoded_value: Union[str, bytes]\n) -&gt; \"BrickflowTaskComsObject\":\ntry:\n_encoded_value = (\nencoded_value\nif isinstance(encoded_value, bytes)\nelse encoded_value.encode(\"utf-8\")\n)\nb64_bytes = base64.b64decode(_encoded_value)\nreturn cls(pickle.loads(b64_bytes).value)\nexcept binascii.Error:\n_decoded_value = (\nencoded_value.decode(\"utf-8\")\nif isinstance(encoded_value, bytes)\nelse encoded_value\n)\nreturn cls(_decoded_value)\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context","title":"<code>brickflow.context.context.Context()</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def __init__(self) -&gt; None:\n# Order of init matters todo: fix this\nself._dbutils: Optional[Any] = None\nself._spark: Optional[Any] = None\nself._task_coms: BrickflowTaskComs\nself._current_task: Optional[str] = None\nself._configure()\nself._current_project: Optional[str] = None\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.Context.current_project","title":"<code>current_project: Optional[str]</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context.current_task","title":"<code>current_task: Optional[str]</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context.dbutils","title":"<code>dbutils: DBUtils</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context.env","title":"<code>env: str</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context.log","title":"<code>log: logging.Logger</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context.spark","title":"<code>spark: SparkSession</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context.task_coms","title":"<code>task_coms: BrickflowTaskComs</code>  <code>property</code>","text":""},{"location":"api/context/#brickflow.context.context.Context-functions","title":"Functions","text":""},{"location":"api/context/#brickflow.context.context.Context.dbutils_widget_get_or_else","title":"<code>dbutils_widget_get_or_else(key: str, debug: Optional[str]) -&gt; Optional[str]</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>@deprecated\ndef dbutils_widget_get_or_else(\nself, key: str, debug: Optional[str]\n) -&gt; Optional[str]:\ntry:\nreturn self.dbutils.widgets.get(key)\nexcept Exception:\n# todo: log error\nreturn debug\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.get_by_env","title":"<code>get_by_env(purpose: str, *, default: Optional[T] = None, local: Optional[T] = None, dev: Optional[T] = None, non_prod: Optional[T] = None, test: Optional[T] = None, qa: Optional[T] = None, prod: Optional[T] = None, uat: Optional[T] = None, **kwargs: Optional[T]) -&gt; Optional[T]</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def get_by_env(\nself,\npurpose: str,\n*,\ndefault: Optional[T] = None,\nlocal: Optional[T] = None,\ndev: Optional[T] = None,\nnon_prod: Optional[T] = None,\ntest: Optional[T] = None,\nqa: Optional[T] = None,\nprod: Optional[T] = None,\nuat: Optional[T] = None,\n**kwargs: Optional[T],\n) -&gt; Optional[T]:\n# deep copy without modifying kwargs\ndef add_if_not_none(\n_d: Dict[str, Optional[T]], _k: str, _v: Optional[T]\n) -&gt; None:\nif _v is None:\nreturn\n_d[_k] = _v\n_dict = copy.deepcopy(kwargs)\nadd_if_not_none(_dict, \"local\", local)\nadd_if_not_none(_dict, \"non_prod\", non_prod)\nadd_if_not_none(_dict, \"dev\", dev)\nadd_if_not_none(_dict, \"test\", test)\nadd_if_not_none(_dict, \"qa\", qa)\nadd_if_not_none(_dict, \"prod\", prod)\nadd_if_not_none(_dict, \"uat\", uat)\n_env = self.env\n_ilog.info(\"Configuring: %s; Using env: '%s' to fetch value...\", purpose, _env)\nif _env not in _dict and default is None:\nraise KeyError(\nf\"Configuring: {purpose}; Unable to find environment key: {_env}, \"\nf\"only found env definitions: {list(_dict.keys())}\"\n)\nif _env not in _dict and default is not None:\n_ilog.info(\n\"Configuring: %s; Found no value configured with env: '%s' using default value...\",\npurpose,\n_env,\n)\nres = _dict.get(_env, default)\nreturn res\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.get_parameter","title":"<code>get_parameter(key: str, debug: Optional[str] = None) -&gt; Optional[str]</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def get_parameter(self, key: str, debug: Optional[str] = None) -&gt; Optional[str]:\ntry:\nreturn self.dbutils.widgets.get(key)\nexcept Exception:\n# todo: log error\n_ilog.debug(\"Unable to get parameter: %s from dbutils\", key)\nreturn debug\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.get_return_value","title":"<code>get_return_value(task_key: Union[str, Callable]) -&gt; Any</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def get_return_value(self, task_key: Union[str, Callable]) -&gt; Any:\ntask_key = task_key.__name__ if callable(task_key) else task_key\nreturn self.task_coms.get(task_key, RETURN_VALUE_KEY)\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.is_local","title":"<code>is_local() -&gt; bool</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def is_local(self) -&gt; bool:\nreturn self.env == BrickflowDefaultEnvs.LOCAL.value\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.job_id","title":"<code>job_id(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the job_id value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.job_id)\ndef job_id(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the job_id value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.parent_run_id","title":"<code>parent_run_id(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the parent_run_id value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.parent_run_id)\ndef parent_run_id(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the parent_run_id value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.run_id","title":"<code>run_id(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the run_id value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.run_id)\ndef run_id(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the run_id value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.set_current_project","title":"<code>set_current_project(project: str) -&gt; None</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def set_current_project(self, project: str) -&gt; None:\n# TODO: not a public api move to internal context or deployment context\nself._ensure_valid_project(project)\nself._current_project = project\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.skip_all_except","title":"<code>skip_all_except(branch_task: Union[Callable, str]) -&gt; None</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def skip_all_except(self, branch_task: Union[Callable, str]) -&gt; None:\nif self._current_task is None:\nraise RuntimeError(\"Current task is empty unable to skip...\")\nbranch_task_key = (\nbranch_task.__name__\nif callable(branch_task) and hasattr(branch_task, \"__name__\") is True\nelse branch_task\n)\nself._task_coms.put(self._current_task, BRANCH_SKIP_EXCEPT, branch_task_key)\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.skip_all_following","title":"<code>skip_all_following() -&gt; None</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def skip_all_following(self) -&gt; None:\nif self._current_task is None:\nraise RuntimeError(\"Current task is empty unable to skip...\")\nself._task_coms.put(self._current_task, BRANCH_SKIP_EXCEPT, SKIP_EXCEPT_HACK)\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.start_date","title":"<code>start_date(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the start_date value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.start_date)\ndef start_date(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the start_date value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.start_time","title":"<code>start_time(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the start_time value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.start_time)\ndef start_time(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the start_time value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.task_key","title":"<code>task_key(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the task_key value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.task_key)\ndef task_key(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the task_key value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.Context.task_retry_count","title":"<code>task_retry_count(*, debug: Optional[str] = None) -&gt; Any</code>","text":"<p>This function fetches the task_retry_count value using the bind_variable decorator. The implementation is intentionally empty because the decorator handles the logic.</p> Source code in <code>brickflow/context/context.py</code> <pre><code>@bind_variable(BrickflowBuiltInTaskVariables.task_retry_count)\ndef task_retry_count(self, *, debug: Optional[str] = None) -&gt; Any:\n\"\"\"\n    This function fetches the task_retry_count value using the bind_variable decorator.\n    The implementation is intentionally empty because the decorator handles the logic.\n    \"\"\"\npass\n</code></pre>"},{"location":"api/context/#brickflow.context.context.ContextMode","title":"<code>brickflow.context.context.ContextMode</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/context/#brickflow.context.context.ContextMode-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.ContextMode.databricks","title":"<code>databricks = 'databricks'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.ContextMode.not_databricks","title":"<code>not_databricks = 'not_databricks'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context.TaskComsObjectResult","title":"<code>brickflow.context.context.TaskComsObjectResult</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/context/#brickflow.context.context.TaskComsObjectResult-attributes","title":"Attributes","text":""},{"location":"api/context/#brickflow.context.context.TaskComsObjectResult.NO_RESULTS","title":"<code>NO_RESULTS = 'NO_RESULTS'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/context/#brickflow.context.context-functions","title":"Functions","text":""},{"location":"api/context/#brickflow.context.context.bind_variable","title":"<code>brickflow.context.context.bind_variable(builtin: BrickflowBuiltInTaskVariables) -&gt; Callable</code>","text":"Source code in <code>brickflow/context/context.py</code> <pre><code>def bind_variable(builtin: BrickflowBuiltInTaskVariables) -&gt; Callable:\ndef wrapper(f: Callable) -&gt; Callable:\n@functools.wraps(f)\ndef func(*args, **kwargs):  # type: ignore\n_self: Context = args[0]  # type: ignore\ndebug = kwargs[\"debug\"]\nf(*args, **kwargs)  # no-op\nif _self.dbutils is not None:\nreturn _self.get_parameter(builtin.value, debug)\nreturn debug\nreturn func\nreturn wrapper\n</code></pre>"},{"location":"api/misc/","title":"Misc","text":""},{"location":"api/project/","title":"Project","text":""},{"location":"api/project/#brickflow.engine.project-classes","title":"Classes","text":""},{"location":"api/project/#brickflow.engine.project.Project","title":"<code>brickflow.engine.project.Project</code>  <code>dataclass</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project-attributes","title":"Attributes","text":""},{"location":"api/project/#brickflow.engine.project.Project.batch","title":"<code>batch: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.bundle_base_path","title":"<code>bundle_base_path: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.bundle_obj_name","title":"<code>bundle_obj_name: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.codegen_kwargs","title":"<code>codegen_kwargs: Optional[Dict[str, Any]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.codegen_mechanism","title":"<code>codegen_mechanism: Optional[Type[CodegenInterface]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.debug_execute_task","title":"<code>debug_execute_task: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.debug_execute_workflow","title":"<code>debug_execute_workflow: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.enable_plugins","title":"<code>enable_plugins: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.entry_point_path","title":"<code>entry_point_path: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.git_reference","title":"<code>git_reference: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.git_repo","title":"<code>git_repo: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.libraries","title":"<code>libraries: Optional[List[TaskLibrary]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.mode","title":"<code>mode: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.provider","title":"<code>provider: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project.s3_backend","title":"<code>s3_backend: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/project/#brickflow.engine.project.Project-functions","title":"Functions","text":""},{"location":"api/project/#brickflow.engine.project.Project.__enter__","title":"<code>__enter__() -&gt; _Project</code>","text":"Source code in <code>brickflow/engine/project.py</code> <pre><code>def __enter__(self) -&gt; \"_Project\":\nself._project = _Project(\nself.name,\nself.git_repo,\nself.provider,\nself.git_reference,\nself.s3_backend,\nself.entry_point_path,\nlibraries=self.libraries,\nbatch=self.batch,\nbundle_obj_name=self.bundle_obj_name,\nbundle_base_path=self.bundle_base_path,\n)\nreturn self._project\n</code></pre>"},{"location":"api/project/#brickflow.engine.project.Project.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb) -&gt; None</code>","text":"Source code in <code>brickflow/engine/project.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:  # type: ignore\nif exc_type is not None:\nerror_types = {Stage.deploy: DeployError, Stage.execute: ExecuteError}\nraise error_types[self._mode](\nf\"Oops... failed during: {self._mode}\"\n) from exc_val\nif len(self._project.workflows) == 0:\n_ilog.info(\"Doing nothing no workflows...\")\nreturn\nif self._mode == Stage.unittest:\n# Mode is purely for testing purposes for the _Project internal class\n_ilog.info(\"Running unit tests...\")\nreturn\nif self._mode.value == Stage.deploy.value:\n_ilog.info(\"Deploying changes... to %s\", ctx.env)\nif self.codegen_mechanism is None:\nraise ValueError(\n\"codegen_mechanism cannot be None; please raise a github issue for this.\"\n)\ncodegen = self.codegen_mechanism(\nproject=self._project,\nid_=f\"{ctx.env}_{self.name}\",\nenv=ctx.env,\n**(self.codegen_kwargs or {}),\n)\ncodegen.synth()\nif self._mode.value == Stage.execute.value:\nwf_id = ctx.get_parameter(\nBrickflowInternalVariables.workflow_id.value,\nself.debug_execute_workflow,\n)\nt_id = ctx.get_parameter(\nBrickflowInternalVariables.task_id.value, self.debug_execute_task\n)\nif wf_id is None or t_id is None:\n_ilog.info(\n\"No workflow id or task key was able to found; doing nothing...\"\n)\nreturn\nworkflow = self._project.get_workflow(wf_id)\ntask = workflow.get_task(t_id)\ntask.execute()\n</code></pre>"},{"location":"api/project/#brickflow.engine.project.Project.__post_init__","title":"<code>__post_init__() -&gt; None</code>","text":"Source code in <code>brickflow/engine/project.py</code> <pre><code>def __post_init__(self) -&gt; None:\n# during entry of project enable logging\n_ilog.setLevel(logging.INFO)\nself._mode = Stage[\nconfig(BrickflowEnvVars.BRICKFLOW_MODE.value, default=Stage.execute.value)\n]\nself.entry_point_path = self.entry_point_path or get_caller_info()\ndeploy_settings = BrickflowProjectDeploymentSettings()\n# setup current_project\nenv_project_name = deploy_settings.brickflow_project_name\nself.libraries = self.libraries or []\nif deploy_settings.brickflow_auto_add_libraries is True:\n_ilog.info(\"Auto adding brickflow libraries...\")\nself.libraries = filter_bf_related_libraries(self.libraries)\n# here libraries should not be null anymore if this branch is invoked\nself.libraries += get_brickflow_libraries()\nif (\nenv_project_name is not None\nand self.name is not None\nand env_project_name != self.name\n):\nraise ValueError(\n\"Project name in config files and entrypoint must be the same\"\n)\nctx.set_current_project(self.name or env_project_name)  # always setup first\n# populate bundle info via env vars\nself.bundle_obj_name = config(\nBrickflowEnvVars.BRICKFLOW_BUNDLE_OBJ_NAME.value,\ndefault=\".brickflow_bundles\",\n)\nself.bundle_base_path = config(\nBrickflowEnvVars.BRICKFLOW_BUNDLE_BASE_PATH.value,\ndefault=\"/Users/${workspace.current_user.userName}\",\n)\nself.git_reference = config(\nBrickflowEnvVars.BRICKFLOW_GIT_REF.value, default=self.get_git_ref()\n)\nif (\nself._mode == Stage.deploy\nand ctx.is_local() is False\nand self.git_reference is None\n):\nraise ValueError(\n\"git_reference must be set when deploying to non-local envs\"\n)\nself.provider = config(\nBrickflowEnvVars.BRICKFLOW_GIT_PROVIDER.value, default=self.provider\n)\nself.git_repo = config(\nBrickflowEnvVars.BRICKFLOW_GIT_REPO.value, default=self.git_repo\n)\nif self.s3_backend is None:\nself.s3_backend = {\n\"bucket\": config(\"BRICKFLOW_S3_BACKEND_BUCKET\", default=None),\n\"key\": config(\"BRICKFLOW_S3_BACKEND_KEY\", default=None),\n\"region\": config(\"BRICKFLOW_S3_BACKEND_REGION\", default=None),\n\"dynamodb_table\": config(\n\"BRICKFLOW_S3_BACKEND_DYNAMODB_TABLE\", default=None\n),\n}\nif all(value is None for value in self.s3_backend.values()):\nself.s3_backend = None\ndeployment_mode = config(\nBrickflowEnvVars.BRICKFLOW_DEPLOYMENT_MODE.value, default=\"cdktf\"\n)\nif deployment_mode == BrickflowDeployMode.CDKTF.value:\nself.codegen_mechanism = HashicorpCDKTFGen\nelif deployment_mode == BrickflowDeployMode.BUNDLE.value:\nself.codegen_mechanism = DatabricksBundleCodegen\nif self.codegen_kwargs is None:\nself.codegen_kwargs = {}\n</code></pre>"},{"location":"api/project/#brickflow.engine.project.Project.get_git_ref","title":"<code>get_git_ref() -&gt; Optional[str]</code>","text":"Source code in <code>brickflow/engine/project.py</code> <pre><code>def get_git_ref(self) -&gt; Optional[str]:\nif self._mode == Stage.deploy:\nif self.git_reference is not None:\nreturn self.git_reference\nelse:\ntry:\nreturn f\"commit/{get_current_commit()}\"\nexcept Exception:\n_ilog.warning(\n\"Unable to get current commit; defaulting to empty string\"\n)\nreturn \"commit/fake-local-stub\" if ctx.is_local() else None\nelse:\nreturn self.git_reference if self.git_reference is not None else \"\"\n</code></pre>"},{"location":"api/secrets/","title":"Secrets","text":""},{"location":"api/secrets/#brickflow_plugins.secrets-attributes","title":"Attributes","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.BRICKFLOW_SECRETS_BACKEND","title":"<code>brickflow_plugins.secrets.BRICKFLOW_SECRETS_BACKEND = 'brickflow_secrets_backend'</code>  <code>module-attribute</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.brickflow_secrets_backend_plugin_impl","title":"<code>brickflow_plugins.secrets.brickflow_secrets_backend_plugin_impl = pluggy.HookimplMarker(BRICKFLOW_SECRETS_BACKEND)</code>  <code>module-attribute</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.brickflow_secrets_plugin_spec","title":"<code>brickflow_plugins.secrets.brickflow_secrets_plugin_spec = pluggy.HookspecMarker(BRICKFLOW_SECRETS_BACKEND)</code>  <code>module-attribute</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets-classes","title":"Classes","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.AbstractSecretsHelper","title":"<code>brickflow_plugins.secrets.AbstractSecretsHelper</code>","text":"<p>             Bases: <code>abc.ABC</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.AbstractSecretsHelper-attributes","title":"Attributes","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.AbstractSecretsHelper.PROTOCOL_STARTS_WITH","title":"<code>PROTOCOL_STARTS_WITH: Optional[Union[str, List[str]]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.AbstractSecretsHelper-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.AbstractSecretsHelper.get_secret_value_from_url","title":"<code>get_secret_value_from_url(url_parsed_result: ParseResult)</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>def get_secret_value_from_url(self, url_parsed_result: ParseResult):\nallowed_protocols = (\n[self.PROTOCOL_STARTS_WITH]\nif isinstance(self.PROTOCOL_STARTS_WITH, str)\nelse self.PROTOCOL_STARTS_WITH\n)\nif self.PROTOCOL_STARTS_WITH is not None and not any(\n[\nurl_parsed_result.scheme.lower().startswith(protocol)\nfor protocol in allowed_protocols\n]\n):\nreturn None\nreturn self._get_secret_value_from_url(url_parsed_result)\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.B64SecretsHelper","title":"<code>brickflow_plugins.secrets.B64SecretsHelper</code>","text":"<p>             Bases: <code>AbstractSecretsHelper</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.B64SecretsHelper-attributes","title":"Attributes","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.B64SecretsHelper.PROTOCOL_STARTS_WITH","title":"<code>PROTOCOL_STARTS_WITH = ['base64', 'b64']</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.Base64BrickflowSecretPluginImpl","title":"<code>brickflow_plugins.secrets.Base64BrickflowSecretPluginImpl</code>","text":"<p>             Bases: <code>BrickflowSecretPluginSpec</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.Base64BrickflowSecretPluginImpl-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.Base64BrickflowSecretPluginImpl.get_secret_value","title":"<code>get_secret_value(url_parsed_result: ParseResult) -&gt; Optional['str']</code>  <code>staticmethod</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>@staticmethod\n@brickflow_secrets_backend_plugin_impl\ndef get_secret_value(url_parsed_result: ParseResult) -&gt; Optional[\"str\"]:\nreturn B64SecretsHelper().get_secret_value_from_url(url_parsed_result)\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretPluginSpec","title":"<code>brickflow_plugins.secrets.BrickflowSecretPluginSpec</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretPluginSpec-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretPluginSpec.get_secret_value","title":"<code>get_secret_value(url_parsed_result: ParseResult) -&gt; Optional['str']</code>  <code>staticmethod</code>","text":"<p>Custom execute method that is able to be plugged in.</p> Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>@staticmethod\n@brickflow_secrets_plugin_spec(firstresult=True)\ndef get_secret_value(url_parsed_result: ParseResult) -&gt; Optional[\"str\"]:\n\"\"\"Custom execute method that is able to be plugged in.\"\"\"\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretsBackend","title":"<code>brickflow_plugins.secrets.BrickflowSecretsBackend</code>","text":"<p>             Bases: <code>BaseSecretsBackend</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretsBackend-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretsBackend.get_conn_value","title":"<code>get_conn_value(conn_id: str) -&gt; str | None</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>def get_conn_value(self, conn_id: str) -&gt; str | None:\nparsed_url = urlparse(conn_id)\nreturn get_brickflow_tasks_hook().get_secret_value(url_parsed_result=parsed_url)\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretsBackend.set_backend_env","title":"<code>set_backend_env()</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>def set_backend_env(self):\nfor k, v in self._get_secrets_backend_env().items():\nos.environ[k] = v\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.BrickflowSecretsBackend.unset_backend_env","title":"<code>unset_backend_env()</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>def unset_backend_env(self):\nfor k in self._get_secrets_backend_env().keys():\nos.environ.pop(k, None)\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusBrickflowSecretPluginImpl","title":"<code>brickflow_plugins.secrets.CerberusBrickflowSecretPluginImpl</code>","text":"<p>             Bases: <code>BrickflowSecretPluginSpec</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusBrickflowSecretPluginImpl-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusBrickflowSecretPluginImpl.get_secret_value","title":"<code>get_secret_value(url_parsed_result: ParseResult) -&gt; Optional['str']</code>  <code>staticmethod</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>@staticmethod\n@brickflow_secrets_backend_plugin_impl\ndef get_secret_value(url_parsed_result: ParseResult) -&gt; Optional[\"str\"]:\nreturn CerberusSecretsHelper().get_secret_value_from_url(url_parsed_result)\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusSecretsHelper","title":"<code>brickflow_plugins.secrets.CerberusSecretsHelper</code>","text":"<p>             Bases: <code>AbstractSecretsHelper</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusSecretsHelper-attributes","title":"Attributes","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusSecretsHelper.PROTOCOL_STARTS_WITH","title":"<code>PROTOCOL_STARTS_WITH = 'cerberus'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusSecretsHelper-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.CerberusSecretsHelper.parse_path_and_key","title":"<code>parse_path_and_key(path: Optional[str]) -&gt; Optional[Tuple[str, str]]</code>  <code>staticmethod</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>@staticmethod\ndef parse_path_and_key(path: Optional[str]) -&gt; Optional[Tuple[str, str]]:\nif path is not None:\n_cleaned_path = path.lstrip(\"/\").rstrip(\"/\")\nreturn \"/\".join(_cleaned_path.split(\"/\")[:-1]), _cleaned_path.split(\"/\")[-1]\nreturn None\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets.DatabricksSecretsBrickflowSecretPluginImpl","title":"<code>brickflow_plugins.secrets.DatabricksSecretsBrickflowSecretPluginImpl</code>","text":"<p>             Bases: <code>BrickflowSecretPluginSpec</code></p>"},{"location":"api/secrets/#brickflow_plugins.secrets.DatabricksSecretsBrickflowSecretPluginImpl-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.DatabricksSecretsBrickflowSecretPluginImpl.get_secret_value","title":"<code>get_secret_value(url_parsed_result: ParseResult) -&gt; Optional['str']</code>  <code>staticmethod</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>@staticmethod\n@brickflow_secrets_backend_plugin_impl\ndef get_secret_value(url_parsed_result: ParseResult) -&gt; Optional[\"str\"]:\n# not implemented yet\nreturn None\n</code></pre>"},{"location":"api/secrets/#brickflow_plugins.secrets-functions","title":"Functions","text":""},{"location":"api/secrets/#brickflow_plugins.secrets.get_brickflow_tasks_hook","title":"<code>brickflow_plugins.secrets.get_brickflow_tasks_hook() -&gt; BrickflowSecretPluginSpec</code>  <code>cached</code>","text":"Source code in <code>brickflow_plugins/secrets/__init__.py</code> <pre><code>@functools.lru_cache\ndef get_brickflow_tasks_hook() -&gt; BrickflowSecretPluginSpec:\npm = pluggy.PluginManager(BRICKFLOW_SECRETS_BACKEND)\npm.add_hookspecs(BrickflowSecretPluginSpec)\npm.load_setuptools_entrypoints(BRICKFLOW_SECRETS_BACKEND)\npm.register(CerberusBrickflowSecretPluginImpl())\npm.register(Base64BrickflowSecretPluginImpl())\nfor name, plugin_instance in pm.list_name_plugin():\nlog.info(\n\"Loaded plugin with name: %s and class: %s\",\nname,\nplugin_instance.__class__.__name__,\n)\nreturn pm.hook\n</code></pre>"},{"location":"api/task/","title":"Task","text":""},{"location":"api/task/#brickflow.engine.task-classes","title":"Classes","text":""},{"location":"api/task/#brickflow.engine.task.Task","title":"<code>brickflow.engine.task.Task</code>  <code>dataclass</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.Task.brickflow_default_params","title":"<code>brickflow_default_params: Dict[str, str]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.builtin_notebook_params","title":"<code>builtin_notebook_params: Dict[str, str]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.cluster","title":"<code>cluster: Cluster</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.custom_execute_callback","title":"<code>custom_execute_callback: Optional[Callable] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.custom_task_parameters","title":"<code>custom_task_parameters: Dict[str, str]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.databricks_task_type_str","title":"<code>databricks_task_type_str: str</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.depends_on","title":"<code>depends_on: List[Union[Callable, str]] = field(default_factory=lambda : [])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.depends_on_names","title":"<code>depends_on_names: Iterator[str]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.description","title":"<code>description: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.ensure_brickflow_plugins","title":"<code>ensure_brickflow_plugins: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.libraries","title":"<code>libraries: List[TaskLibrary] = field(default_factory=lambda : [])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.name","title":"<code>name: str</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.parents","title":"<code>parents: List[str]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.task_func","title":"<code>task_func: Callable</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.task_func_name","title":"<code>task_func_name: str</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.task_id","title":"<code>task_id: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.task_settings","title":"<code>task_settings: Optional[TaskSettings] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.task_type","title":"<code>task_type: TaskType = TaskType.BRICKFLOW_TASK</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.trigger_rule","title":"<code>trigger_rule: BrickflowTriggerRule = BrickflowTriggerRule.ALL_SUCCESS</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.Task-functions","title":"Functions","text":""},{"location":"api/task/#brickflow.engine.task.Task.execute","title":"<code>execute(ignore_all_deps: bool = False) -&gt; Any</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>@with_brickflow_logger\ndef execute(self, ignore_all_deps: bool = False) -&gt; Any:\n# Workflow is:\n#   1. Check to see if there selected tasks and if there are is this task in the list\n#   2. Check to see if the previous task is skipped and trigger rule.\n#   3. Check to see if this a custom python task and execute it\n#   4. Execute the task function\n_ilog.setLevel(logging.INFO)  # enable logging for task execution\nctx._set_current_task(self.name)\nself._ensure_brickflow_plugins()  # if you are expecting brickflow plugins to be installed\nif ignore_all_deps is True:\n_ilog.info(\n\"Ignoring all dependencies for task: %s due to debugging\", self.name\n)\n_select_task_skip, _select_task_skip_reason = self._skip_because_not_selected()\nif _select_task_skip is True and ignore_all_deps is False:\n# check if this task is skipped due to task selection\n_ilog.info(\n\"Skipping task... %s for reason: %s\",\nself.name,\n_select_task_skip_reason,\n)\nctx._reset_current_task()\nreturn\n_skip, reason = self.should_skip()\nif _skip is True and ignore_all_deps is False:\n_ilog.info(\"Skipping task... %s for reason: %s\", self.name, reason)\nctx.task_coms.put(self.name, BRANCH_SKIP_EXCEPT, SKIP_EXCEPT_HACK)\nctx._reset_current_task()\nreturn\n_ilog.info(\"Executing task... %s\", self.name)\n_ilog.info(\"%s\", pretty_print_function_source(self.name, self.task_func))\ninitial_resp: TaskResponse = get_brickflow_tasks_hook().task_execute(\ntask=self, workflow=self.workflow\n)\nresp: TaskResponse = get_brickflow_tasks_hook().handle_results(\nresp=initial_resp, task=self, workflow=self.workflow\n)\nif resp.push_return_value is True:\nctx.task_coms.put(self.name, RETURN_VALUE_KEY, resp.response)\nctx._reset_current_task()\nreturn resp.response\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.Task.get_obj_dict","title":"<code>get_obj_dict(entrypoint: str) -&gt; Dict[str, Any]</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def get_obj_dict(self, entrypoint: str) -&gt; Dict[str, Any]:\nreturn {\n\"notebook_path\": self.handle_notebook_path(entrypoint),\n\"base_parameters\": {\n**self.builtin_notebook_params,\n**self.brickflow_default_params,\n**self.custom_task_parameters,  # type: ignore\n# **(self.custom_unique_task_parameters or {}),\n# TODO: implement only after validating limit on parameters\n},\n}\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.Task.get_runtime_parameter_values","title":"<code>get_runtime_parameter_values() -&gt; Dict[str, Any]</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def get_runtime_parameter_values(self) -&gt; Dict[str, Any]:\n# if dbutils returns None then return v instead\nreturn {\nk: (ctx.get_parameter(k, str(v)) or v)\nfor k, v in (\ninspect.getfullargspec(self.task_func).kwonlydefaults or {}\n).items()\n}\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.Task.handle_notebook_path","title":"<code>handle_notebook_path(entrypoint: str) -&gt; str</code>  <code>staticmethod</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>@staticmethod\ndef handle_notebook_path(entrypoint: str) -&gt; str:\n# local will get created as workspace notebook job and not a git source job\nif ctx.env == BrickflowDefaultEnvs.LOCAL.value:\n# check and ensure suffix has .py extension\nreturn entrypoint if entrypoint.endswith(\".py\") else f\"{entrypoint}.py\"\nreturn entrypoint\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.Task.is_valid_task_signature","title":"<code>is_valid_task_signature() -&gt; None</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def is_valid_task_signature(self) -&gt; None:\n# only supports kwonlyargs with defaults\nspec: inspect.FullArgSpec = inspect.getfullargspec(self.task_func)\nsig: inspect.Signature = inspect.signature(self.task_func)\nsignature_error_msg = (\n\"Task signatures only supports kwargs with defaults. or catch all varkw **kwargs\"\n\"For example def execute(*, variable_a=None, variable_b=None, **kwargs). \"\nf\"Please fix function def {self.task_func_name}{sig}: ...\"\n)\nkwargs_default_error_msg = (\nf\"Keyword arguments must be Strings. \"\nf\"Please handle booleans and numbers via strings. \"\nf\"Please fix function def {self.task_func_name}{sig}: ...\"\n)\nvalid_case = spec.args == [] and spec.varargs is None and spec.defaults is None\nfor _, v in (spec.kwonlydefaults or {}).items():\n# in python boolean is a type of int must be captured here via short circuit\nif not (isinstance(v, str) or v is None):\nraise InvalidTaskSignatureDefinition(kwargs_default_error_msg)\nif valid_case:\nreturn\nraise InvalidTaskSignatureDefinition(signature_error_msg)\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.Task.should_skip","title":"<code>should_skip() -&gt; Tuple[bool, Optional[str]]</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def should_skip(self) -&gt; Tuple[bool, Optional[str]]:\n# return true or false and reason\nnode_skip_checks = []\nfor parent in self.parents:\nif parent != ROOT_NODE:\ntry:\ntask_to_not_skip = ctx.task_coms.get(parent, BRANCH_SKIP_EXCEPT)\nif self.name != task_to_not_skip:\n# set this task to skip hack to keep to empty to trigger failure\n# key look up will fail\nnode_skip_checks.append(True)\nelse:\nnode_skip_checks.append(False)\nexcept Exception:\n# ignore errors as it probably doesnt exist\n# TODO: log errors\nnode_skip_checks.append(False)\nif not node_skip_checks:\nreturn False, None\nif self.trigger_rule == BrickflowTriggerRule.NONE_FAILED:\n# by default a task failure automatically skips\nreturn self._get_skip_with_reason(\nall(node_skip_checks),\n\"At least one task before this were not successful\",\n)\n# default is BrickflowTriggerRule.ALL_SUCCESS\nreturn self._get_skip_with_reason(\nany(node_skip_checks), \"All tasks before this were not successful\"\n)\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.EmailNotifications","title":"<code>brickflow.engine.task.EmailNotifications</code>  <code>dataclass</code>","text":""},{"location":"api/task/#brickflow.engine.task.EmailNotifications-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.EmailNotifications.on_failure","title":"<code>on_failure: Optional[List[str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.EmailNotifications.on_start","title":"<code>on_start: Optional[List[str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.EmailNotifications.on_success","title":"<code>on_success: Optional[List[str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.EmailNotifications-functions","title":"Functions","text":""},{"location":"api/task/#brickflow.engine.task.EmailNotifications.to_tf_dict","title":"<code>to_tf_dict() -&gt; Dict[str, Optional[List[str]]]</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def to_tf_dict(self) -&gt; Dict[str, Optional[List[str]]]:\nreturn {\n\"on_start\": self.on_start,\n\"on_failure\": self.on_failure,\n\"on_success\": self.on_success,\n}\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.JarTaskLibrary","title":"<code>brickflow.engine.task.JarTaskLibrary</code>  <code>dataclass</code>","text":"<p>             Bases: <code>StorageBasedTaskLibrary</code></p> <p>Parameters:</p> Name Type Description Default <code>jar</code> <code>str</code> <p>String to s3/dbfs path for jar</p> required"},{"location":"api/task/#brickflow.engine.task.JarTaskLibrary-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.JarTaskLibrary.jar","title":"<code>jar: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.EggTaskLibrary","title":"<code>brickflow.engine.task.EggTaskLibrary</code>  <code>dataclass</code>","text":"<p>             Bases: <code>StorageBasedTaskLibrary</code></p> <p>Parameters:</p> Name Type Description Default <code>egg</code> <code>str</code> <p>String to s3/dbfs path for egg</p> required"},{"location":"api/task/#brickflow.engine.task.EggTaskLibrary-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.EggTaskLibrary.egg","title":"<code>egg: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.WheelTaskLibrary","title":"<code>brickflow.engine.task.WheelTaskLibrary</code>  <code>dataclass</code>","text":"<p>             Bases: <code>StorageBasedTaskLibrary</code></p> <p>Parameters:</p> Name Type Description Default <code>whl</code> <code>str</code> <p>String to s3/dbfs path for whl</p> required"},{"location":"api/task/#brickflow.engine.task.WheelTaskLibrary-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.WheelTaskLibrary.whl","title":"<code>whl: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.PypiTaskLibrary","title":"<code>brickflow.engine.task.PypiTaskLibrary</code>  <code>dataclass</code>","text":"<p>             Bases: <code>TaskLibrary</code></p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package in pypi i.e. requests, requests==x.y.z, git+https://github.com/Nike-Inc/brickflow.git</p> required <code>repo</code> <code>Optional[str]</code> <p>The repository where the package can be found. By default pypi is used</p> <code>None</code>"},{"location":"api/task/#brickflow.engine.task.PypiTaskLibrary-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.PypiTaskLibrary.dict","title":"<code>dict: Dict[str, Union[str, Dict[str, str]]]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.PypiTaskLibrary.package","title":"<code>package: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.PypiTaskLibrary.repo","title":"<code>repo: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.MavenTaskLibrary","title":"<code>brickflow.engine.task.MavenTaskLibrary</code>  <code>dataclass</code>","text":"<p>             Bases: <code>TaskLibrary</code></p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>str</code> <p>Gradle-style Maven coordinates. For example: org.jsoup:jsoup:1.7.2.</p> required <code>repo</code> <code>Optional[str]</code> <p>Maven repo to install the Maven package from. If omitted, both Maven Central Repository and Spark Packages are searched.</p> <code>None</code> <code>exclusions</code> <code>Optional[List[str]]</code> <p>List of dependences to exclude. For example: [\"slf4j:slf4j\", \"*:hadoop-client\"]. Maven dependency exclusions: https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html.</p> <code>None</code>"},{"location":"api/task/#brickflow.engine.task.MavenTaskLibrary-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.MavenTaskLibrary.coordinates","title":"<code>coordinates: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.MavenTaskLibrary.dict","title":"<code>dict: Dict[str, Union[str, Dict[str, str]]]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.MavenTaskLibrary.exclusions","title":"<code>exclusions: Optional[List[str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.MavenTaskLibrary.repo","title":"<code>repo: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.CranTaskLibrary","title":"<code>brickflow.engine.task.CranTaskLibrary</code>  <code>dataclass</code>","text":"<p>             Bases: <code>TaskLibrary</code></p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The name of the CRAN package to install.</p> required <code>repo</code> <code>Optional[str]</code> <p>The repository where the package can be found. If not specified, the default CRAN repo is used.</p> <code>None</code>"},{"location":"api/task/#brickflow.engine.task.CranTaskLibrary-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.CranTaskLibrary.dict","title":"<code>dict: Dict[str, Union[str, Dict[str, str]]]</code>  <code>property</code>","text":""},{"location":"api/task/#brickflow.engine.task.CranTaskLibrary.package","title":"<code>package: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.CranTaskLibrary.repo","title":"<code>repo: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.BrickflowTriggerRule","title":"<code>brickflow.engine.task.BrickflowTriggerRule</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/task/#brickflow.engine.task.BrickflowTriggerRule-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.BrickflowTriggerRule.ALL_SUCCESS","title":"<code>ALL_SUCCESS = 'all_success'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.BrickflowTriggerRule.NONE_FAILED","title":"<code>NONE_FAILED = 'none_failed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.BrickflowTaskEnvVars","title":"<code>brickflow.engine.task.BrickflowTaskEnvVars</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/task/#brickflow.engine.task.BrickflowTaskEnvVars-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.BrickflowTaskEnvVars.BRICKFLOW_SELECT_TASKS","title":"<code>BRICKFLOW_SELECT_TASKS = 'BRICKFLOW_SELECT_TASKS'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings","title":"<code>brickflow.engine.task.TaskSettings</code>  <code>dataclass</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.email_notifications","title":"<code>email_notifications: Optional[EmailNotifications] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.max_retries","title":"<code>max_retries: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.min_retry_interval_millis","title":"<code>min_retry_interval_millis: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.notification_settings","title":"<code>notification_settings: Optional[TaskNotificationSettings] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.retry_on_timeout","title":"<code>retry_on_timeout: Optional[bool] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.timeout_seconds","title":"<code>timeout_seconds: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings-functions","title":"Functions","text":""},{"location":"api/task/#brickflow.engine.task.TaskSettings.merge","title":"<code>merge(other: Optional['TaskSettings']) -&gt; 'TaskSettings'</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def merge(self, other: Optional[\"TaskSettings\"]) -&gt; \"TaskSettings\":\n# overrides top level values\nif other is None:\nreturn self\nreturn TaskSettings(\nother.email_notifications or self.email_notifications,\nother.notification_settings or self.notification_settings,\nother.timeout_seconds or self.timeout_seconds or 0,\nother.max_retries or self.max_retries,\nother.min_retry_interval_millis or self.min_retry_interval_millis,\nother.retry_on_timeout or self.retry_on_timeout,\n)\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.TaskSettings.to_tf_dict","title":"<code>to_tf_dict() -&gt; Dict[str, Optional[str] | Optional[int] | Optional[bool] | Optional[Dict[str, Optional[List[str]]]]]</code>","text":"Source code in <code>brickflow/engine/task.py</code> <pre><code>def to_tf_dict(\nself,\n) -&gt; Dict[\nstr,\nOptional[str]\n| Optional[int]\n| Optional[bool]\n| Optional[Dict[str, Optional[List[str]]]],\n]:\nemail_not = (\nself.email_notifications.to_tf_dict()\nif self.email_notifications is not None\nelse {}\n)\nnotification_settings = (\n{}\nif self.notification_settings is None\nelse {\"notification_settings\": self.notification_settings.dict()}\n)\nreturn {\n**notification_settings,\n\"email_notifications\": email_not,\n\"timeout_seconds\": self.timeout_seconds,\n\"max_retries\": self.max_retries,\n\"min_retry_interval_millis\": self.min_retry_interval_millis,\n\"retry_on_timeout\": self.retry_on_timeout,\n}\n</code></pre>"},{"location":"api/task/#brickflow.engine.task.TaskType","title":"<code>brickflow.engine.task.TaskType</code>","text":"<p>             Bases: <code>Enum</code></p>"},{"location":"api/task/#brickflow.engine.task.TaskType-attributes","title":"Attributes","text":""},{"location":"api/task/#brickflow.engine.task.TaskType.BRICKFLOW_TASK","title":"<code>BRICKFLOW_TASK = 'brickflow_task'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskType.CUSTOM_PYTHON_TASK","title":"<code>CUSTOM_PYTHON_TASK = 'custom_python_task'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskType.DLT","title":"<code>DLT = 'pipeline_task'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskType.NOTEBOOK_TASK","title":"<code>NOTEBOOK_TASK = 'notebook_task'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/task/#brickflow.engine.task.TaskType.SQL","title":"<code>SQL = 'sql_task'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/","title":"Workflow","text":""},{"location":"api/workflow/#brickflow.engine.workflow-classes","title":"Classes","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow","title":"<code>brickflow.engine.workflow.Workflow</code>  <code>dataclass</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow-attributes","title":"Attributes","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.active_task","title":"<code>active_task: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.bfs_layers","title":"<code>bfs_layers: List[str]</code>  <code>property</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.clusters","title":"<code>clusters: List[Cluster] = field(default_factory=lambda : [])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.common_task_parameters","title":"<code>common_task_parameters: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.default_cluster","title":"<code>default_cluster: Optional[Cluster] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.default_task_settings","title":"<code>default_task_settings: TaskSettings = TaskSettings()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.email_notifications","title":"<code>email_notifications: Optional[WorkflowEmailNotifications] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.enable_plugins","title":"<code>enable_plugins: Optional[bool] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.graph","title":"<code>graph: nx.DiGraph = field(default_factory=nx.DiGraph)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.libraries","title":"<code>libraries: List[TaskLibrary] = field(default_factory=lambda : [])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.max_concurrent_runs","title":"<code>max_concurrent_runs: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.max_tasks_in_workflow","title":"<code>max_tasks_in_workflow: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.name","title":"<code>name: str</code>  <code>property</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.notification_settings","title":"<code>notification_settings: Optional[WorkflowNotificationSettings] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.permissions","title":"<code>permissions: WorkflowPermissions = WorkflowPermissions()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.prefix","title":"<code>prefix: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.run_as_service_principal","title":"<code>run_as_service_principal: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.run_as_user","title":"<code>run_as_user: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.schedule_quartz_expression","title":"<code>schedule_quartz_expression: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.suffix","title":"<code>suffix: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.tags","title":"<code>tags: Optional[Dict[str, str]] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.tasks","title":"<code>tasks: Dict[str, Task] = field(default_factory=lambda : {})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.timezone","title":"<code>timezone: str = 'UTC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.trigger","title":"<code>trigger: Optional[Trigger] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.unique_new_clusters","title":"<code>unique_new_clusters: List[Cluster]</code>  <code>property</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.webhook_notifications","title":"<code>webhook_notifications: Optional[WorkflowWebhookNotifications] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow-functions","title":"Functions","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.bfs_task_iter","title":"<code>bfs_task_iter() -&gt; Iterator[Task]</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def bfs_task_iter(self) -&gt; Iterator[Task]:\nfor layer in self.bfs_layers:\nfor task_key in layer:\nyield self.get_task(task_key)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.check_no_active_task","title":"<code>check_no_active_task() -&gt; None</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def check_no_active_task(self) -&gt; None:\nif self.active_task is not None:\nraise AnotherActiveTaskError(\n\"You are calling another active task in another task. \"\n\"Please abstract the code more.\"\n)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.dlt_task","title":"<code>dlt_task(task_func: Optional[Callable] = None, name: Optional[str] = None, depends_on: Optional[Union[Callable, str, List[Union[Callable, str]]]] = None) -&gt; Callable</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def dlt_task(\nself,\ntask_func: Optional[Callable] = None,\nname: Optional[str] = None,\ndepends_on: Optional[Union[Callable, str, List[Union[Callable, str]]]] = None,\n) -&gt; Callable:\nreturn self.task(task_func, name, task_type=TaskType.DLT, depends_on=depends_on)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.get_task","title":"<code>get_task(task_id: str) -&gt; Task</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>@wraps_keyerror(TaskNotFoundError, \"Unable to find task: \")\ndef get_task(self, task_id: str) -&gt; Task:\nreturn self.tasks[task_id]\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.notebook_task","title":"<code>notebook_task(task_func: Optional[Callable] = None, name: Optional[str] = None, cluster: Optional[Cluster] = None, libraries: Optional[List[TaskLibrary]] = None, task_settings: Optional[TaskSettings] = None, depends_on: Optional[Union[Callable, str, List[Union[Callable, str]]]] = None) -&gt; Callable</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def notebook_task(\nself,\ntask_func: Optional[Callable] = None,\nname: Optional[str] = None,\ncluster: Optional[Cluster] = None,\nlibraries: Optional[List[TaskLibrary]] = None,\ntask_settings: Optional[TaskSettings] = None,\ndepends_on: Optional[Union[Callable, str, List[Union[Callable, str]]]] = None,\n) -&gt; Callable:\nreturn self.task(\ntask_func,\nname,\ncluster=cluster,\nlibraries=libraries,\ntask_type=TaskType.NOTEBOOK_TASK,\ntask_settings=task_settings,\ndepends_on=depends_on,\n)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.parents","title":"<code>parents(node: str) -&gt; Iterator</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def parents(self, node: str) -&gt; Iterator:\nreturn self.graph.predecessors(node)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.pop_task","title":"<code>pop_task(task_id: str) -&gt; None</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>@wraps_keyerror(TaskNotFoundError, \"Unable to find task: \")\ndef pop_task(self, task_id: str) -&gt; None:\n# Pop from dict and graph\nself.tasks.pop(task_id)\nself.graph.remove_node(task_id)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.task","title":"<code>task(task_func: Optional[Callable] = None, name: Optional[str] = None, cluster: Optional[Cluster] = None, libraries: Optional[List[TaskLibrary]] = None, task_type: TaskType = TaskType.BRICKFLOW_TASK, depends_on: Optional[Union[Callable, str, List[Union[Callable, str]]]] = None, trigger_rule: BrickflowTriggerRule = BrickflowTriggerRule.ALL_SUCCESS, custom_execute_callback: Optional[Callable] = None, task_settings: Optional[TaskSettings] = None, ensure_brickflow_plugins: bool = False) -&gt; Callable</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def task(\nself,\ntask_func: Optional[Callable] = None,\nname: Optional[str] = None,\ncluster: Optional[Cluster] = None,\nlibraries: Optional[List[TaskLibrary]] = None,\ntask_type: TaskType = TaskType.BRICKFLOW_TASK,\ndepends_on: Optional[Union[Callable, str, List[Union[Callable, str]]]] = None,\ntrigger_rule: BrickflowTriggerRule = BrickflowTriggerRule.ALL_SUCCESS,\ncustom_execute_callback: Optional[Callable] = None,\ntask_settings: Optional[TaskSettings] = None,\nensure_brickflow_plugins: bool = False,\n) -&gt; Callable:\nif len(self.tasks) &gt;= self.max_tasks_in_workflow:\nraise ValueError(\n\"You have reached the maximum number of tasks allowed in a databricks workflow. \"\n\"Please split your workflow into multiple workflows or raise a feature request \"\n\"with your Databricks team.\"\n)\ndef task_wrapper(f: Callable) -&gt; Callable:\ntask_id = name or f.__name__\nself._add_task(\nf,\ntask_id,\ncluster=cluster,\ntask_type=task_type,\nlibraries=libraries,\ndepends_on=depends_on,\ntrigger_rule=trigger_rule,\ncustom_execute_callback=custom_execute_callback,\ntask_settings=task_settings,\nensure_brickflow_plugins=ensure_brickflow_plugins,\n)\n@functools.wraps(f)\ndef func(*args, **kwargs):  # type: ignore\ntry:\nself.check_no_active_task()\nself._set_active_task(task_id)\nresp = f(*args, **kwargs)\nreturn resp\nexcept Exception as e:\nself._reset_active_task()\nraise e\nfinally:\nself._reset_active_task()\nreturn func\nif task_func is not None:\nif callable(task_func):\nreturn task_wrapper(task_func)\nelse:\nraise NoCallableTaskError(\n\"Please use task decorator against a callable function.\"\n)\nreturn task_wrapper\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.task_exists","title":"<code>task_exists(task_id: str) -&gt; bool</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def task_exists(self, task_id: str) -&gt; bool:\nreturn task_id in self.tasks\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.task_iter","title":"<code>task_iter() -&gt; Iterator[Task]</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def task_iter(self) -&gt; Iterator[Task]:\nfor task in self.bfs_task_iter():\nyield task\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.unique_new_clusters_dict","title":"<code>unique_new_clusters_dict() -&gt; List[Dict[str, Any]]</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def unique_new_clusters_dict(self) -&gt; List[Dict[str, Any]]:\nself.validate_new_clusters_with_unique_names()\nall_unique_clusters = self.unique_new_clusters\nreturn [\n# job clusters do not need names\n{\n\"job_cluster_key\": c.name,\n\"new_cluster\": c.as_dict(remove_fields=[\"name\"]),\n}\nfor c in all_unique_clusters\n]\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Workflow.validate_new_clusters_with_unique_names","title":"<code>validate_new_clusters_with_unique_names() -&gt; None</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def validate_new_clusters_with_unique_names(self) -&gt; None:\nall_unique_clusters = self.unique_new_clusters\nunique_name_list: Dict[str, Optional[str]] = {}\nduplicates = []\nfor cluster in all_unique_clusters:\nif cluster.name not in unique_name_list:\nunique_name_list[cluster.name] = None\nelse:\nduplicates.append(cluster.name)\nduplicate_list = list(set(duplicates))\nif len(duplicate_list) &gt; 0:\nraise DuplicateClustersDefinitionError(\nf\"Found duplicate cluster definitions in your workflow: {self.name}, \"\nf\"with names: {duplicate_list}\"\n)\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.User","title":"<code>brickflow.engine.workflow.User</code>","text":"<p>             Bases: <code>ScimEntity</code></p>"},{"location":"api/workflow/#brickflow.engine.workflow.User-functions","title":"Functions","text":""},{"location":"api/workflow/#brickflow.engine.workflow.User.to_access_control","title":"<code>to_access_control() -&gt; Dict[str, str]</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def to_access_control(self) -&gt; Dict[str, str]:\nreturn {\"user_name\": self.name}\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.Group","title":"<code>brickflow.engine.workflow.Group</code>","text":"<p>             Bases: <code>ScimEntity</code></p>"},{"location":"api/workflow/#brickflow.engine.workflow.Group-functions","title":"Functions","text":""},{"location":"api/workflow/#brickflow.engine.workflow.Group.to_access_control","title":"<code>to_access_control() -&gt; Dict[str, str]</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def to_access_control(self) -&gt; Dict[str, str]:\nreturn {\"group_name\": self.name}\n</code></pre>"},{"location":"api/workflow/#brickflow.engine.workflow.ServicePrincipal","title":"<code>brickflow.engine.workflow.ServicePrincipal</code>","text":"<p>             Bases: <code>ScimEntity</code></p>"},{"location":"api/workflow/#brickflow.engine.workflow.ServicePrincipal-functions","title":"Functions","text":""},{"location":"api/workflow/#brickflow.engine.workflow.ServicePrincipal.to_access_control","title":"<code>to_access_control() -&gt; Dict[str, str]</code>","text":"Source code in <code>brickflow/engine/workflow.py</code> <pre><code>def to_access_control(self) -&gt; Dict[str, str]:\nreturn {\"service_principal_name\": self.name}\n</code></pre>"},{"location":"api/workflow_dependency_sensor/","title":"WorkflowDependencySensor","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor-attributes","title":"Attributes","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor-classes","title":"Classes","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor","title":"<code>brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor(databricks_host: str, databricks_secrets_scope: str, databricks_secrets_key: str, dependency_job_id: int, delta: timedelta, timeout_seconds: int, poke_interval_seconds: int = 60)</code>","text":"<p>This is used to have dependencies on the databricks workflow</p> Example Usage in your brickflow task <p>WorkflowDependencySensor(     databricks_host=https://your_workspace_url.cloud.databricks.com,     databricks_secrets_scope=\"brickflow-demo-tobedeleted\",     databricks_secrets_key=\"service_principle_id\"     dependency_job_id=job_id,     poke_interval=20,     timeout=60,     delta=timedelta(days=1) )</p> Source code in <code>brickflow_plugins/databricks/workflow_dependency_sensor.py</code> <pre><code>def __init__(\nself,\ndatabricks_host: str,\ndatabricks_secrets_scope: str,\ndatabricks_secrets_key: str,\ndependency_job_id: int,\ndelta: timedelta,\ntimeout_seconds: int,\npoke_interval_seconds: int = 60,\n):\nself.databricks_host = databricks_host\nself.dependency_job_id = dependency_job_id\nself.databricks_secrets_scope = databricks_secrets_scope\nself.databricks_secrets_key = databricks_secrets_key\nself.poke_interval = poke_interval_seconds\nself.timeout = timeout_seconds\nself.delta = delta\nself.log = logging\nself.start_time = time.time()\n</code></pre>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor-attributes","title":"Attributes","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.databricks_host","title":"<code>databricks_host = databricks_host</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.databricks_secrets_key","title":"<code>databricks_secrets_key = databricks_secrets_key</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.databricks_secrets_scope","title":"<code>databricks_secrets_scope = databricks_secrets_scope</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.delta","title":"<code>delta = delta</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.dependency_job_id","title":"<code>dependency_job_id = dependency_job_id</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.log","title":"<code>log = logging</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.poke_interval","title":"<code>poke_interval = poke_interval_seconds</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.start_time","title":"<code>start_time = time.time()</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.timeout","title":"<code>timeout = timeout_seconds</code>  <code>instance-attribute</code>","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor-functions","title":"Functions","text":""},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.execute","title":"<code>execute()</code>","text":"Source code in <code>brickflow_plugins/databricks/workflow_dependency_sensor.py</code> <pre><code>def execute(self):\nsession = self.get_http_session()\nurl = f\"{self.databricks_host.rstrip('/')}/api/2.0/jobs/runs/list\"\nheaders = {\n\"Authorization\": f\"Bearer {self.get_token()}\",\n\"Content-Type\": \"application/json\",\n}\n# http://www.unixtimestampconverter.com/\nparams = {\n\"limit\": 25,\n\"job_id\": self.dependency_job_id,\n\"expand_tasks\": \"true\",\n\"start_time_from\": self.get_the_execution_date(),\n}\nwhile True:\noffset = 0\nhas_more = True\nwhile has_more is True:\nparams[\"offset\"] = offset\nresp = session.get(url, params=params, headers=headers).json()\nfor run in resp.get(\"runs\", []):\nself.log.info(\nf\"Found the run_id: {run['run_id']}, and it's result_state is: {run.get('state', {}).get('result_state', None)}\"\n)\nif run.get(\"state\", {}).get(\"result_state\", None) == \"SUCCESS\":\nself.log.info(f\"Found a successful run: {run['run_id']}\")\nreturn\noffset += params[\"limit\"]\nhas_more = resp.get(\"has_more\", False)\nself.log.info(f\"This is offset: {offset}, this is has_more: {has_more}\")\nself.log.info(\"Didn't find a successful run yet\")\nif (\nself.timeout is not None\nand (time.time() - self.start_time) &gt; self.timeout\n):\nraise WorkflowDependencySensorTimeOutException(f\"The job has timed out\")\nself.log.info(f\"sleeping for: {self.poke_interval}\")\ntime.sleep(self.poke_interval)\n</code></pre>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.get_http_session","title":"<code>get_http_session()</code>  <code>cached</code>","text":"Source code in <code>brickflow_plugins/databricks/workflow_dependency_sensor.py</code> <pre><code>@functools.lru_cache(maxsize=None)\ndef get_http_session(self):\nsession = requests.Session()\nmax_retries = int(os.getenv(\"DATABRICKS_REQUEST_RETRY_COUNT\", 10))\nretries = self.get_retry_class(max_retries)(\ntotal=max_retries,\nbackoff_factor=1,\nstatus_forcelist=[500, 501, 502, 503, 504, 429],\n)\nsession.mount(\"https://\", HTTPAdapter(max_retries=retries))\nsession.mount(\"http://\", HTTPAdapter(max_retries=retries))\nreturn session\n</code></pre>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.get_retry_class","title":"<code>get_retry_class(max_retries)</code>","text":"Source code in <code>brickflow_plugins/databricks/workflow_dependency_sensor.py</code> <pre><code>def get_retry_class(self, max_retries):\nfrom urllib3 import Retry\nlog = self.log\nclass LogRetry(Retry):\n\"\"\"\n        Adding extra logs before making a retry request\n        \"\"\"\ndef __init__(self, *args, **kwargs):\nif (\nkwargs.get(\"total\", None) != max_retries\nand kwargs.get(\"total\", None) &gt; 0\n):\nlog.info(f\"Retrying with kwargs: {kwargs}\")\nsuper().__init__(*args, **kwargs)\nreturn LogRetry\n</code></pre>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.get_the_execution_date","title":"<code>get_the_execution_date() -&gt; str</code>","text":"Source code in <code>brickflow_plugins/databricks/workflow_dependency_sensor.py</code> <pre><code>def get_the_execution_date(self) -&gt; str:\nsession = self.get_http_session()\nurl = f\"{self.databricks_host.rstrip('/')}/api/2.0/jobs/runs/get\"\nheaders = {\n\"Authorization\": f\"Bearer {self.get_token()}\",\n\"Content-Type\": \"application/json\",\n}\nrun_id = ctx.dbutils_widget_get_or_else(\"brickflow_parent_run_id\", None)\nif run_id is None:\nraise WorkflowDependencySensorException(\n\"run_id is empty, brickflow_parent_run_id parameter is not found \"\n\"or no value present\"\n)\nparams = {\"run_id\": run_id}\nresp = session.get(url, params=params, headers=headers).json()\n# Convert Unix timestamp to datetime object\nstart_time = datetime.fromtimestamp(resp[\"start_time\"] / 1000)\nexecution_date = start_time - self.delta\nself.log.info(start_time)\nself.log.info(execution_date)\nself.log.info(execution_date.strftime(\"%s\"))\nreturn execution_date.strftime(\"%s\")\n</code></pre>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensor.get_token","title":"<code>get_token()</code>  <code>cached</code>","text":"Source code in <code>brickflow_plugins/databricks/workflow_dependency_sensor.py</code> <pre><code>@functools.lru_cache\ndef get_token(self):\nreturn ctx.dbutils.secrets.get(\nself.databricks_secrets_scope, self.databricks_secrets_key\n)\n</code></pre>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensorException","title":"<code>brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensorException</code>","text":"<p>             Bases: <code>Exception</code></p>"},{"location":"api/workflow_dependency_sensor/#brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensorTimeOutException","title":"<code>brickflow_plugins.databricks.workflow_dependency_sensor.WorkflowDependencySensorTimeOutException</code>","text":"<p>             Bases: <code>TimeoutError</code></p>"},{"location":"cli/reference/","title":"Commands","text":"<p>This page provides documentation for our command line tools.</p>"},{"location":"cli/reference/#bf","title":"bf","text":"<p>CLI for managing Databricks Workflows</p> <p>Usage:</p> <pre><code>bf [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --version  Show the version and exit.\n  --help     Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#bundle","title":"bundle","text":"<p>CLI for proxying to databricks bundles cli.</p> <p>Usage:</p> <pre><code>bf bundle [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#cdktf","title":"cdktf","text":"<p>CLI for proxying to cdktf cli.</p> <p>Usage:</p> <pre><code>bf cdktf [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#deploy","title":"deploy","text":"<p>CLI for deploying workflow projects.</p> <p>Usage:</p> <pre><code>bf deploy [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --auto-approve                  Auto approve brickflow pipeline without\n                                  being prompted to approve.\n  --deploy-mode [cdktf|bundle]    Which deployment framework to use to deploy.\n                                  [default: cdktf]\n  --force-acquire-lock            Force acquire lock for databricks bundles\n                                  deploy.\n  -p, --profile TEXT              The databricks profile to use for\n                                  authenticating to databricks during\n                                  deployment.\n  --git-provider TEXT             The github provider for brickflow this is\n                                  used for configuring github on DBX jobs.\n  --git-ref TEXT                  The commit/tag/branch to use in github.\n  -r, --repo-url TEXT             The github url in which to run brickflow\n                                  with.\n  -e, --env TEXT                  Set the environment value, certain tags\n                                  [TBD] get added to the workflows based on\n                                  this value.\n  -w, --workflow TEXT             Provide the workflow file names which you\n                                  want to deploy, each file name separated by\n                                  space! Example: bf deploy -p DEFAULT -l -w\n                                  wf1.py -w wf2.py\n  -wd, --workflows-dir DIRECTORY  Provide the workflow directory that has to\n                                  be deployed\n  -l, --local-mode                Set the environment flag to local and other\n                                  components [TBD] are disabled in local mode.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#destroy","title":"destroy","text":"<p>CLI for destroying workflow projects.</p> <p>Usage:</p> <pre><code>bf destroy [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --auto-approve                  Auto approve brickflow pipeline without\n                                  being prompted to approve.\n  --deploy-mode [cdktf|bundle]    Which deployment framework to use to deploy.\n                                  [default: cdktf]\n  --force-acquire-lock            Force acquire lock for databricks bundles\n                                  destroy.\n  -p, --profile TEXT              The databricks profile to use for\n                                  authenticating to databricks during\n                                  deployment.\n  --git-provider TEXT             The github provider for brickflow this is\n                                  used for configuring github on DBX jobs.\n  --git-ref TEXT                  The commit/tag/branch to use in github.\n  -r, --repo-url TEXT             The github url in which to run brickflow\n                                  with.\n  -e, --env TEXT                  Set the environment value, certain tags\n                                  [TBD] get added to the workflows based on\n                                  this value.\n  -w, --workflow TEXT             Provide the workflow file names which you\n                                  want to deploy, each file name separated by\n                                  space! Example: bf deploy -p DEFAULT -l -w\n                                  wf1.py -w wf2.py\n  -wd, --workflows-dir DIRECTORY  Provide the workflow directory that has to\n                                  be deployed\n  -l, --local-mode                Set the environment flag to local and other\n                                  components [TBD] are disabled in local mode.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#diff","title":"diff","text":"<p>CLI for identifying diff in projects (only cdktf supported).</p> <p>Usage:</p> <pre><code>bf diff [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -p, --profile TEXT              The databricks profile to use for\n                                  authenticating to databricks during\n                                  deployment.\n  --git-provider TEXT             The github provider for brickflow this is\n                                  used for configuring github on DBX jobs.\n  --git-ref TEXT                  The commit/tag/branch to use in github.\n  -r, --repo-url TEXT             The github url in which to run brickflow\n                                  with.\n  -e, --env TEXT                  Set the environment value, certain tags\n                                  [TBD] get added to the workflows based on\n                                  this value.\n  -w, --workflow TEXT             Provide the workflow file names which you\n                                  want to deploy, each file name separated by\n                                  space! Example: bf deploy -p DEFAULT -l -w\n                                  wf1.py -w wf2.py\n  -wd, --workflows-dir DIRECTORY  Provide the workflow directory that has to\n                                  be deployed\n  -l, --local-mode                Set the environment flag to local and other\n                                  components [TBD] are disabled in local mode.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#docs","title":"docs","text":"<p>Use to open docs in your browser...</p> <p>Usage:</p> <pre><code>bf docs [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#init","title":"init","text":"<p>Initialize your project with Brickflow...</p> <p>Usage:</p> <pre><code>bf init [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -n, --project-name TEXT\n  -g, --git-https-url TEXT        Provide the github URL for your project,\n                                  example: https://github.com/nike-eda-\n                                  apla/brickflow\n  -wd, --workflows-dir DIRECTORY\n  -bfv, --brickflow-version TEXT\n  -sev, --spark-expectations-version TEXT\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#projects","title":"projects","text":"<p>Manage one to many brickflow projects</p> <p>Usage:</p> <pre><code>bf projects [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#add","title":"add","text":"<p>Adds a project to the brickflow-multi-project.yml file and a entrypoint.py file in workflows dir</p> <p>Usage:</p> <pre><code>bf projects add [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --name TEXT                     Name of the project\n  --path-from-repo-root-to-project-root DIRECTORY\n                                  Path from repo root to project root\n  --path-project-root-to-workflows-dir TEXT\n                                  Path from project root to workflows dir\n  --deployment-mode [bundle]      Deployment mode\n  -g, --git-https-url TEXT        Provide the github URL for your project,\n                                  example: https://github.com/nike-eda-\n                                  apla/brickflow\n  -bfv, --brickflow-version TEXT\n  -sev, --spark-expectations-version TEXT\n  --skip-entrypoint               Skip creating entrypoint.py file\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#deploy_1","title":"deploy","text":"<p>Deploy projects in the brickflow-multi-project.yml file</p> <p>Usage:</p> <pre><code>bf projects deploy [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --force-acquire-lock  Force acquire lock for databricks bundles destroy.\n  --skip-libraries      Skip automatically adding brickflow libraries.\n  --auto-approve        Auto approve brickflow pipeline without being prompted\n                        to approve.\n  -p, --profile TEXT    The databricks profile to use for authenticating to\n                        databricks during deployment.\n  --project []          Select the project of workflows you would like to\n                        deploy.\n  -e, --env TEXT        Set the environment value, certain tags [TBD] get\n                        added to the workflows based on this value.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#destroy_1","title":"destroy","text":"<p>Destroy projects in the brickflow-multi-project.yml file</p> <p>Usage:</p> <pre><code>bf projects destroy [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --force-acquire-lock  Force acquire lock for databricks bundles destroy.\n  --skip-libraries      Skip automatically adding brickflow libraries.\n  --auto-approve        Auto approve brickflow pipeline without being prompted\n                        to approve.\n  -p, --profile TEXT    The databricks profile to use for authenticating to\n                        databricks during deployment.\n  --project []          Select the project of workflows you would like to\n                        deploy.\n  -e, --env TEXT        Set the environment value, certain tags [TBD] get\n                        added to the workflows based on this value.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#list","title":"list","text":"<p>Lists all projects in the brickflow-multi-project.yml file</p> <p>Usage:</p> <pre><code>bf projects list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#remove","title":"remove","text":"<p>Removes a project from the brickflow-multi-project.yml file</p> <p>Usage:</p> <pre><code>bf projects remove [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --name []  Name of the project\n  --help     Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#synth","title":"synth","text":"<p>Synth the bundle.yml for project</p> <p>Usage:</p> <pre><code>bf projects synth [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --skip-libraries    Skip automatically adding brickflow libraries.\n  -p, --profile TEXT  The databricks profile to use for authenticating to\n                      databricks during deployment.\n  --project []        Select the project of workflows you would like to\n                      deploy.\n  -e, --env TEXT      Set the environment value, certain tags [TBD] get added\n                      to the workflows based on this value.\n  --help              Show this message and exit.\n</code></pre>"},{"location":"cli/reference/#sync","title":"sync","text":"<p>Synchronize your bundle tree to databricks workspace (only supported by bundle deployment mode).</p> <p>Usage:</p> <pre><code>bf sync [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --deploy-mode [bundle]          Which deployment framework to use to deploy.\n                                  [default: bundle]\n  --watch                         Enable filewatcher to sync files over.\n  --full                          Run a full sync.\n  --interval-duration TEXT        File system polling interval (for --watch).\n  --debug TEXT                    File system polling interval (for --watch).\n  -p, --profile TEXT              The databricks profile to use for\n                                  authenticating to databricks during\n                                  deployment.\n  --git-provider TEXT             The github provider for brickflow this is\n                                  used for configuring github on DBX jobs.\n  --git-ref TEXT                  The commit/tag/branch to use in github.\n  -r, --repo-url TEXT             The github url in which to run brickflow\n                                  with.\n  -e, --env TEXT                  Set the environment value, certain tags\n                                  [TBD] get added to the workflows based on\n                                  this value.\n  -w, --workflow TEXT             Provide the workflow file names which you\n                                  want to deploy, each file name separated by\n                                  space! Example: bf deploy -p DEFAULT -l -w\n                                  wf1.py -w wf2.py\n  -wd, --workflows-dir DIRECTORY  Provide the workflow directory that has to\n                                  be deployed\n  -l, --local-mode                Set the environment flag to local and other\n                                  components [TBD] are disabled in local mode.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"faq/airflow-operator-rfc/","title":"Airflow operator rfc","text":""},{"location":"faq/airflow-operator-rfc/#airflow-operator-brickflow-support-rfc","title":"Airflow Operator - Brickflow Support RFC","text":"Airflow Operator Databricks Native Equivalent Will Implement Link to Issues Link to Impl Link to Docs Snowflake Operator Branch Python Operator Slack Operator Email Operator Task Dependency Sensor Canary Operator Bash Operator Short Circuit Operator S3 Sensor Compute Bash Operator Look at Bash Operator Compute Python Operator Use a task EMR Operator Use a task Spark Operator Use a task Python Operator Use a task Dummy Operator Use a task Genie Snowflake Operator Look at snowflake operator Genie Hive Operator N/A Genie S3 Dist CP Operator N/A Athena Operator Use DBSQL Nike EMR Operator Use a task Nike Spark Submit Operator Use a task Compute S3 Prefix Sensor Look at S3 sensor"},{"location":"faq/airflow-operator-rfc/#operators","title":"Operators","text":""},{"location":"faq/airflow-operator-rfc/#snowflake-operator","title":"Snowflake operator","text":""},{"location":"faq/airflow-operator-rfc/#branch-python-operator","title":"Branch python operator","text":""},{"location":"faq/airflow-operator-rfc/#slack-operator","title":"Slack operator","text":""},{"location":"faq/airflow-operator-rfc/#email-operator","title":"Email operator","text":""},{"location":"faq/airflow-operator-rfc/#task-dependency-sensor","title":"Task dependency sensor","text":""},{"location":"faq/airflow-operator-rfc/#bash-operator","title":"Bash operator","text":""},{"location":"faq/airflow-operator-rfc/#short-circuit-operator","title":"Short circuit operator","text":""},{"location":"faq/airflow-operator-rfc/#s3-prefix-sensor","title":"S3 Prefix Sensor","text":""},{"location":"faq/airflow-operator-rfc/#operators-which-will-not-be-supported","title":"Operators which will not be supported","text":""},{"location":"faq/airflow-operator-rfc/#compute-bash-operator","title":"Compute bash operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#compute-python-operator","title":"Compute python operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_1","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#emr-operator","title":"Emr operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_2","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#spark-operator","title":"Spark operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_3","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#python-operator","title":"Python operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_4","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#dummy-operator","title":"Dummy operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_5","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#canary-operator","title":"Canary operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_6","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#genie-snowflake-operator","title":"Genie snowflake operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_7","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#genie-hive-operator","title":"Genie hive operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_8","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#genie-s3-dist-cp-operator","title":"Genie s3 dist cp operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_9","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#athena-operator","title":"Athena operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_10","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#nike-emr-operator","title":"Nike emr operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_11","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#nike-spark-submit-operator","title":"Nike spark submit operator","text":""},{"location":"faq/airflow-operator-rfc/#alternative_12","title":"Alternative:","text":""},{"location":"faq/airflow-operator-rfc/#compute-s3-prefix-sensor","title":"Compute s3 prefix sensor","text":""},{"location":"faq/airflow-operator-rfc/#alternative_13","title":"Alternative:","text":""},{"location":"faq/faq/","title":"Faq","text":""}]}